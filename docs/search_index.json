[
["index.html", "Introducción al análisis funcional y a la teoría de la medida Ciencia de los Datos Financieros Prefacio ¿Por qué leer este libro? Estructura del libro Información sobre los programas y convenciones Prácticas interactivas con R Agradecimientos", " Introducción al análisis funcional y a la teoría de la medida Ciencia de los Datos Financieros Synergy Vision 2019-02-27 Prefacio La versión en línea de este libro se comparte bajo la licencia Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. ¿Por qué leer este libro? Este libro es el resultado de enfocarnos en proveer la mayor cantidad de material sobre Análisis y teoría de la medida con un desarrollo teórico lo más explícito posible, con el valor agregado de incorporar ejemplos de las finanzas y la programación en R. Finalmente tenemos un libro interactivo que ofrece una experiencia de aprendizaje distinta e innovadora. El un mundo abierto, ya no es tanto el acceso a la información, sino el acceso al conocimiento. Es mucha la literatura, pero son pocas las opciones donde se pueda navegar el libro de forma amigable y además contar con ejemplos en R y ejercicios interactivos, además del contenido multimedia. Esperamos que ésta sea un contribución sobre nuevas prácticas para publicar el contenido y darle vida, crear una experiencia distinta, una experiencia interactiva y visual. El reto es darle vida al contenido asistidos con las herramientas de Internet. Finalmente este es un intento de ofrecer otra visión sobre la enseñanza y la generación de material más accesible. Estamos en un mundo multidisciplinado, es por ello que ahora hay que generar contenido que conjugue en un mismo lugar las matemáticas, estadística, finanzas y la computación. Lo dejamos público ya que las herramientas que usamos para ensamblarlo son abiertas y públicas. Estructura del libro TODO: Describir la estructura Información sobre los programas y convenciones Este libro es posible gracias a una gran cantidad de desarrolladores que contribuyen en la construcción de herramientas para generar documentos enriquecidos e interactivos. En particular al autor de los paquetes Yihui Xie xie2015. Prácticas interactivas con R Vamos a utilizar el paquete Datacamp Tutorial que utiliza la librería en JavaScript Datacamp Light para crear ejercicios y prácticas con R. De esta forma el libro es completamente interactivo y con prácticas incluidas. De esta forma estamos creando una experiencia única de aprendizaje en línea. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6ImIgPC0gNSIsInNhbXBsZSI6IiMgQ3JlYSB1bmEgdmFyaWFibGUgYSwgaWd1YWwgYSA1XG5cblxuIyBNdWVzdHJhIGVsIHZhbG9yIGRlIGEiLCJzb2x1dGlvbiI6IiMgQ3JlYSB1bmEgdmFyaWFibGUgYSwgaWd1YWwgYSA1XG5hIDwtIDVcblxuIyBNdWVzdHJhIGVsIHZhbG9yIGRlIGFcbmEiLCJzY3QiOiJ0ZXN0X29iamVjdChcImFcIilcbnRlc3Rfb3V0cHV0X2NvbnRhaW5zKFwiYVwiLCBpbmNvcnJlY3RfbXNnID0gXCJBc2VnJnVhY3V0ZTtyYXRlIGRlIG1vc3RyYXIgZWwgdmFsb3IgZGUgYGFgLlwiKVxuc3VjY2Vzc19tc2coXCJFeGNlbGVudGUhXCIpIn0= Agradecimientos A todo el equipo de Synergy Vision que no deja de soñar. Hay que hacer lo que pocos hacen, insistir, insistir hasta alcanzar. Lo más importante es concretar las ideas. La idea es sólo el inicio y solo vale cuando se concreta. Synergy Vision, Caracas, Venezuela "],
["acerca-del-autor.html", "Acerca del Autor", " Acerca del Autor Este material es un esfuerzo de equipo en Synergy Vision, (http://synergy.vision/nosotros/). El propósito de este material es ofrecer una experiencia de aprendizaje distinta y enfocada en el estudiante. El propósito es que realmente aprenda y practique con mucha intensidad. La idea es cambiar el modelo de clases magistrales y ofrecer una experiencia más centrada en el estudiante y menos centrado en el profesor. Para los temas más técnicos y avanzados es necesario trabajar de la mano con el estudiante y asistirlo en el proceso de aprendizaje con prácticas guiadas, material en línea e interactivo, videos, evaluación contínua de brechas y entendimiento, entre otros, para procurar el dominio de la materia. Nuestro foco es la Ciencia de los Datos Financieros y para ello se desarrollará material sobre: Probabilidad y Estadística Matemática en R, Programación Científica en R, Mercados, Inversiones y Trading, Datos y Modelos Financieros en R, Renta Fija, Inmunización de Carteras de Renta Fija, Teoría de Riesgo en R, Finanzas Cuantitativas, Ingeniería Financiera, Procesos Estocásticos en R, Series de Tiempo en R, Ciencia de los Datos, Ciencia de los Datos Financieros, Simulación en R, Desarrollo de Aplicaciones Interactivas en R, Minería de Datos, Aprendizaje Estadístico, Estadística Multivariante, Riesgo de Crédito, Riesgo de Liquidez, Riesgo de Mercado, Riesgo Operacional, Riesgo de Cambio, Análisis Técnico, Inversión Visual, Finanzas, Finanzas Corporativas, Valoración, Teoría de Portafolio, entre otros. Nuestra cuenta de Twitter es (https://twitter.com/bysynergyvision) y nuestros repositorios están en GitHub (https://github.com/synergyvision). Somos Científicos de Datos Financieros "],
["introduccion.html", "Capítulo 1 Introducción", " Capítulo 1 Introducción "],
["conjuntos.html", "Capítulo 2 Conjuntos 2.1 Operaciones conjuntistas 2.2 Operaciones conjuntistas 2.3 Sistemas numéricos 2.4 Relaciones 2.5 Cardinalidad", " Capítulo 2 Conjuntos Los términos conjunto, colección y familia suelen ser sinónimos, a pesar de que en algunos contextos una de estas palabras tiene prioridad sobre otra. Por ejemplo, se suele decir colección de conjuntos y familia de funciones. Los conjuntos usualmente se denotan con una letra mayúscula, mientras que para los elementos de un conjunto se utilizan minúsculas. La notación \\(a \\in A\\) indica que el elemento \\(a\\) pertenece al conjunto \\(A\\). Por ejemplo, \\[2 \\in \\{a, 3, 2, b\\}\\] indica que el elemento \\(2\\) pertenece al conjunto \\(\\{a, 3, 2, b\\}\\). Nótese que el orden en que se escriben los elementos de un conjunto es indiferente. Así el conjunto anterior es igual a \\[\\{3,b,a,2\\}.\\] Las llaves \\(\\{ \\ \\}\\) siempre se usarán en la definición de conjuntos. En general, para describir a un conjunto exsisten dos formas de hacerlo. Cuando un conjunto se define dando la lista completa de todos sus elementos, decimo por ejemplo 2.1 Operaciones conjuntistas Si \\(A\\) es un subconjunto de \\(B\\), escribimos \\(A \\subseteq B\\). Si todos los conjuntos con los que se trabaja son subconjuntos de un conjunto \\(X\\), entonces \\(X\\) es llamado el conjunto universal. El conjunto potencia de un conjunto \\(X\\) es la colección \\(\\mathcal{P}(X)\\) de todos los subconjuntos de \\(X\\), es decir: \\[ \\mathcal{P}(X) = \\{Y \\, : \\, Y \\subseteq X\\}.\\] Si \\(A, B \\subseteq X\\), entonces \\(A \\cup B\\), \\(A \\cap B\\) y \\(A \\setminus B\\) denotan la unión, intersección y diferencia relativa de \\(A\\) y \\(B\\), El complemento de \\(A\\), definido como los elementos de \\(X\\) que no están en \\(A\\), se denota por \\(A^c\\). Si \\(\\mathcal{A}\\) es un subconjunto de \\(\\mathcal{P}(X)\\) y \\(B \\in \\mathcal{P}(X)\\), se define la traza de \\(\\mathcal{A}\\) en \\(B\\) como \\[\\mathcal{A}\\cap B := \\{A \\cap B \\, : \\, A \\in \\mathcal{A}\\}.\\] La unión e intersección de una familia indexada \\(\\mathcal{A}=\\{A_i \\, : \\, i \\in \\mathcal{I}\\}\\) se denotan, respectivamente, como \\[\\cap \\mathcal{A}:= \\cap_{i \\, \\in \\, \\mathcal{I}},\\] y \\[\\cup \\mathcal{A}:= \\cup_{i \\, \\in \\, \\mathcal{I}},\\] Si el conjunto de índices \\(\\mathcal{I}\\) es \\(\\{1,2, \\hdots, n\\}\\) o \\(\\{1,2, \\hdots\\}\\), se escribe \\[\\cup_{j=1}^n A_j=A_1 \\cup \\hdots \\cup A_n\\] \\[\\cap_{j=1}^n A_j=A_1 \\cap \\hdots \\cap A_n\\] \\[\\cup_{j \\geq 1} A_j=A_1 \\cup A_2 \\cup \\cdots\\] \\[\\cap_{j \\geq 1} A_j=A_1 \\cap A_2 \\cap \\cdots\\] 2.2 Operaciones conjuntistas Si \\(A\\) es un subconjunto de \\(B\\), escribimos \\(A \\subseteq B\\). Si todos los conjuntos con los que se trabaja son subconjuntos de un conjunto \\(X\\), entonces \\(X\\) es llamado el conjunto universal. El conjunto potencia de un conjunto \\(X\\) es la colección \\(\\mathcal{P}(X)\\) de todos los subconjuntos de \\(X\\), es decir: \\[ \\mathcal{P}(X) = \\{Y \\, : \\, Y \\subseteq X\\}.\\] Si \\(A, B \\subseteq X\\), entonces \\(A \\cup B\\), \\(A \\cap B\\) y \\(A \\setminus B\\) denotan la unión, intersección y diferencia relativa de \\(A\\) y \\(B\\), El complemento de \\(A\\), definido como los elementos de \\(X\\) que no están en \\(A\\), se denota por \\(A^c\\). Si \\(\\mathcal{A}\\) es un subconjunto de \\(\\mathcal{P}(X)\\) y \\(B \\in \\mathcal{P}(X)\\), se define la traza de \\(\\mathcal{A}\\) en \\(B\\) como \\[\\mathcal{A}\\cap B := \\{A \\cap B \\, : \\, A \\in \\mathcal{A}\\}.\\] La unión e intersección de una familia indexada \\(\\mathcal{A}=\\{A_i \\, : \\, i \\in \\mathcal{I}\\}\\) se denotan, respectivamente, como \\[\\cap \\mathcal{A}:= \\cap_{i \\, \\in \\, \\mathcal{I}},\\] y \\[\\cup \\mathcal{A}:= \\cup_{i \\, \\in \\, \\mathcal{I}},\\] Si el conjunto de índices \\(\\mathcal{I}\\) es \\(\\{1,2, \\hdots, n\\}\\) o \\(\\{1,2, \\hdots\\}\\), se escribe \\[\\cup_{j=1}^n A_j=A_1 \\cup \\hdots \\cup A_n\\] \\[\\cap_{j=1}^n A_j=A_1 \\cap \\hdots \\cap A_n\\] \\[\\cup_{j \\geq 1} A_j=A_1 \\cup A_2 \\cup \\cdots\\] \\[\\cap_{j \\geq 1} A_j=A_1 \\cap A_2 \\cap \\cdots\\] \\begin{prop} La unión e intersección de conjuntos satisfacen: \\begin{itemize} {} \\begin{itemize} ({i }A_i )^c = {i }A_i^c; (A{i } A_i )^c={i }A_i^c; \\end{itemize} \\begin{itemize} {} A ({i }A_i ) = {i A A_i}; A ({i } A_i ) = {i }A A_i. \\end{itemize} \\end{prop} Una familia \\(\\{A_i \\, : \\, i \\in \\mathcal{I}\\}\\) de conjuntos se dice {} si \\(A_i \\cap A_j=\\emptyset\\), siempre que \\(i \\neq j\\). En este caso, la uni'on \\(\\bigcup_{i \\in \\mathcal{I}}\\) se dice {}. Una {} (o {}) de un conjunto \\(X\\) es una colecci'on de conuntos disjuntos no vac'ios , cuya uni'on disjunta es todo \\(X\\). Una sucesi'on de conjuntos \\(\\{A_n\\}_{n \\geq 0}\\) se dice {} si \\(A_1 \\leq subseteq A_2 \\subseteq \\cdots\\), en cuyo caso escribiremos \\(A_n \\uparrow\\). Si \\(A_1 \\supseteq A_2 \\supseteq \\cdots\\), se dice que la sucesi'on es {}. Denotaremos por \\(A_n \\downarrow\\) a una sucesi'on que es decreciente. Si \\(\\{A_n\\}_{n \\geq 0}\\) es creciente y \\(A=A_1 \\cup A_2 \\cup \\cdots\\) (respectivamente si \\(\\{A_n\\}_{n \\geq 0}\\) es decreciente y \\(A=A_1 \\cap A_2 \\cap \\cdots\\)), escribiremos \\(A_n \\uparrow A\\) (respectivamente \\(A_n \\downarrow A\\)). El producto cartesiano de sucesiones finitas o infinitas numerables de conjuntos \\(A_1, A_2, \\hdots\\) se denotan, respectivamente, por \\(\\begin{array}[ccc] \\prod_{n=1}^d A_n = A_1 \\times \\cdots \\times A_d &amp;y&amp; \\prod_{n=1}^\\infty A_n = A_1 \\times A_2 \\times \\cdots. \\end{array}\\) Si \\(A_1 = A_2 = \\cdots = A\\), escribiremos \\(A^d\\) para el producto finito y \\(A^\\infty\\) para el producto infinito. El producto cartesiano \\(X:=\\prod_{i \\in \\mathcal{I}}X_i\\) de una familia de conjuntos no vacios \\(X_i\\) se define como la colecci'on de todas las funciones \\(f:\\mathcal{I}\\to \\bigcup_{i \\in \\mathcal{I}}X_i\\) tales que \\(f(i)\\in X_i\\), para cada \\(i\\). N'otese que el producto cartesiano de conjuntos no vac'ios es siempre no vac'io, por el axioma de elecci'on. El valor \\(f(i)\\) se denomina la {}. En el caso particular en que el conjunto de 'indices \\(\\mathcal{I}\\) sea el conjunto \\(\\{1,2, \\hdots, n\\}\\), se tiene que toda funci'on \\(f: \\{1,2, \\hdots, n\\}\\to X\\) est'a completamente determinada por por la \\(n\\)-tupla \\((f(1), f(2), \\dots, f(n))\\). 2.3 Sistemas numéricos Usaremos la siguiente notaci'on para el sistema num'e rico est'andar: Los términos conjuntos, colecciones y familias son sinónimos, a pesar de que en algunos contextos se tenga mas preferencia sobre una frase que otra. Por ejemplo, se suele decir coleccion de objetos y familia de funciones. Los conjuntos se denotan usualmente con una letra mayuscula, y elementos de un conjunto con una letra minuscula. Se usa la notacion \\[a \\in A\\] para indicar que el elemento \\(a\\) pertenece al conjunto \\(A\\). Existen dos maneras de representar a un conjunto (comprehensiva y extensiva, ver notas de Manuel.) Dos subconjuntos de \\(\\mathbb{C}\\) son de particular importancia: En general, el s'imbolo \\(\\mathbb{K}\\) denota a un cuerpo de caracter'istica \\(0\\), como por ejemplo \\(\\mathbb{Q}\\), \\(\\mathbb{R}\\) o \\(\\mathbb{C}\\). Si \\(A \\subseteq \\mathbb{R}\\), escribiremos \\(A^+\\) para denotar al conjunto \\(A \\cap [0, \\infty)\\). Asi, por ejemplo, se tiene que \\(\\mathbb{Z}^+=\\{n \\in \\mathbb{Z} \\, : \\, n \\geq 0\\}=\\mathbb{N}\\). Si \\(A \\subseteq \\mathbb{C}\\), se denota por \\(A_*\\) al conjunto de elementos no nulos de \\(A\\). Sea \\(d\\) un entero positivo. Si \\(A\\) es un conjunto, se define \\[A^d:=A \\times A \\times \\cdots \\times A.\\] Los {} se definen, respectivamente, como \\(\\mathbb{R}^d\\) y \\(\\mathbb{C}^d\\), con \\(d \\in \\mathbb{P}\\). Un {} en \\(\\mathbb{R}^d\\) es un producto cartesiano de intervalos en \\(\\mathbb{R}\\). Es decir, si \\({\\bf a}:=(a_1, a_2, \\hdots, a_d)\\) y \\({\\bf b}:=(b_1, b_2, \\hdots, b_d)\\) son elementos de \\(\\mathbb{R}^d\\), un intervalo \\(d\\)-dimensional es de la forma \\[({\\bf a}, {\\bf b}]:=(a_1, b_1] \\times (a_2, b_2] \\times \\cdots \\times (a_d, b_d].\\] La {} en \\(\\mathbb{K}^d\\) se define como \\[|{\\bf z}|:=\\sqrt{|z_1|^2+|z_2|^2+ \\cdots + |z_d|^2},\\] para cada \\({\\bf z}=(z_1, z_2, \\hdots, z_d) \\in \\mathbb{C}^d\\). El {} es el conjunto \\[\\overline{\\mathbb{R}}:=\\mathbb{R} \\cup \\{\\pm \\infty\\}=[-\\infty, +\\infty].\\] Aqui, \\(-\\infty\\) y \\(+ \\infty\\) denotan dos s'imbolos abstractos, que satisfacen los siguientes axiomas: Los elementos de \\([0, +\\infty]=\\overline{\\mathbb{R}}^+\\) se denominan {}. 2.4 Relaciones Una {} en un conjunto no vac'io \\(X\\) es un conjunto \\(R \\subseteq X \\times X\\), no vac'io. Si \\(R\\) es una relaci'on de \\(X\\) y \\((x,y) \\in R\\), se escribe \\(x \\sim_R y\\), o \\(x \\sim y\\). Una relaci'on de \\(X\\) se dice: Una relaci'on en \\(X\\) que es reflexiva, sim'etrica y transitiva se conoce como {}. La {} es el conjunto \\[[x]:= \\{y \\in X \\, : \\, x \\sim y\\}.\\] La colecci'on de clases de equivalencias distintas de \\(X\\) se denota como \\(X/\\sim\\). As'i: \\[X/\\sim = \\{[x] \\, : \\, x \\in X\\}.\\] Las relaciones de equivalencia de un conjunto no vac'io X y las particiones conjuntistas de \\(X\\) son dos nociones equivalentes. En efecto, dado que \\[\\begin{equation} x \\in [y] \\Longleftrightarrow y \\in [x] \\Longleftrightarrow [x]=[y], \\end{equation}\\] es claro que los elementos de \\(X/\\sim\\) son disjuntos dos a dos. Adem'as, \\[\\begin{equation} \\bigcup_{[x] \\, \\in \\, X/\\sim}[x]=\\{y \\in X \\, : \\, x \\sim y, \\text{ para alg\\&#39;un } x \\in X\\}=X. \\end{equation}\\] As'i, la colecci'on \\(X/\\sim\\) es una partici'on de \\(X\\). Rec'iprocamente, dada una partici'on de \\(X\\), la relaci'on definida por \\[\\begin{equation} x \\sim y \\Longleftrightarrow x \\text{ y } y \\text{ pertenecen al mismo bloque de la partici\\&#39;on } \\end{equation}\\] es una relaci'on de equivalencia para \\(X\\), donde cada clase de equivalencia es precisamente un bloque de la partici'on. Un ejemplo de relaci'on de equivalencia en \\(\\mathbb{R}\\) es el siguiente: la relaci'on \\(\\sim\\) est'a dada por \\[\\begin{equation} x\\sim y \\Longleftrightarrow x-y \\in \\mathbb{Q}. \\end{equation}\\] En este caso, existe solo una clase \\([x] \\in \\mathbb{R}/\\sim\\) formada por n'umeros racionales. De hecho, no es dif'icil demostrar que \\([x]=\\mathbb{Q}\\). Cualquier otra clase \\([y]\\) de \\(\\mathbb{R}/\\sim\\) est'a formada por n'umeros irracionales. Una relaci'on que es reflexiva, antisim'etrica y transitiva es llamada un {}. En este caso, se escribe \\(x \\leq y\\) para denotar que el par \\((x,y)\\) pertenecen a este tipo de relaci'on. Un conjunto con un orden parcial se denomina {}. Una {} (respectivamente, {}) de un subconjunto \\(Y\\) de un conjunto parcialmente ordenado \\((X, \\leq)\\) es un elemento \\(x \\in X\\) tal que \\(y \\leq x\\) (respectivamente, \\(x \\leq y\\)), para todo \\(y \\in Y\\). Un {} (respectivamente, un {})de \\(Y\\) es una cota superior (respectivamente, inferior) \\(x_0\\) de \\(Y\\) tal que \\(x_0 \\leq x\\) (respectivamente, \\(x_0 \\geq x\\)) para toda cota superior (respectivamente, inferior) \\(x\\) de \\(Y\\). Por ejemplo, el par \\((\\mathcal{P}(X), \\subseteq)\\) forma un conjunto parcialmente ordenado, para cada conjunto \\(X\\). Si \\(Y \\in \\mathcal{P}(X)\\), entonces los subconjuntos \\(\\cap_{y \\, \\in \\, Y}y\\) y \\(\\cup_{y \\, \\in \\, Y}y\\) corresponden al 'infimo y al supremo de \\(Y\\), respectivamente. Un elemento \\(x\\) de \\(X\\) se dice {} si para cada \\(y \\in X\\) tal que \\(x \\leq y\\), se tiene que \\(x=y\\). Del mismo modo, un elemento \\(x\\) de \\(X\\) se dice {} si para cada \\(y \\in X\\) tal que \\(x \\geq y\\), se tiene que \\(x=y\\). Un subconjunto no vac'io \\(Y\\) de un conjunto parcialmente ordenado \\((X, \\leq)\\) se dice {} si, para cada \\(y_1, y_2 \\in Y\\) distintos se satisface \\(y_1 \\leq y_2\\) o \\(y_2 \\leq y_1\\). Un conjunto totalmente ordendao tambi'en suele llamarse una {}. La noci'on de cadena aparece en el siguiente resultado importante, que se prueba utilizando directamente el axioma de elecci'on: Lema de Zorn: Sea \\(X\\) un conjunto parcialmente ordenado, tal que cada cadena posea una cota superior en \\(X\\). Entonces \\(X\\) posee un elemento maximal. ##Funciones Sean \\(X\\) y \\(Y\\) conjuntos. Una {funci'on} de \\(X\\) en \\(Y\\) es (completar). Una funci'on \\(f\\) cuyo conjunto de partida es \\(X\\) y conjunto de llegada es \\(Y\\) se denota por \\(f: X \\to Y\\). La colecci'on de todas las funciones de \\(X\\) en \\(Y\\) se denota como \\(Y^X\\). La {} de \\(A \\subseteq X\\) y la {preimagen} de \\(B \\subseteq Y\\) bajo la funci'on \\(f:X \\to Y\\) se definen, respectivamente, como \\[f(A):=\\{f(x) \\in B \\, : \\, x \\in A\\},\\] y \\[f^{-1}(A):=\\{x \\in A \\, : \\, f(x) \\in B\\}.\\] Una funci'on \\(f:X \\to Y\\) es {} si \\(f(X)=Y\\), e {} si \\(x_1 \\neq x_2\\) implica que \\(f(x_1)\\neq f(x_2)\\). Una {} (respectivamente, una {}) es una funci'on que es sobreyectiva (respectivamente, inyetiva). Una funci'on que es a la vez sobreyectiva e inyectiva se dice {}. Un ejemplo importante de funci'on sobreyectiva es la {funci'on cociente} \\(C_\\sim: X \\to X/\\sim\\), asociada a una relaci'on de equivalencia \\(\\sim\\) de un conjunto no vac'io \\(X\\). Por definici'on, \\(C_\\sim(x):=[x]\\). En particular, la preimagen de un subconjunto \\(B\\) de \\(X/\\sim\\) bajo \\(C_\\sim\\) es la uni'on de todas las clases de equivalencias \\([x]\\) in \\(B\\). Si \\(X\\), \\(Y\\) son conjuntos y \\(\\{X_i \\, : \\, i \\in \\mathcal{I}\\}\\), \\(\\{Y_j \\, : \\, j \\in \\mathcal{J}\\}\\) son subconjuntos de \\(X\\) y \\(Y\\), respectivamente, se tiene que: Las igualdades en \\((d)\\) y \\((h)\\) se satisfacen si \\(f\\) es inyectiva, mientras que las igualdades de \\((f)\\) y \\((g)\\) suceden si \\(f\\) es sobreyectiva. Para cada \\(f:X \\to Y\\), \\(\\mathcal{A} \\subseteq \\mathcal{P}(X)\\) y \\(\\mathcal{B} \\subseteq \\mathcal{P}(X)\\), se definen las colecciones \\[f(\\mathcal{A})=\\{f(A) \\, : \\, A \\in \\mathcal{A}\\} \\subseteq \\mathcal{P}(y)\\] \\[f^{-1}(\\mathcal{B})=\\{f^{-1}(B) \\, : \\, B \\in \\mathcal{B}\\}\\subseteq \\mathcal{P}(y)\\] Si \\(f:X \\to Y\\) y \\(g: Y \\to Z\\) son funciones, donde \\(f(X)\\subseteq Y\\), la función \\(g \\circ f:X \\to Z\\) definida por \\[(g\\circ f)(x)=g(f(x))\\], para todo \\(x \\in X\\), se denomina {}. Si \\(A \\subseteq Z\\), nótese que se tiene la siguiente relación: \\[(g \\circ f)^{-1}=f^{-1}(g^{-1}(A))\\]. La {} \\(\\text{id}_X\\) de un conjunto \\(X\\) se define como \\(\\text{id}_X(x)=x\\), para cada \\(x \\in X\\). Si \\(A \\subseteq X\\), la restricción de \\(\\text{id}_X\\) a \\(A\\) se denomina {inclusión} o {morfina de inclusión} y se denota como \\(\\iota_A : A \\hookuparrow X\\). Si \\(f:X \\to Y\\) es biyectiva, la {} \\(f^{-1}:Y \\to X\\) es la función definida como \\[f^{-1}(y)=x \\Longleftrigtharrow y=f(x).\\] Se tiene que \\begin{array}[ccc] f^{-1}f=_X &amp;y&amp; f f^{-1}=_Y. \\end{array} Si \\(X\\) es un conjunto universal y \\(A \\subseteq X\\), la {} es la función definida por \\[\\begin{equation} {\\bf 1}_A(x)= \\left\\{ \\begin{array}{lcc} 1 &amp; \\text{ si } &amp; x \\in A \\\\ 0 &amp; \\text{ si } &amp; x \\in A^c \\end{array} \\right. \\end{equation}\\] Toda función indicatriz satisface las siguientes propiedades: \\begin{itemize} {}_{A B}={}_A{}_B; {}_{A B}= {}_A + {}B - {}{A B}; {}_{A^c}=1-{}_A. \\end{itemize} Un caso particular de función indicatriz es la función {}: \\[\\begin{equation} \\delta_{x,y}= \\left\\{ \\begin{array}{lcc} 1 &amp; \\text{ si } &amp; x=y \\\\ 0 &amp; \\text{ si } &amp; x \\neq y \\end{array} \\right. \\end{equation}\\] En efecto, si \\(A=\\{(x,x) \\, : \\, x \\in X\\}\\), entonces \\(\\delta_{x,y}={\\bf 1}_A(x,y)\\). Las funciones \\(x^+\\) y \\(x^-\\) en \\(\\mathbb{R}\\) se definen como \\[\\begin{array}{ccc} x^+:=\\max\\{x,0\\} &amp; \\text{ y } &amp; x^-:=\\max\\{-x,0\\}. \\end{array}\\] De estas definiciones se desprende fácilmente que \\[\\begin{array}{ccc} x=x^+-x^- &amp; \\text{ y } &amp; |x|=x^+ + x^-. \\end{array}\\] La parte real y la parte imaginaria de un número complejo \\(z\\) se denotan, respectívamente, por \\(\\Re(z)\\) y \\(\\Im(z)\\). Asimismo, la conjugada de \\(z\\) y el módulo de \\(z\\) se denotan por \\(\\overline{z}\\) y \\(|z|\\). Se tiene que \\[\\begin{array}{cccccc} z=\\Re(z) + i\\Im(z) &amp;,&amp; \\overline{z}=\\Re(z)-i\\Im(z) &amp; \\text{ y } &amp; |z|=\\sqrt{(\\Re(z))^2+(\\Im(z))^2}. \\end{array}\\] El signo \\(\\sgn(z)\\) de una variable compleja \\(z\\) se define como \\[\\begin{equation} \\sgn(z)= \\left\\{ \\begin{array}{lcc} \\frac{|z|}{z} &amp; \\text{ si } &amp; z \\neq 0 \\\\ 0 &amp; \\text{ si } &amp; z = 0. \\end{array} \\right. \\end{equation}\\] Así, \\(|z|=z\\sgn(z)\\) para todo \\(z \\in \\mathbb{C}\\) y \\(|\\sgn(z)|=1\\) siempre que \\(z\\neq 0\\). Si \\(F \\subseteq Y^X\\), el funcional de evaluación en \\(x\\) es la función \\(\\text{ev}_x:F \\to Y\\) dada por \\[\\begin{equation} \\text{ev}_x(f):=f(x), \\qquad f \\in F. \\end{equation}\\] El adjunto o dual de una función \\(\\phi:Z \\to X\\) (con respecto a \\(F\\)) es la función \\(\\phi^*:F \\to Y^Z\\) definida como \\[\\begin{equation} \\phi^*(f):=f \\circ \\phi, \\qquad f \\in F. \\end{equation}\\] Las siguientes notaciones se utilizarán en los capítulos siguientes, para cada función a valores reales: \\begin{array}{cc} f^+(x):= &amp; f^-:= (-f)^+\\ (f_1 f_n)(x):={1 , , k ,, n}f_k(x) &amp; (f_1 f_n)(x):={1 , , k ,, n}f_k(x)\\ (_n f_n)(x):=_n f_n(x) &amp; (_n f_n)(x):=_n f_n(x)\\ (_n f_n)(x):=_n f_n(x) &amp; (_n f_n)(x):= _n f_n(x). \\end{array} 2.5 Cardinalidad Dos conjuntos \\(A\\) y \\(B\\) se dicen tener la misma cardinalidad si existe una biyección de \\(A\\) a \\(B\\).Un conjunto \\(A\\) se dice finito si \\(A\\) tiene la misma cardinalidad que \\(\\{1,2, \\hdots, n\\}\\), para algún \\(n \\in \\mathbb{N}\\). En este caso, los elementos de \\(A\\) se pueden indexar utilizando el conjunto \\(\\{1,2, \\hdots, n\\}\\), de modo que se puede escribir \\(A=\\{a_1, a_2, \\hdots, a_n\\}\\). Un conjunto \\(A\\) es infinito numerable si tiene la misma cardinalidad que el conjunto \\(\\mathbb{N}\\) de los n, en cuyo caso se escribe \\(A=\\{a_0, a_1, a_2, \\hdots\\}\\). Un conjunto es numerable si es finito o infinito numerable; en caso contrario, se dice que el conjunto es no-numerable. Los conjuntos \\(\\mathbb{Z}\\) y \\(\\mathbb{Q}\\) son numerables. El conjunto \\(\\mathbb{R}\\) es no numerable, asi como cualquier intervalo de números reales. La cardinalidad de \\(\\mathbb{R}\\) se denota por \\(\\mathfrak{c}\\), mientras que la cardinalidad de \\(\\mathbb{N}\\) se denota por \\(\\aleph_0\\). "],
["sistema-numerico-real-y-complejo.html", "Capítulo 3 Sistema numérico real y complejo 3.1 Introducción 3.2 Propiedades algebraicas de \\(\\mathbb{R}\\) 3.3 Estructura de orden de \\(\\mathbb{R}\\) 3.4 Propiedades de completitud de \\(\\mathbb{R}\\) 3.5 Inducción matemática 3.6 Espacios euclídeos", " Capítulo 3 Sistema numérico real y complejo 3.1 Introducción En este capítulo se introducen las nociones esenciales para el estudio del sistema numérico real, el cual proveerá el basamento teórico para introducir la noción de límite. Por sistema numérico real entendemos a un conjunto no vacío \\(\\mathbb{R}\\), junto con dos operaciones binarias, llamadas suma y producto, y una relación de orden total, satisfaciendo tres tipos de axiomas: los axiomas de cuerpo, los axiomas de orden y el axioma de completitud. Existen varias maneras de construir reigurosamente al conjunto de los números reales. Una de ellas consiste primero en definir al conjunto de los números naturales \\(\\mathbb{N}\\), luego a los enteros \\(\\mathbb{Z}\\), siguiendo con los racionales \\(\\mathbb{Q}\\) y finalmente llegando a los números reales \\(\\mathbb{R}\\). En esta construcción, se pide que el conjunto de los números naturales satisfagan una serie de axiomas, llamados axiomas de Peano, los cuales permiten definir las operaciones de suma y producto entre naturales. La resta se introduce luego, agrandando el sistema numérico de los naturales a \\(\\mathbb{Z}\\), y permitiendo resolver ecuaciones de la forma \\(x+m=n\\) para una indeterminada \\(x\\), y \\(m,n \\in \\mathbb{N}\\). Para obtener la división, se identifica el cociente \\(m/n\\), para \\(m,n \\in \\mathbb{Z}\\) y \\(n \\neq 0\\), con la clase \\([(m,n)]\\) definida a partir de la relación \\(\\sim\\) en \\(\\mathbb{Z} \\times (\\mathbb{Z}\\setminus\\{0\\})\\) dada por \\[\\begin{equation} (m,n) \\sim (m&#39;,n&#39;) \\Longleftrightarrow mn&#39;=nm&#39;. \\end{equation}\\] En este sistema numérico, las ecuaciones del tipo \\(ax+b=c\\), con \\(a,b,c \\in \\mathbb{Z}\\) y \\(a\\neq 0\\), poseen solución. El último paso, la construcción de \\(\\mathbb{R}\\) a partir de \\(\\mathbb{Q}\\), se realiza (informalmente) llenando los vacíos entre números racionales, mediante la incorporación de los números irracionales. Este es el principio del axioma de completitud. 3.2 Propiedades algebraicas de \\(\\mathbb{R}\\) 3.3 Estructura de orden de \\(\\mathbb{R}\\) 3.4 Propiedades de completitud de \\(\\mathbb{R}\\) 3.5 Inducción matemática 3.6 Espacios euclídeos "],
["estructuras-algebraicas.html", "Capítulo 4 Estructuras algebraicas 4.1 Semigrupos y grupos 4.2 Espacios vectoriales 4.3 Transformaciones lineales 4.4 Espacios vectoriales cocientes 4.5 Álgebras", " Capítulo 4 Estructuras algebraicas 4.1 Semigrupos y grupos 4.2 Espacios vectoriales 4.3 Transformaciones lineales 4.4 Espacios vectoriales cocientes 4.5 Álgebras "],
["sucesiones-numericas.html", "Capítulo 5 Sucesiones numéricas 5.1 Límite de una sucesión 5.2 Sucesiones monótonas 5.3 Subsucesiones y sucesiones de Cauchy 5.4 Límites inferior y superior", " Capítulo 5 Sucesiones numéricas 5.1 Límite de una sucesión 5.2 Sucesiones monótonas 5.3 Subsucesiones y sucesiones de Cauchy 5.4 Límites inferior y superior "],
["sucesiones-y-series.html", "Capítulo 6 Sucesiones y series 6.1 Límite de una función 6.2 Límites inferior y superior 6.3 Funciones contínuas 6.4 Propiedades de las funciones contínuas 6.5 Continuidad uniforme", " Capítulo 6 Sucesiones y series 6.1 Límite de una función 6.2 Límites inferior y superior 6.3 Funciones contínuas 6.4 Propiedades de las funciones contínuas 6.5 Continuidad uniforme "],
["diferenciacion.html", "Capítulo 7 Diferenciación 7.1 Definición y ejemplos 7.2 El teorema del valor medio 7.3 Funciones convexas 7.4 Funciones inversas 7.5 Regla de L’Hospital 7.6 Teorema de Taylor en \\(\\mathbb{R}\\) 7.7 Método de Newton", " Capítulo 7 Diferenciación A lo largo de esta secci'on vamos a trabajar en \\(\\real^n={(x_1,\\dots c, x_n) \\ x_j \\in \\real, j=1,...,N}\\) Durante todo el año denotaremos al vector \\((x_1,x_2,\\dots,x_n)\\) como \\(\\gx\\) por comodidad. En general, un producto escalar es una matriz definida positiva y se opera de la siguiente manera: \\[\\pesc{\\gx,\\gy} = (x_1,x_2,...,x_N) \\cdot \\begin{pmatrix} a_11 &amp;\\cdots&amp; a_1N\\\\ \\vdots&amp; \\ddots &amp; \\vdots\\\\ a_N1 &amp; \\cdots&amp; a_NN \\end{pmatrix} \\cdot \\begin{pmatrix} y_1\\\\ \\vdots\\\\ y_N \\end{pmatrix}\\] Hay tres casos particulares, la norma uno \\[ \\md{\\gx}_1 = |x_1| + |x_2| + ... + |x_n| \\] La norma 2, que es la norma euclídea y la norma infinito \\[\\md{\\gx}_{\\infty} = \\max\\left\\{|x_1|,|x_2|,\\dots,|x_n|\\right\\} \\] Vamos a demostrar que la norma \\(p\\) cumple las 3 propiedades de una norma. Para ello, nos apoyaremos en dos teoremas previos: Una vez probadas las dos desigualdades anteriores, pasamos a probar la desigualdad triangular: EJERCICIO PROPUESTO: Tomamos en el plano el conjunto de los puntos cuya norma es 1. Tomando en la norma p=2 sale la circunferencia. ¿Y en p=3? Propiedades: Con un producto escalar puedo definir una norma y con esa norma puedo definir una distancia. Pero… ¿Podemos definir una norma que no venga de un producto escalar y/o alguna distancia que no provenga de una norma? Sí, por ejemplo \\[ \\tilde{d} (\\gx,\\gy) = \\abs{\\arctg(y) - \\arctg(x)} \\] No cuesta mucho comprobar que cumple las 3 propiedades de una distancia. Además, esta distancia es cuanto menos curiosa porque nunca será mayor de \\(\\pi\\). Podemos comprobar que si existiera una norma que midiese esta distancia tendríamos \\[\\tilde{\\md{\\gx}} = \\tilde{d} (\\gx,\\gor{0}) = \\abs{\\arctg (x)} \\] pero esto no cumple la propiedad: \\(\\tilde{\\md{\\lambda x}} = \\abs{\\arctg \\lambda x} \\neq \\abs{\\lambda}\\abs{\\arctg x} = \\abs{\\lambda x}\\tilde{||x||}\\) ya que ninguna distancia puede ser mayor que \\(\\pi\\) y tomando un \\(\\lambda &gt; \\pi\\) se produciría la contradicción. Esa norma asociada al producto escalar tiene dos propiedades importantes: Sea \\(\\md{\\cdot}\\) una norma en \\(\\real^N\\). Si intento calcular la norma de un vector \\(\\gx\\) \\[ \\md{\\gx} = \\md{\\sum x_i e_i} \\leq \\sum_{i=1}^N \\md{x_1 e_1} = \\sum_{i=1}^N|x_i|\\cdot\\md{e_i} \\] Tenemos: \\(\\md{\\gx} \\leq \\sum_{i=1}^N c_i |x_i|\\) siendo \\(c_i = \\md{e_i}\\). Aplicando Cauchy-Schwarz nos queda \\[ \\sum_{i=1}^N \\left(c_i^2\\right)^\\frac{1}{2} \\cdot \\sum \\left(|x_i|^2\\right)^\\frac{1}{2} \\] Es decir, puedo controlar cualquier norma con una constante y la norma euclídea: \\[|||\\gx||| \\leq C \\md{x}_{2}\\] En particular, \\(0 \\leq |||\\gor{x_n} - \\gx|||\\leq c ||\\gor{x_n}-\\gx||\\). Sea \\(F(\\gx) = |||\\gx|||\\) y \\(F:\\real^N \\rightarrow \\real^N\\) \\[|F(\\gx)-F(\\gy)| = \\left| |||\\gx - \\gy||| \\right| = |||\\gx - \\gy||| \\leq C ||\\gx - \\gy||\\] Utilizando: \\(|||\\ga-\\gb||| \\ge |||\\ga||| - |||\\gb|||\\) Es decir, cualquier norma en \\(\\real^n\\) es respecto de la norma euclídea. Para evitar jaleos, al tratar la distancia vamos a tomar la norma euclídea. Como todas las normas son equivalentes, nos da igual tomar una que otra. \\(F(\\gor{x}) = |||\\gor{x}|||\\) una norma (que ya sabemos que es continua): \\(m \\md{x} \\leq |||\\gor{x}||| \\leq C||\\gor{x}||\\) Al tener definida una norma de vectores podemos definir convergencia y continuidad: A partir de esta definición podemos estudiar qué hacen las funciones continuas con conjuntos abiertos y cerrados. Sea \\(F\\) continua. Contrario a lo que podríamos intuir, Empezamos definiendo en qué consiste una función inversa: A partir de aquí podemos extraer dos conclusiones que sí nos ayudarán a discernir si un conjunto imagen es abierto y cerrado. Este teorema nos sirve para decir fácilmente si un conjunto es abierto o cerrado. Por ejemplo, consideremos \\[ M=\\{(x,y,z) \\in \\real^3 \\tq x^2 + \\cos\\left(x\\abs{y}\\right) - e^z &lt; 1 \\} \\] Podemos definir ahí la función \\(F\\) como \\[ F(x,y,z) = x^2 + \\cos\\left(x\\abs{y}\\right) - e^z\\] que va de \\(\\real^3\\) a cierto conjunto \\(A = \\{ a \\in \\real \\tq a &lt; 1 \\}\\). Podemos reescribir \\(M\\) como \\[M=\\{(x,y,z) \\in \\real^3 \\tq F(x,y,z) \\in A\\}\\] o, de otra forma, \\(M = \\inv{F}(A)\\). Según el teorema (), como \\(A\\) es abierto entonces \\(M\\) también es abierto. Además, toda aplicación lineal se puede escribir en forma de matriz. \\[L(\\gor{x})=A\\gor{x} = \\begin{pmatrix} a_{11} &amp; \\cdots &amp; a_{1n} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; \\cdots &amp; a_{nn} \\end{pmatrix}\\] Consideramos una aplicación continua \\[\\begin{align*} \\appl{F}{\\real^N&amp;}{\\real} \\\\ \\gor{x} &amp;\\longrightarrow F(\\gx) = \\underbrace{||A\\gx||}_{L(\\gx)} \\end{align*}\\] Sabemos que existe \\(C&gt;0\\) tal que \\(||A\\gx|| \\leq C||\\gx||\\), es decir, \\(||A\\gx||\\leq C\\) si \\(||\\gx||=1\\). Queremos saber cuál es la mejor constante que podemos encontrar, la que más se ajuste. Consideramos el conjunto \\(M\\) de todos los vectores de la esfera unidad, es decir \\[ M = \\{||A\\gx|| \\tq ||\\gx||=1\\}\\subset \\real \\] Entonces mejor constante \\(C\\) es la cota superior mínima (supremo) que vamos a llamar \\(\\alpha\\). Al ser \\(F\\) continua y \\(M\\) compacto, sabemos que el supremo \\(\\alpha \\in M\\). Basándonos en lo que hemos obtenido, calculamos la norma uno de \\[A = \\begin{pmatrix} 1&amp;2\\\\-3&amp;1\\\\2&amp;0 \\end{pmatrix}\\] Tenemos que maximizar, sabiendo que \\(|x|+|y| = 1\\): \\[ |x+2y| + |-3x+y| + |2x| \\leq |x|+|2y| + |3x| + |y| + 2|x| = 6|x|+3|y| \\leq 6 (|x|+|y|) =6 \\] ¿Podemos encontrar un vector \\(\\gx = (x_0,y_0)\\) tal que \\(||A(x_0,y_0)^T||_1 = 6\\)?\\ Tomando \\(x_0 = 1\\) y \\(y_0 = 0\\) lo encontramos. Como \\(\\gx\\) está en la esfera unidad y es una cota, es el máximo y por lo tanto la norma que buscamos. Curiosamente, y escoger el más grande. Si tomamos la norma infinito de \\[A = \\begin{pmatrix} 1&amp;2\\\\-3&amp;1\\\\2&amp;0 \\end{pmatrix}\\] Tenemos que . Queremos buscar ahora cuánto vale la norma de una matriz cuando usamos la norma euclídea. Usaremos los dos lemas siguientes para apoyarnos. Tenemos \\(A^TA\\) diagonalizable, de dimensión \\(N \\times N\\). Buscamos cuánto vale \\(\\norm{A\\gx}\\) con \\(\\gx \\in \\real^N\\). Empezamos desarrollando el vector en una base ortonormal de \\(A^T A\\): \\[ \\gx = \\sum \\alpha_i \\gv_i \\] y por lo tanto \\[ ||\\gx|| = \\sum \\alpha_i^2 \\pesc{\\gv_i,\\gv_i}\\]. Desarrollamos el producto \\[\\trans{A}A\\gx = \\trans{A}A\\left(\\sum \\alpha_i \\gv_i\\right) = \\sum \\left(\\alpha_i\\lambda_i\\gor{v}_i\\right) \\] donde \\(\\lambda_i\\) son los autovalores de \\(\\trans{A}A\\). Ahora queremos hallar el máximo de \\(||A\\gx||\\) cuando \\(||\\gx|| = 1\\): \\[ ||A\\gx||^2 = \\pesc{A\\gx,A\\gx} = \\pesc{\\trans{A}A\\gx,\\gx} = \\pesc{\\sum \\lambda_i \\alpha_i \\gv_i,\\sum\\alpha_i \\gv_i} \\] Como la base de \\(\\{\\gv_n\\}\\) es ortonormal, \\(\\pesc{\\gv_i, \\gv_j} = 0\\) si \\(i≠j\\), por lo tanto sólo nos queda \\[ \\pesc{\\sum \\lambda_i \\alpha_i \\gv_i,\\sum\\alpha_i \\gv_i} = \\sum \\lambda_i \\alpha_i^2 \\leq \\lambda_{max} \\left(\\sum \\alpha_i^2\\right) = \\lambda_{max} \\] teniendo en cuenta que \\(\\norm{\\gx} = 1\\) y donde \\(\\lambda_max\\) es el autovalor máximo. Es decir, hemos llegado a que \\[ \\max\\norm{A\\gx} ≤ \\sqrt{\\lambda_{max}} \\] Hemos definido una cota para \\(\\norm{A\\gx}\\). Ahora bien, esa cota se alcanza cuando \\(\\gx\\) es el autovector asociado a \\(\\lambda_{max}\\), por lo tanto la cota es un máximo y la norma de la matriz. Importante el detalle de \\(0 &lt; ||\\gx-\\ga||\\), no es un \\(\\leq\\), porque no se necesita que la función esté siquiera definida en el punto \\(\\ga\\). Idea para el cálculo de límites: Si \\(\\displaystyle\\lim F({t\\gor{v}})\\) toma valores distintos dependiendo de \\(\\gor{v}\\) entonces \\(\\nexists \\displaystyle\\mylim{x}{0}{\\gx}\\) Por otra parte, si \\(\\forall t \\in \\real, \\mylim{x}{a}{t\\gor{v}} = L\\), entonces \\(\\gor{L}\\) es el candidato a ser el límite (no tiene por qué serlo). El siguiente paso sería demostrar con argumentos de comparación (Sandwich) u otros que \\(\\mylim{x}{a}{\\gx} = L\\). El contraejemplo para ver que la existencia del límite por rectas no implica la existencia del límite es estudiar la función \\[ f(x,y) = \\frac{x y^2}{x^2 + y^4}\\] . Veamos por qué. Si os acercamos al límite por medio de rectas: \\[ f(x,y) = f(x,mx) = \\frac{x\\cdot(mx)^2}{x^2 + (mx)^4} = \\frac{m^2x^3}{(1 + x^2m^4)x^2} \\] \\[ \\lim{x\\to 0} (f(x,mx)) \\rightarrow 0, \\forall m \\in \\real \\] Pero si nos acercamos al límite por medio de \\(x = y^2\\) tenemos: \\[ f(x,y) = f(y^2,y) = \\frac{y^2y^2}{y^4+y^4} = \\frac{y^4}{2y^4} = \\frac{1}{2} ≠ 0\\] Por lo tanto, el límite no existe a pesar de que existe cuando nos acercamos por rectas. Aproximación lineal \\(\\sim\\) Diferencial. Matriz jacobiana \\(\\sim\\) Jacobiana. El contraejemplo para demostrar la implicación a la izquierda es el mismo que en los límites a lo largo de rectas. $f(x,y) = { \\[\\begin{matrix} \\displaystyle\\frac{xy^2}{x+y^2} &amp; (x,y) \\neq (0,0) \\\\ 0 &amp; (x,y)=(0,0) \\end{matrix}\\] . $ Según las aplicaciones que tengamos, la diferencial se suele llamar de una u otra forma. Con \\(\\appl{\\delta}{\\real}{\\real^M}\\) utilizamos notación vectorial en vez de matricial porque tendríamos una matriz columna. Por ejemplo: la velocidad (en un instante de tiempo, un punto en el espacio). Consideremos la composición de dos funciones de la siguiente forma: \\(\\appl{F}{\\real^N}{\\real^M}\\). \\(\\appl{G}{\\real^M}{\\real^K}\\). \\(\\appl{H=G\\circ F}{\\real^N}{\\real^K}\\). $ ^N, ^M$ Con \\(F\\) diferenciable en \\(\\ga\\) y \\(G\\) diferenciable en \\(F(\\gor{a})\\). Entonces \\(H=G\\circ F\\) es diferenciable en $$. Además la expresión matricial es: \\[ \\underbrace{DH(\\ga)}_{K\\times N} = \\underbrace{DG(F(\\ga))}_{K\\times M}\\cdot \\underbrace{DF(\\ga)}_{M\\times N} \\] No hace falta calcular toda la matriz si sólo queremos un elemento. Para calcular 1 único elemento de la matriz diferencial (el de la fila \\(i\\), columna \\(j\\)), usamos la siguiente fórmula \\[ \\dpa{H_i}{x_j}(\\ga) = \\sum_{k=1}^M \\dpa{G_i}{y_k}\\cdot\\dpa{F_k}{x_j} \\] Siempre teniendo en cuenta que \\(\\displaystyle\\dpa{G_i}{y_k}\\) está evaluado en \\(F(\\ga)\\) y \\(\\displaystyle\\dpa{F_k}{x_j}\\) está evaluado en \\(\\ga\\). El teorema anterior que vimos del valor medio era para funciones de una variable, y proponía lo siguiente: Después el teorema lo extendimos a funciones de \\(\\real^N\\) en \\(\\real\\): dada \\(\\appl{F}{\\real^N}{\\real}\\) entonces definíamos \\[ \\sigma(t) = t\\gor{b}+(1-t)\\gor{a} \\] y \\(g = F\\circ \\sigma\\) era una aplicación de los reales a los reales, y entonces aplicando el teorema anterior teníamos que \\[ F(\\gor{b}-\\gor{a}) = g(1)-g(0) = g&#39;(s) \\] para algún \\(s\\in[0,1]\\). Sin embargo, al tratar de extrapolar este resultado a una función \\(\\appl{F}{\\real^N}{\\real^2}\\) nos queda que \\[ F(\\gor{b})-F(\\ga) = \\begin{pmatrix} \\pesc{\\nabla F_1(\\gor{c_1}),{\\gor{b}-\\ga}}\\\\ \\pesc{\\nabla F_2(\\gor{c_2}),\\gor{b}-\\ga} \\end{pmatrix} \\] Tenemos 2 \\(c\\) distintos, uno para cada \\(f\\), por lo que este teorema pierde sentido. Hay que buscar una extensión, otra formulación del teorema que nos permita aplicarlo a las funciones que estamos estudiando. Este teorema nos sirve, por ejemplo, para ver que si tenemos \\(\\appl{F}{\\real^N}{\\real^M}, F \\in C^1\\), definida en un conjunto abierto y conexo y \\(DF(\\gx) \\equiv 0\\; \\forall\\gx\\), entonces \\(F\\) es constante. \\(\\appl{F}{\\real^N}{\\real^M}\\) (escalar) \\(\\ga \\sim\\) Recta que pasa por \\(\\ga\\) con dirección \\(\\gor{v}\\). \\(r(t) = \\ga + t\\gor{v}\\). Como una recta tiene infinitos vectores directores (dependiendo de la longitud), siempre tomaremos vectores directores unitarios, con \\(\\norm{\\gor{v}} = 1\\). Vamos a estudiar: \\(g(t) = F(\\ga + t\\gor{v}) = F \\circ r (t)\\). \\(t \\sim 0 \\dimplies \\ga + t\\gor{v} \\sim \\ga\\) La existencia de \\(D_{\\gor{v}}F(\\ga), \\forall \\gor{v}\\in \\real^N\\) NO garantiza que \\(F\\) sea derivable. Si sabemos que \\(F\\) SÍ es diferenciable podemos usar la regla de la cadena obteniendo: \\(D_{\\gor{v}}F(\\ga) = g&#39;(0) = D(F\\circ r)(0) = ... = \\pesc{\\nabla F(\\ga),\\gor{v}}\\) Contraejemplo de la no reciprocidad: \\(f(x) = x^2 sin\\left(\\frac{1}{x}\\right)\\) \\(\\deriv{}{y}\\left(\\deriv{f}{x}\\right) \\equiv \\deriv{^2f}{x \\partial y}\\) Tenemos una función \\(\\appl{F}{\\real^N}{\\real}\\), con \\(F\\in C^k\\) (\\(k\\) veces derivable), y queremos el desarrollo de Taylor de \\(F\\) alrededor de \\(\\ga \\in \\real^N\\). En dimensión 1, el desarrollo era el que sigue \\[ g(x) = g(0) + g&#39;(0)x + \\frac{g&#39;&#39;(0)}{2!}x^2 + ... + \\frac{g^{k)}(0)}{k!}x^k + \\underbrace{\\frac{g^{k+1)}(s)}{(k+1)!}x^{(k+1)}}_{\\text{error}} \\] Tenemos que expandir este desarrollo a más dimensiones. Tomamos \\[ g(t) \\equiv F(t(\\ga + \\gor{h}) + (1-t)\\ga) \\] reduciendo así el cálculo a dimensión 1. Operamos ahora para calcular las derivadas: \\[\\begin{gather*} g&#39;(t) = \\pesc{\\nabla F(a+th),h} = \\sum_{i=1}^N \\deriv{F}{x_i}(a+th)\\cdot h_i \\\\ g&#39;&#39;(t) = \\sum_{i=1}^N\\left(\\sum_{j=1}^N \\deriv{}{x_j}\\deriv{F}{x_i}(\\ga+\\gor{h})\\cdot{h_j}\\right)h_i = \\sum_{i,j = 1}^N \\deriv{^2F}{x_i \\partial x_j}(\\ga+t\\gor{h})h_ih_j \\\\ \\dotsb \\\\ \\frac{g^{s)} (0)}{s!} = \\frac{1}{s!}\\sum_{i_1,i_2,...,i_s=1}^N \\frac{\\partial^s F}{\\partial x_{i_1},x_{i_2},...,x_{i_s}} \\end{gather*}\\] De esta forma, el desarrollo de Taylor de orden \\(k\\) de \\(F\\) en \\(\\ga\\) en general es \\[\\begin{equation} T^k_F(x) = \\sum_{\\alpha = 0}^k \\left( \\frac{1}{\\alpha !} \\sum_{i_1,\\dotsc,i_\\alpha = 0}^N (x_{i_1} - a_{i_1}) \\dotsb (x_{i_\\alpha} - a_{i_\\alpha}) \\frac{\\partial^\\alpha F}{\\partial x_{i_1} \\dotsb \\partial x_{i_\\alpha}} (\\ga) \\right) \\end{equation}\\] Por ejemplo, el desarrollo de Taylor de una función \\(F(x,y)\\) con \\(\\ga = (a,b)\\) queda lo siguiente (con \\(F_{xyz\\dotsc} = \\dfrac{\\partial^nF}{\\partial x \\partial y \\partial z \\dotsb}\\)) \\[\\begin{align*} T^k_{F}(\\gx) &amp;= F(\\ga) \\\\ &amp; + (x - a) F_x(\\ga) + (y - b) F_y(\\ga) \\\\ &amp; +\\frac{1}{2!}\\left[(x-a)^2F_{xx}(\\ga) + 2(x-a)(y-b)F_{xy}(\\ga) + (y-b)^2F_{yy}(\\ga)\\right] \\\\ &amp; +\\frac{1}{3!}\\left[(x-a)^3F_{xxx}(\\ga) + 3(x-a)^2(y-b)F_{xxy}(\\ga) + 3(x-a)(y-b)^2 F_{xyy} (\\ga) + (y-b)^3F_{yyy} (\\ga)\\right] \\\\ &amp; +\\dotsb \\end{align*}\\] Existe una forma más compacta para el desarrollo de Taylor de orden dos usando el producto escalar, el vector gradiente y la matriz hessiana (\\(D^2F\\)) de derivadas segundas: \\[ F(\\gor{a}+\\gor{h}) = F(\\ga) + \\pesc{\\grad F(\\ga),\\gor{h}} + \\frac{1}{2} \\gor{h}^T D^2F(\\ga)\\gor{h} \\] En el mundo lineal tenemos podemos resolver sistemas de dos formas. Con \\(\\appl{F}{\\real^N}{\\real^N}\\) y \\(L(\\gx) = A\\gx\\), siendo \\(A\\) una matriz \\(N\\x N\\); queremos resolver el sistema \\(A\\gx = \\gy\\) sabiendo que \\(A\\gor{0} = \\gor{0}\\). Este sistema tiene solución si y sólo si \\(\\det(A) \\neq 0\\): es la condición para que exista \\(A^{-1}\\). Existe también la posibilidad de tener una función \\(\\appl{F}{\\real^{N+M}}{\\real^N}\\) con \\(L(\\gx) = A\\gx\\), \\(A\\) matriz \\(N\\x (N+M)\\). Para resolver el sistema \\(A\\gx = \\gy\\) parametrizamos \\(M\\) variables. En el mundo , consideramos una función \\(\\appl{F}{\\real^N}{\\real^N}\\). Entonces tenemos un sistema de ecuaciones \\[ \\left.\\begin{matrix} F(x_1,\\dotsc,x_N) = y_1\\\\ F(x_1,\\dotsc,x_N) = y_2\\\\ \\vdots\\\\ F(x_1,\\cdots,x_N) = y_N \\end{matrix} \\right\\} F(\\gx)=\\gy \\] Vamos a intentar resolver este problema utilizando Taylor para aproximar al orden lineal, pero tenemos que pagar un precio: para que taylor funcione tenemos que trabajar cerca del punto. Esto significa que . El teorema de la aplicación contractiva nos sirve, por ejemplo, para comprobar si hay una solución de una ecuación diferencial ordinaria (EDO). \\[\\left.\\begin{matrix}y&#39;(x) = f(x,y(x))\\\\ y(x_0) = y_0 \\end{matrix}\\right\\} \\leftrightarrow y(x) = y_0 + \\int_{x_0}^x f(s,y(s)) ds\\] Podemos definir: \\[\\begin{align*} y_1(x) &amp;= y_0 + \\int_{x_0}^{x} f(s,y_0)ds\\\\ y_2(x) &amp;= y_0 + \\int_{x_0}^x f(s,y_1(s))ds \\equiv T(y_1)\\\\ &amp;\\dots\\\\ y_n &amp;= T(y_{n-1}) = y_0 + \\int_{x_0}^x f(s,y_{n-1}(s))ds\\\\ ¿T(y) &amp;= y? \\end{align*}\\] Aquí es donde entraría la diferencia entre trabajar en \\(\\real^N\\) y un espacio de funciones. Ejercicio propuesto: Aplicar este argumento a iterativo \\(\\left. \\begin{matrix} y&#39; = y\\\\ y(0) = 1 \\equiv y_0 \\end{matrix}\\right\\}\\) En Cálculo I teníamos el siguiente teorema: El teorema nos daba un resultado que asegura que existe la inversa, que es diferenciable y además nos daba su fórmula. Buscamos ahora lo que ocurre en dimensión \\(N\\). Tomamos \\(\\appl{f}{\\real^N}{\\real^N}\\) y supongamos que en algún abierto \\(\\exists F^{-1}\\) y \\(F,F^{-1} \\in C^1\\). Entonces está claro que \\[ (F\\circ \\F)(y) = y \\implies DF(\\F(y))D\\F(y) = Id \\] \\[ (\\F\\circ F)(y) = y \\implies D\\F(F(y))DF(y) = Id \\] Con \\(\\appl{F}{\\Omega\\subset \\real^N}{\\real^N}\\) queremos probar que existe una aplicación \\(\\appl{G}{V}{U}\\) que verifique las siguientes condiciones \\[\\begin{gather*} G\\circ F(\\gx) = \\gx, \\forall\\gx\\in U \\subset \\Omega \\\\ F\\circ G(\\gy) = \\gy, \\forall \\gy \\in V \\\\ G \\text{ diferenciable} \\end{gather*}\\] Sea \\(F(x,y) = (x^2-5y^2,4xy)\\). Tomamos \\((x_0,y_0)\\). ¿\\(F\\) invertible en un entorno de \\(F(x_0,y_0)\\)? Calculamos Df: \\[Df = \\begin{pmatrix} 2x&amp;-10y\\\\ 4y &amp; 4x \\end{pmatrix}\\] \\[\\det(DF) = 8x^2 + 40y^2 \\neq 0 \\text{ si } (x,y) \\neq (0,0)\\] Cuando el determinante sea 0, significa que no puedo aplicar el teorema, por tanto, no sé si la función es invertible o no. Aplicamos los fundamentos. ¿Es inyectiva la función? No es inyectiva en ningún entorno del (0,0). El hecho de que el teorema sea local nos puede llevar a confusiones. Por ejemplo, sea \\[ F (r,\\sigma) = (rcos(\\sigma),r\\sen(\\sigma)), r \\in [0,1], \\sigma \\in [0,4\\pi]\\] Entonces hay dos soluciones para la inversa: \\[ F^{-1} (0,\\frac{1}{2}) = \\left\\{\\begin{matrix}2\\pi+\\frac{\\pi}{2}\\\\\\frac{pi}{2}\\end{matrix}\\right. \\] Lo que hay que hacer es partir de un punto del conjunto de salida. El teorema dice que escogido un punto del conjunto de salida, existe un entorno en el conjunto de llegada en el que se puede definir la función inversa. Otro ejemplo es considerar \\(g(x) = f(x) + \\varepsilon x\\) siendo \\(f(s) =\\left\\{\\begin{matrix}x^2sin\\left(\\frac{1}{x}\\right)&amp; \\text{ si } x\\neq0\\\\0 &amp;\\text{si } x=0\\end{matrix}\\right.\\) Esta función \\(g \\notin C^1\\). \\(g\\) es derivable (\\(g&#39;(0) = \\varepsilon&gt;0\\)) pero la derivada no es continua. Se deja como ejercicio para el lector la comprobación. Tomemos el caso particular de una superficie en \\(\\real^3\\). Puede venir dada de dos formas: ¿Existe alguna forma de expresar \\(F(x,y,z)\\) de la forma \\(z=f(x,y)\\)? Pensando en el ejemplo de la esfera: \\(F(x,y,z) = x^2+y^2+z^2+1\\), se puede expresar de dos formas: \\[ z = \\pm \\sqrt{1 - x^2 - y^2} \\] ¿Cuál es la condición que necesitamos para poder despejar \\(z\\)? Supongamos que sabemos despejar \\(z=f(x,y)\\), entonces tenemos: \\(F(x,y,f(x,y)) = 0\\). Derivando implíctamente \\[ \\underbrace{\\frac{\\partial}{\\partial x} [F(x,y,f(x,y)]}_{\\dpa{F}{x} + \\dpa{F}{z}\\cdot\\dpa{f}{x}}= \\underbrace{\\frac{\\partial}{\\partial y} [F(x,y,f(x,y)]}_{\\dpa{F}{y} + \\dpa{F}{z}\\cdot\\dpa{f}{y}} = 0 \\] Si \\(f\\) es diferenciable tenemos: \\[\\dpa{f}{x}(x,y) = - \\frac{\\dpa{F}{x}(x,y,f(x,y))}{\\dpa{F}{z}(x,y,f(x,y))}\\] \\[\\dpa{f}{y}(x,y) = - \\frac{\\dpa{F}{y}(x,y,f(x,y))}{\\dpa{F}{z}(x,y,f(x,y))}\\] Necesitamos entonces que \\(\\displaystyle\\dpa{F}{z}\\left(x,y,f(x,y)\\right) \\neq 0\\). Veamos cómo extrapolar esto de forma general con tres variables. Estudiar si es posible despejar \\(u(x,y,z), v(x,y,z)\\) en las ecuaciones: \\[\\left\\{\\begin{matrix} xy^2+xzu+yv^2 &amp;= 3\\\\ xyu^3+2xv-u^2v^2 &amp;= 2\\end{matrix}\\right.\\] En un entorno de \\((x,y,z) = (1,1,1)\\) Vamos a tener que definir una \\[\\appl{F}{\\real^5}{\\real^2}\\] \\[(x,y,z,u,v) \\rightarrow F(x,y,z,u,v)= (xy^2+xzu+yv^2-3,xyu^3+2xv-u^2v^2-2)\\] Podemos comprobar fácilmente que \\(F\\in C^{\\infty}\\) y \\(F(1,1,1,1,1) = ... = (0,0)\\). Para poder despejar u,v tenemos que evaluar \\(D_{(u,v)}F\\) en \\((1,1,1,1,1)\\). \\[D_{(u,v)} = \\begin{pmatrix} \\dpa{F_1}{u}&amp;\\dpa{F_1}{v}\\\\ \\dpa{F_2}{u} &amp;\\dpa{F_2}{v}\\end{pmatrix} = \\begin{pmatrix} xz &amp; 2yv\\\\3xyu^2-2uv^2 &amp; 2x-2u^2v \\end{pmatrix} = ... = \\begin{pmatrix} 1&amp;2\\\\ 3&amp;0 \\end{pmatrix}\\] Tenemos \\(\\det D_{(u,v)} = -6 \\neq 0\\). Entonces estamos en las hipótesis para utilizar el teorema y garantizar que en un entorno del punto \\((1,1,1)\\) blablabla. COMPLETAR Vamos a calcular (porque lo pide el enunciado) \\[\\dpa{u}{x},\\dpa{v}{x},\\dpa{v}{z}\\]. Como el teorema garantiza que existe, derivamos implícitamente: Vamos a derivar implícitamente respecto a \\(x\\) el sistema: \\[\\left\\{\\begin{matrix} y^2+zu+xzu_x + y 2v v_x = 0 \\\\ yu^3+xy3u^3u_x + 2v + 2xv_x - 2uu_xv^2-2u^2vv_x = 0 \\end{matrix}\\right.\\] Donde \\(u_x = \\dpa{u}{x}\\). si \\((x,y,z) = (1,1,1) \\implies (u,v) = (1,1)\\) Sustiuyendo: \\[\\left\\{\\begin{matrix}1+1+u_x+2v_x &amp;= 0 \\\\ 1+3u_x+2+2v_x-2u_x-2v_x &amp;= 0\\end{matrix}\\right.\\] \\[\\left\\{\\begin{matrix}u_x(1,1,1) + 2v_x(1,1,1) &amp;= -2\\\\ u_x(1,1,1) &amp;= -3 \\end{matrix}\\right.\\] Faltaría calcular \\(\\dpa{v}{z}\\) Demostrar T.F.Inversa a partir del T.F Implícita Tenemos: \\[\\begin{gather} \\appl{F}{\\real^N}{\\real^N}\\\\ F\\in C^1\\\\ F(\\ga) = \\gb\\\\ \\det DF(\\ga) \\neq 0 \\end{gather}\\] ¿Podemos despejar \\(F(\\gx) = \\gy\\) para \\(\\gx\\) en un entorno de \\(\\ga\\) \\(\\gy\\) en un entorno de \\(\\gb\\)? \\(F(\\gx) = \\gy\\) es lo mismo que \\(H(\\gx,\\gy) = 0\\) con \\(H(\\gx,\\gy) = F(\\gx)-\\gy\\). \\(\\appl{H}{\\real^N\\times \\real^N}{\\real^N}. H\\in C^1\\) y \\(H(\\ga,\\gb) = 0\\). Tenemos el punto de partida en el que anclar el teorema. Queremos hallar \\(\\gx\\) como función de \\(\\gy\\) en la ecuación \\(H(\\gx,\\gy) = 0\\). Necesitamos para aplicar el teorema: \\(\\det D_x H(\\gx,\\gb) \\neq 0\\). \\(D_x H = D_x F \\neq 0\\) (por (4)) \\(\\exists f(\\gy) \\tlq H(f(\\gy),\\gy) = \\gor{0} \\equiv F(f(\\gy)) - \\gy = \\gor{0} \\equiv F(f(\\gy)) = \\gy\\). Ahora tenemos que ver que la composición en el otro sentido también nos da la identidad. \\[f(F(\\underbrace{f(\\gy)}_{v}) = f(\\gy) \\implies f(F(v)) = v\\] ¿Dónde está escondida la indentidad en un sistema de ecuaciones (lineales o no)? \\[\\left\\{\\begin{array}{cc} F_1\\equiv x^2+z^2+2xz-2x-2z+1&amp;=0\\\\ F_2\\equiv x^2+4y^2+4z^2+4xy+4xz+8xy&amp;=0 \\end{array}\\right.\\] ¿Es depejable en función de z en un entorno de \\((x,y) = (0,-1), z=1\\)? Solución: Ver que el punto safisface las ecuaciones. Ese sistema es como definir: \\[\\appl{F}{\\real^3}{\\real^2}, F\\in C^1, F(0,-1,1) = (0,0)\\] Calculamos el determinante de \\(DF(0,-1,1)\\) y vemos que da \\(0\\). Conclusión: no podemos aplicar el teorema, pero pensando un poco vemos que \\[F_1 = (x+z)^2 -2(x+z) + 1 = (x+z-1)^2\\] \\[F_1 = 0 \\dimplies x=1-z\\] \\[F_2(x,y,z) = F_2(1-z,y,z) = (1-z)^2 + 4y^2 + 4z^2 + 4(1-z)y + 4(1-z)z+8yz\\] \\[ =... = a(z)y^2+b(z)y+c(z) \\implies y=\\frac{-b\\pm \\sqrt{...}}{2a}\\] ¿Cual de las 2 soluciones escoger? Sabemos (por el enunciado) que si \\(z=1\\) entonces \\(y=-1\\). Escogeremos la solución a la que dandole el valor \\(z=1\\) nos de \\(-1\\). \\[\\begin{array}{cc} x^3+z^3y^3+z &amp;= 0\\\\ cos(xyz)+sen(z)-1 = 0 \\end{array}\\] ¿Es despejable en función de \\(z\\) en un entorno de \\((x,y) = (0,0), z=0\\). Solución: El punto es solución del sistema. Definimos \\[\\appl{F}{\\real^3}{\\real^2}, F\\in C^1, F(0,0,0)=(0,0)\\] Calculamos el determinante de \\(DF(0,0,0)\\) y vemos que da \\(0\\). Conclusión: no podemos aplicar el teorema\\ Supongamos que si se puede despejar. Entonces tendríamos algo de la forma: \\[\\begin{gather*} \\left\\{\\begin{array}{cc} [x(z)]^3+z^3[y(z)]^3+z &amp;= 0\\\\ cos(x(z)y(z)z)+sen(z)-1 &amp;= 0 \\end{array}\\right\\}\\rightarrow\\\\ \\,\\\\ \\left\\{ \\begin{array}{cc} 3[x(z)]^3x&#39;(z) + 3z^2[y(z)]^2 + z^32y(z)y&#39;(z) + 1 &amp;=0\\\\ -sen(x(z)y(z)z) \\cdot\\{...\\} + cos(z) &amp;= 0 \\end{array}\\right\\} \\equiv\\\\\\,\\\\ \\left\\{ \\begin{array}{cc} 0+1&amp;=0\\\\ 0+1&amp;=0 \\end{array}\\right. \\end{gather*}\\] Esto demuestra que no pueden existir las derivadas. 7.1 Definición y ejemplos 7.2 El teorema del valor medio 7.3 Funciones convexas 7.4 Funciones inversas 7.5 Regla de L’Hospital 7.6 Teorema de Taylor en \\(\\mathbb{R}\\) 7.7 Método de Newton "],
["integracion-de-riemann.html", "Capítulo 8 Integración de Riemann 8.1 Integral de Riemann-Darboux 8.2 Propiedades de la integral 8.3 Evaluación de la integral 8.4 Fórmula de Stirling 8.5 Teoremas del valor medio, versión integral 8.6 Estimación de la integral 8.7 Integrales impropias 8.8 La integrabilidad según Riemann 8.9 Funciones a variación acotada 8.10 La integral de Riemann-Stieltjes", " Capítulo 8 Integración de Riemann En dimensión 1, la teoría de integración la basábamos en la integral de Riemann. Tomábamos una partición de \\([a,b]\\): \\(\\mathcal{P} = a = t_0 &lt; t_1&lt;...&lt;t_k = b\\) y a cada subintervalo de esa partición \\(I_i =[t_i,t_{i+1}]\\) asignábamos dos pares de valores, \\(M_i\\) y \\(m_i\\): \\[\\begin{gather*} M_i = \\sup \\{f(x)\\tq x\\in I_i\\}\\\\ m_i = \\inf \\{f(x)\\tq x\\in I_i\\} \\end{gather*}\\] Definíamos las sumas superiores (\\(\\mathbb{U}\\)) e inferiores (\\(\\mathbb{L}\\)): \\[\\begin{gather*} \\mathbb{U}(f,\\mathcal{P}) = \\sum_{i=0}^{k-1} M_i(t_{i+1}-t_i) \\\\ \\mathbb{L}(f,\\mathcal{P}) = \\sum_{i=0}^{k-1} m_i(t_{i+1}-t_i) \\end{gather*}\\] Esto nos da una aproximación al área debajo de la función como una suma de rectángulos. Tal y como los hemos definido, está claro que \\[ \\mathbb{L}(f,\\mathcal{P}) ≤ \\int_a^b f ≤ \\mathbb{U}(f,\\mathcal{P}) \\] También parece obvio que, cuanto más pequeña sea la anchura de los intervalos, más se aproximarán las sumas superiores e inferiores al valor real. Definimos entonces \\[\\begin{gather*} \\alpha = \\lim_{\\abs{\\mathcal{P}}\\to 0} \\mathbb{U}(f,\\mathcal{P}) \\\\ β = \\lim_{\\abs{\\mathcal{P}}\\to 0} \\mathbb{L}(f,\\mathcal{P}) \\end{gather*}\\] , donde \\(\\abs{\\mathcal{P}} = \\max t_{i+1} - t-i\\), es decir, la anchura máxima de los intervalos que formarn \\(\\mathcal{P}\\). A partir de aquí podemos llegar a la definición de función integrable: El recíproco es . Se ve fácilmente si \\(f\\) es una función escalonada, por ejemplo: podemos calcular el área debajo de ella sin problemas pero no es continua. En dimensión mayor que uno teníamos una serie de generalizaciones, y empezábamos con el teorema de Fubini que nos permite el cambio en el orden de integración: Por ejemplo, si tenemos la siguiente integral (ver figura ) \\[ I_1 = \\int_0^2\\int_{y^2}^{2y} f(x,y)\\,dx\\,dy \\] y queremos invertirla cambiamos los límites de integración: \\[ I_1 = \\int_0^4 \\int_{\\frac{x}{2}}^{\\sqrt{x}} f(x,y)\\, dy\\,dx \\] Otro ejemplo: estudiemos \\[ I_2 = \\int_a^bf(x)\\,dx = \\int_a^b\\int_0^{f(x)}dx\\,dy \\]. Si probamos a cambiar el orden de integración, tenemos que \\[ I_2 = \\int_0^M\\int_{A(y)}dx\\,dy \\] Donde \\(M\\) es el máximo de \\(f\\) y \\(A(y) = \\{ x\\in [a,b] \\tq f(x) ≥ y \\}\\). La longitud de ese conjunto \\(A\\) se denomina la medida, y entonces podemos expresar \\[ I_2 = \\int_0^\\infty \\abs{\\{x \\tq f(x) &gt; y\\}}\\,dy \\] ya que la medida del conjunto será \\(0\\) cuando \\(y &gt; M\\). Curiosamente, hemos pasado de una integral de Riemann a otra con una expresión distinta, la llamada . Esto pertence al campo de la , y permite estudiar conjuntos extraños y más monstruos y engendros varios. En general, podemos expresar nuestro cambio de variable de forma más general para cualquier cambio de variable: Para calcular la longitud aplicamos las ideas básicas del cálculo integral: el intervalo \\([a,b]\\), y aproximamos cada uno de esos trozos por un segmento. Calculamos la suma de la longitud de esos segmentos, hacemos tender la anchura de los y si converge, la longitud se puede medir. Este teorema responde a una idea con respecto al cambio de variable: si tomamos \\(\\Gamma\\) como la curva que queremos integrar, entonces se puede expresar \\[ L(\\sigma) = \\int_\\Gamma 1\\,d\\sigma = \\int_a^b \\md{\\sigma&#39;(t)}\\,dt \\] donde \\(\\md{\\sigma&#39;(t)}\\) es el cambio en la medida correspondiente. en general si la curva es sólo continua \\[\\appl{\\sigma}{[0,1]}{\\real^2}\\] Descripción gráfica: entre \\(\\frac{1}{2^k}\\) y \\(\\frac{1}{2^{k+1}}\\) y formo un triángulo isósceles con esos 2 puntos y de altura hasta la bisectriz. Sea el triangulo K: \\[Longitud =L_k = ... ??? ... = \\frac{1}{2k+1} \\{\\sqrt{\\frac{1}{4k^2}+1} + \\sqrt{\\frac{1}{4k^2+4} +1} \\} \\ge \\frac{1}{2k+1}\\] \\[\\text{Longitud total } = \\sum L_k \\ge \\sum_k \\frac{1}{2k+1} = \\infty\\] Este mostruito no tiene longitud. ¿Y si en vez de rectas tomamos sinusoides? Esa función si debería ser \\(C^1\\) pero la longitud es \\(\\infty\\). Entonces la curva no puede ser \\(C^1\\) (sería un contrajemplo del teorema) \\[ \\appl{\\sigma}{[a,b]}{\\real^n} \\] con \\(\\sigma\\) curva \\(C^1\\) y regular. Entonces \\[ L(s) = \\int_a^s \\md{\\sigma&#39;(t)}\\,dt \\] y por el Teorema Fundamental del Cálculo \\[ L&#39;(s) = \\md{\\sigma&#39;(s)} \\] Al imponer que la curva sea regular, entonces \\(\\sigma&#39;(s)\\neq 0\\) y por lo tanto existe la inversa \\(\\inv{L}\\). Si tenemos entonces un \\(\\tau ∈ [0,L(b)]\\), entonces \\(S=\\inv{L}(\\tau) ∈ [a,b]\\). Definimos \\[\\begin{gather*} \\sigma^\\ast = \\sigma \\circ \\inv{L} \\\\ \\sigma^\\ast = \\sigma(\\inv{L}(\\tau)) \\\\ (\\sigma^\\ast)&#39;(\\tau) = \\sigma&#39;(\\inv{L}(tau)) (\\inv{L}(\\tau))&#39; = \\sigma&#39;(\\inv{L}(\\tau)) \\frac{1}{L&#39;(\\inv{L}(\\tau))} = \\sigma&#39;(s) \\frac{1}{L&#39;(s)} = \\frac{\\sigma&#39;(s)}{\\md{\\sigma&#39;(s)}} \\end{gather*}\\] Es decir, hemos conseguido una parametrización con velocidad constante \\(\\md{(\\sigma^\\ast)&#39;} = 1\\) y por lo tanto \\[ L(\\tau) = \\int_0^\\tau \\md{(\\sigma^\\ast)&#39;}\\,ds = \\int_0^s 1\\,ds = s \\] Supongamos que estamos en \\(\\real^3\\). Podemos hablar de la longitud de una variedad de dimensión 1, del área de una de dimensión 2 o del volumen de una de dimensión 3. Ahora bien, ¿qué ocurre cuando pasamos a dimensiones superiores? La denominación será la siguiente Para calcular esas empezaremos partiendo del área de un parelelepípedo. Definiremos el paralelepípedo como, dados \\(k\\) vectores independientes \\(\\{\\gv_i\\} \\subset \\real^N\\), \\[ P_k = \\sum_{i=1}^k \\lambda_i\\gv_i\\;\\lambda_i \\in [0,1] \\] Elemento de área: \\(P_k \\equiv \\{ \\sum_{i=1}^k \\lambda_i\\gor{v}_i, \\lambda_i \\in [0,1]\\}\\subset \\real^N\\) Definimos una \\[\\begin{gather*} \\appl{\\Psi}{\\real^N}{\\real^N}\\\\ \\gx \\longrightarrow (\\Psi_1(\\gx),...,\\Psi_n(\\gx)) \\end{gather*}\\] Si \\(\\gx,\\gy\\in P_k \\implies \\pesc{\\gx,\\gy} = \\underbrace{\\pesc{\\Psi(\\gx),\\Psi(\\gy)}}_{\\text{Prod } \\real^N} = \\underbrace{{\\tilde{\\Psi}(\\gx),\\tilde{\\Psi}(\\gy)}}_{\\text{Prod } \\real^K}\\) Área \\((P_k)\\) = Área \\((\\tilde{\\Psi}(P_k)) \\equiv \\displaystyle \\int_{\\tilde{\\Psi}(P_k)}d\\gor{s}\\), una integral en \\(\\real^K, K&lt;N\\) El paso que vamos a dar ahora es: Construir una aplicación \\(L\\) tal que \\(L(\\gor{e}_i) = \\tilde{\\Psi}(\\gor{v}_i)\\) Y aplicamos el cambio de variables: \\[\\int_{\\tlps(P_k)} d\\gor{s} = \\int_{[0,1]^K} \\abs{\\det\\left( \\dpa{s}{t} \\right)} d\\gor{t}\\] Tenemos: Con la segunda propiedad podemos construir la aplicación \\(L\\). \\[\\begin{gather*} L = \\begin{pmatrix} \\tlps(\\gv_1) &amp; \\tlps(\\gv_2) &amp; ... &amp; \\tlps(\\gv_k)\\\\ \\downarrow &amp; \\downarrow &amp; \\ddots &amp; \\downarrow \\end{pmatrix} = \\frac{d\\gor{s}}{d\\gor{t}}\\end{gather*}\\] \\[Area(P_k) = \\abs{\\det L} = \\left|\\det \\begin{pmatrix} \\tlps(\\gv_1) &amp; \\tlps(\\gv_2) &amp; ... &amp; \\tlps(\\gv_k)\\\\ \\downarrow &amp; \\downarrow &amp; \\ddots &amp; \\downarrow \\end{pmatrix}\\right|\\] Pero… tenemos 2 problemas: Vamos a reventar 2 pajaros de un tiro. \\[ Area(P_k) = \\left( \\det \\begin{pmatrix} \\tlps(\\gv_1) &amp; \\tlps(\\gv_2) &amp; ... &amp; \\tlps(\\gv_k)\\\\ \\downarrow &amp; \\downarrow &amp; \\ddots &amp; \\downarrow \\end{pmatrix} \\begin{pmatrix} \\tlps(\\gv_1) &amp; \\tlps(\\gv_2) &amp; ... &amp; \\tlps(\\gv_k)\\\\ \\downarrow &amp; \\downarrow &amp; \\ddots &amp; \\downarrow \\end{pmatrix} ^T \\right)^{\\frac{1}{2}}\\] ¿Qué ganamos? \\[\\begin{pmatrix} \\pesc{\\tlps(\\gv_1),\\tlps(\\gv_1)} &amp; \\pesc{\\tlps(\\gv_1),\\tlps(\\gv_2)} &amp; ... &amp; \\pesc{\\tlps(\\gv_1),\\tlps(\\gv_k)}\\\\ \\pesc{\\tlps(\\gv_2),\\tlps(\\gv_1)} &amp; \\pesc{\\tlps(\\gv_2),\\tlps(\\gv_2)} &amp; ... &amp; \\pesc{\\tlps(\\gv_2),\\tlps(\\gv_k)}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ \\pesc{\\tlps(\\gv_k),\\tlps(\\gv_1)} &amp; \\pesc{\\tlps(\\gv_k),\\tlps(\\gv_2)} &amp; ... &amp; \\pesc{\\tlps(\\gv_k),\\tlps(\\gv_k)} \\end{pmatrix} = (\\ast) = \\left(\\det(\\pesc{v_i,v_j})\\right)^{\\frac{1}{2}}\\] (\\(\\ast\\)) Hemos construido las cosas de tal manera que se mantiene el producto escalar. \\(P_2 \\subset \\real^3\\) generado por \\(\\gu,\\gv\\in\\real^2\\) \\(Area(P_2) = \\left(\\det \\begin{pmatrix} \\pesc{\\gu,\\gu} &amp; \\pesc{\\gu,\\gv}\\\\ \\pesc{\\gv,\\gu} &amp; \\pesc{\\gv,\\gv} \\end{pmatrix} \\right)^{\\frac{1}{2}} = \\left(\\pesc{\\gu,\\gu}\\pesc{\\gv,\\gv} - \\pesc{\\gu,\\gv}^2\\right)^{\\frac{1}{2}} =\\left(\\md{\\gu}\\md{\\gv} - \\md{\\gu}\\md{\\gv}(cos(\\theta)^2)\\right)^2 = \\md{u}\\md{v} \\sqrt{1-cos^2(\\theta)} = \\md{u}\\md{v} sen(\\theta) = ||u\\times v||\\) Bueno, esto cumple lo que la sabíamos de selectividad. El área de un paralelogramo 2 dimensional en \\(\\real^3\\) es la raiz cuadrada del módulo del producto vectorial. Vamos a aplicar esto para hallar el área de una \\(k-variedad\\) en \\(\\real^N\\). \\(Area(\\Phi(Q_k)) = Area(P_k) + err(h)\\) donde \\(Area(P_k) = \\left(\\det (\\pesc{v_i,v_j})\\right)^{\\frac{1}{2}}, \\{v_j\\}\\) genera \\(P_k\\). \\[\\left(\\det\\left(\\pesc{\\dpa{\\Phi}{s_i},\\dpa{\\Phi}{s_j}}h_ih_j\\right)\\right)^{\\frac{1}{2}}\\] Tomamos \\(h_i = h, \\forall i\\) \\[\\underbrace{\\underbrace{h^k}_{Area(Q_k)} \\left(\\det\\left(\\pesc{\\dpa{\\Phi}{s_i},\\dpa{\\Phi}{s_j}}\\right)\\right)^{\\frac{1}{2}}}_{Area(P_k)}\\] Conclusión: \\[Area(M) = \\sum_n Area(Q_k^n) \\left(\\det\\left(\\pesc{\\dpa{\\Phi}{s_i}(\\gs_0^n),\\dpa{\\Phi}{s_j}(\\gs_0^n)}\\right)\\right)^{\\frac{1}{2}} + err(h)\\] Si llamamos \\(F(s_0^n) = \\left(\\det\\left(\\pesc{\\dpa{\\Phi}{s_i}(\\gs_0^n),\\dpa{\\Phi}{s_j}(\\gs_0^n)}\\right)\\right)^{\\frac{1}{2}}\\) Tenemos: \\[\\sum_n Area(Q_k) F(s_0^n) \\convs[Riemann] \\int_D F(\\gs)d\\gs\\] \\(Area(M) = \\int_D \\left(\\det\\left(\\pesc{\\dpa{\\Phi}{s_i},\\dpa{\\Phi}{s_j}}\\right)\\right)^{\\frac{1}{2}}\\) Lo que en Cálculo 2 llamábamos \\(\\md{T_u\\times T_v}\\). Dada una \\(\\appl{f}{\\real^N}{\\real}\\) podemos definir: \\[\\int_{\\Phi(D)}f dA = \\int_{D}f(\\Phi(s)) \\left(\\det(\\pesc{\\Phi_{s_i},\\Phi_{s_j}})\\right)^{\\frac{1}{2}} ds\\] \\[\\appl{\\sigma}{[a,b]}{\\real^N}\\] Sea \\(\\Gamma = \\{\\sigma(t)\\in\\real^N\\tq t\\in[a,b]\\}\\) El elemento de área sería: \\[\\int_{\\Gamma} fdA = \\int_a^bf(\\sigma(t)) (\\pesc{\\sigma&#39;,\\sigma&#39;})^{\\frac{1}{2}}\\] \\[ \\Phi : D\\subset\\real^2\\rightarrow \\real^3\\] \\[(s_1,s_2)\\rightarrow (x(s_1,s_2),y(s_1,s_2),z(s_1,s_2))\\] Tenemos en este caso: \\[\\int_{\\Phi(D)}fdA = \\int_D f(\\Phi(s_1,s_2)) (\\det(\\pesc{\\Phi_{s_i},\\Phi_{s_j}}))^{\\frac{1}{2}})ds\\] \\[(\\det(\\pesc{\\Phi_{s_i},\\Phi_{s_j}}))^{\\frac{1}{2}}) = (*) = \\md{\\Phi_1 \\times \\Phi_2}\\] \\((*)\\) visto anteriormente. Integrales de sobre y en \\(\\real^3\\). Volvemos a plantearnos el problema: ¿Distintas parametrizaciones nos da el mismo trabajo/flujo? Depende de la de la parametrización. No es lo mismo el trabajo para ir desde abajo de la curva hasta arriba que para bajarla. ¿Este problema traducido a \\(\\real^K\\), flipas o k ase? Recordamos algunos teoremas y definiciones vistos en Calculo 2: Los campos vectoriales son funciones \\(\\appl{F}{\\real^N}{\\real^N}\\), en el que a cada punto se le asigna un vector. Todos estos teoremas son parecidos en cuanto a que a la izquierda se tiene el campo evaluado en la frontera y a la derecha tenemos una integral en el interior de una expresión más o menos compleja en la que aparecen las derivadas. Cabe esperar que sean casos particulares de un teorema superior, cosa que es cierta. Este teorema se le llama de en general. De aquí al final de curso nos dedicaremos a llegar a ese teorema y ver que estos 3 teoremas son casos particulares. Para ello nos adentraremos en el espinoso jardín de la orientación. El tema de la orientación tiene que ver con tener cuidado con el orden. Vamos a ver los ejemplos de pocas dimensiones: \\[\\mathcal{B}_1 = \\{(0,1),(1,0)\\}\\] \\[\\mathcal{B}_2 = \\{(1,0),(0,1)\\}\\] Sea \\(\\gv = (v_1,v_2)_{\\mathcal{B}_1} = (v_2,v_1)_{\\mathcal{B}_2} \\neq \\gv\\) Lo que haremos en este caso será un cambio de base, \\(\\mathcal{B}_1,\\mathcal{B}_2\\) bases en \\(\\real^N\\). Sea \\(\\mathcal{C}_{\\mathcal{B}_1\\to\\mathcal{B}_2}\\) matriz del cambio de base de \\(\\mathcal{B}_1\\) a \\(\\mathcal{B}_2\\). ¿Que pasaría si el determinante de esa matriz de cambio de base sea 0? No puede ser (por definición de cambio de base, que tiene que ser reversible). \\(\\real^3\\). Sean \\[\\mathcal{B}_1 = (\\gu,\\gv,\\gw),\\mathcal{B}_2 = (\\gv,\\gu,\\gw),\\mathcal{B}_3 = (\\gw,\\gu,\\gv)\\] Si tomamos $= \\[\\begin{pmatrix} 1\\\\0\\\\0\\end{pmatrix}\\] _{_1} = \\[\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}\\] _{_2} = \\[\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}\\] _{_3} $ Siguiendo este razonamiento llegamos a la matriz de cambio de base: \\[\\mathcal{C}_{\\mathcal{B}_1\\to\\mathcal{B}_2} = \\begin{pmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}\\] Cuyo determinante es negativo. Lo mismo con \\[\\mathcal{C}_{\\mathcal{B}_1\\to\\mathcal{B}_1} = \\begin{pmatrix} 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{pmatrix}\\] Cuyo determinante es positivo. ¿Porqué al cambiar el orden se cambia la orientación? Aceptamos como orientación positiva la de la base canónica ordenada como Dios manda: \\(\\{(1,0,0,...,0), (0,1,0,0,...,0), (0,0,1,0,0,...,0),...,(0,0,...,0,1)\\}\\) Vamos con algún ejemplo más: (Importante) En \\(\\real^3\\), sean \\(\\gu,\\gv\\) independientes. Construimos \\(\\mathcal{B} = \\{\\gu,\\gv,\\gu\\times\\gv\\}\\). Queremos saber si esta base es de orientación positiva u orientación negativa. \\[\\gu\\times\\gv = \\left| \\begin{array}{ccc} \\overrightarrow{i} &amp; \\overrightarrow{j} &amp; \\overrightarrow{k}\\\\ u_1 &amp; u_2 &amp; u_3 \\\\ v_1 &amp; v_2 &amp; v_3 \\end{array}\\right| = ... = \\left(\\left|\\begin{array}{cc}u_2&amp;v_2\\\\u_3&amp;v_3\\end{array}\\right|,\\left|\\begin{array}{cc}u_1&amp;v_1\\\\u_3&amp;v_3\\end{array}\\right|,-\\left|\\begin{array}{cc}u_1&amp;v_1\\\\u_2&amp;v_2\\end{array}\\right|\\right)\\] [_{} = \\begin{pmatrix} u_1&amp;v_1&amp; |\\begin{array}{cc} u_2&amp;v_2\\u_3&amp;v_3 \\end{array}|\\ u_2&amp;v_2&amp; -|\\begin{array}{cc} u_1&amp;v_1\\u_3&amp;v_3 \\end{array}|\\ u_3&amp;v_3&amp; |\\begin{array}{cc} u_1&amp;v_1\\u_2&amp;v_2\\end{array}| \\end{pmatrix}] Cuyo determinante es \\[\\left|\\begin{array}{cc}u_2&amp;v_2\\\\u_3&amp;v_3\\end{array}\\right|^2+\\left|\\begin{array}{cc}u_1&amp;v_1\\\\u_3&amp;v_3\\end{array}\\right|^2+\\left|\\begin{array}{cc}u_1&amp;v_1\\\\u_2&amp;v_2\\end{array}\\right|^2&gt;0\\] \\(\\real^2\\) Sean \\[\\mathcal{C} = \\{(1,0),(0,1)\\}\\] \\[\\mathcal{B} = \\{(u_1,u_2),(v_1,v_2)\\}\\] La matriz fácil de calcular es \\[C_{\\mathcal{B}\\to \\mathcal{C}} = \\begin{pmatrix} u_1&amp;v_1\\\\u_2&amp;v_2\\end{pmatrix}\\] \\[0&lt;\\det \\begin{pmatrix} u_1&amp;v_1\\\\u_2&amp;v_2\\end{pmatrix} = \\pesc{(u_1,u_2),(v_1,v_2)} = \\pesc{(\\lambda,0),(\\beta,-\\alpha)} = \\lambda\\beta\\] Aplicando un giro (de aquí salen \\(\\lambda\\)…) podemos ver que el si el ángulo entre \\(\\gu,\\gv \\in (0,\\pi) \\leadsto +\\) pero si el ángulo entre \\(\\gu,\\gv \\in (\\pi,2\\pi) \\leadsto -\\). ¿Y en \\(\\pi\\)? Entonces los vectores no serían independientes. Vamos a dedicarnos a jugar con los vectores de la base aplicandoles una deformación continua para ver que pasa con la discontinuidad de la orientación en \\(\\pi\\) a ver que encontramos. Sea la deformación \\((\\alpha(t),\\beta(t)), t\\in(0,1)\\) continua, donde \\((\\alpha(0),\\beta(0)) = (e_1,e_2)\\) y \\((\\alpha(1),\\beta(1)) = (e_1,e_2)\\) en \\(\\real^3\\). Idea geométrica: Yo tengo 2 bases y quiero pasar de una a otra, si puedo encontrar un camino en el que los 3 vectores nunca pertenezcan al mismo plano entonces la orientación es la misma. . Sea \\[\\mathcal{B} = \\{e_1,e_2,e_3\\} \\hspace*{150pt} \\mathcal{C} = \\{\\gu,\\gv,\\gw\\}\\] No hay ninguna manera de llevar \\(e_3\\) a \\(\\gw\\) sin formar un único plano con los tres vectores. Aunque podríamos llevar \\(e_3 \\to \\gu\\) y \\(e_1 \\to \\gw\\)… ¿Entonces? Mal! Porque no estamos manteniendo el orden, tendríamos la base \\(\\mathcal{C}* = \\{\\gw,\\gv,\\gu\\}\\neq\\mathcal{C}\\). Ahora que ya tenemos vista la orientación de bases en \\(\\real^N\\) vamos a ver la orientación en \\(T_a M\\) (espacio tangente de una variedad en un punto) Sea \\(\\{e_1,e_2,\\dotsc,e_k\\}\\) la base canónica en \\(\\real^k\\), y \\(\\Phi(u_0)=\\ga\\) Como la variedad viene dada por una parametrización, tomamos \\(\\img D\\Phi\\), es decir: \\[T_{\\ga}M = \\img D\\Phi(u_0) = \\{D\\Phi(u_0)\\gv,\\gv\\in\\real^K\\}\\] Tomando la base euclídea: \\[D\\Phi(u_0) \\{\\sum^k v_i\\gor{e}_i\\}\\equiv \\sum^k v_i\\underbrace{[D\\Phi(u_0)\\gor{e}_i]}{\\in\\real^N} \\] Es decir, la \\(\\img D\\Phi\\) está generada por los k vectores \\[\\begin{equation}\\label{baseTaM} \\{ D\\Phi(u_0)\\gor{e}_1,\\dotsc ,D\\Phi(u_0)\\gor{e}_k \\} \\end{equation}\\] La condición de rango máximo nos asegura que los \\(k\\) vectores son linealmente independientes. El conjunto () es una base de \\(T_{\\ga}M\\) Si tenemos 2 parametrizaciones distintas \\(\\Phi,\\Psi\\) entonces tendremos 2 bases diferentes. Cabe plantearse bajo qué condiciones se mantiene la orientación. (aunque todavía no tengamos muy claro que es la orientación de una variedad) Las orientaciones de una curva en \\(\\real^N\\) es fácil, basta comprobar los sentidos de los vectores tangentes. Vamos a hacer las cuentas: Sean \\(\\sigma,\\beta\\) parametrizacones del mismo trozo de la curva, que parten de orígenes distintos. \\[\\appl{\\sigma}{U}{\\Gamma},s\\to \\sigma(s)\\] \\[\\appl{\\beta}{V}{\\Gamma},t\\to \\beta(t)\\] Tenemos un teorema que nos garantiza que existe un ()g, tal que \\[\\sigma(s) = \\beta(g(s)) \\y \\sigma&#39;(s) = \\beta&#39;(g(s))\\cdot g&#39;(s)\\] En este caso, para que las dos parametrizaciones den la misma orientación, \\(g&#39;&gt;0\\). Las orientaciones de una superficie también, basta comprobar los sentidos de los vectores normales. \\[\\appl{\\Phi}{U}{\\real^3}, (u,v) \\to \\Phi(u,v)\\] \\[\\appl{\\Psi}{V}{\\real^3}, (r,s) \\to \\Psi(r,s)\\] Tenemos un teorema que nos garantiza… tal que \\[g(u,v) = (g_1(u,v),g_2(u,v))\\] En este caso, calculamos \\[\\left.\\begin{array}{c} \\Phi_u\\times\\Phi_v \\\\ \\Psi_r\\times\\Psi_s \\end{array}\\right\\}\\] Tenemos que \\(\\Phi(u,v) = \\Psi(g_1(u,v),g_2(u,v))\\) \\[\\begin{gather*} \\Phi_1(u,v) = \\Psi_1(g_1(u,v),g_2(u,v))\\\\ (\\Phi_1)_{u} = (\\Psi_1)_r r_u + (\\Psi_1)_s s_u = ((\\Psi)_r\\,(\\Psi_1)_s)\\begin{pmatrix} (g_1)_u\\\\(g_2)_v \\end{pmatrix} \\end{gather*}\\] Se calculan las 6 derivadas \\((\\Phi_1,\\Phi_2,\\Phi_3)\\) respecto de \\(u\\) y de \\(v\\) y acabamos llegando a \\[\\begin{pmatrix} (\\Phi_1)_u\\\\(\\Phi_2)_u\\\\(\\Phi_3)_u \\end{pmatrix} = \\begin{pmatrix} (\\Psi_1)_r &amp; (\\Psi_1)_s\\\\ (\\Psi_2)_r &amp; (\\Psi_2)_s\\\\ (\\Psi_3)_r &amp; (\\Psi_3)_s \\end{pmatrix}\\begin{pmatrix} (g_1)_u\\\\ (g_2)_u \\end{pmatrix} \\] Análogamente: \\[\\begin{pmatrix} (\\Phi_1)_v\\\\(\\Phi_2)_v\\\\(\\Phi_3)_v \\end{pmatrix} = \\begin{pmatrix} (\\Psi_1)_r &amp; (\\Psi_1)_s\\\\ (\\Psi_2)_r &amp; (\\Psi_2)_s\\\\ (\\Psi_3)_r &amp; (\\Psi_3)_s \\end{pmatrix}\\begin{pmatrix} (g_1)_v\\\\ (g_2)_v \\end{pmatrix} \\] Ahora procedemos a calcular \\[\\Phi_u\\x\\Phi_v = \\dotsc = \\left( \\left|\\begin{array}{cc} (\\Phi_2)_u &amp; (\\Phi_2)_v\\\\ (\\Phi_3)_u &amp; (\\Phi_3)_v \\end{array} \\right|,- \\left|\\begin{array}{cc} (\\Phi_1)_u &amp; (\\Phi_1)_v\\\\ (\\Phi_3)_u &amp; (\\Phi_3)_v \\end{array} \\right|, \\left|\\begin{array}{cc} (\\Phi_1)_u &amp; (\\Phi_1)_v\\\\ (\\Phi_2)_u &amp; (\\Phi_2)_v \\end{array} \\right| \\right)\\] Tomando el primer elemento calculamos: \\[ \\det\\left( \\begin{array}{cc} (\\Psi_2)_r &amp; (\\Psi_2)_s\\\\ (\\Psi_3)_r &amp; (\\Psi_3)_s \\end{array} \\right)\\begin{pmatrix} (g_1)_u &amp;(g_1)_v \\\\ (g_2)_u &amp; (g_2)_v \\end{pmatrix} = \\left| \\begin{array}{cc} (\\Psi_2)_r &amp; (\\Psi_2)_s\\\\ (\\Psi_3)_r &amp; (\\Psi_3)_s \\end{array}\\right| \\det Dg \\] Repitiendo la cuenta con el resto de los elementos, llegamos a \\[\\Phi_u\\x\\Phi_v = \\Psi_r\\x\\Psi_s\\cdot (\\det Dg)\\] Para mantener la orientación necesitamos \\(\\det Dg &gt; 0\\) Este teorema solo afecta a la zona de la subvariedad \\(M\\) parametrizada por \\(\\Phi\\) y \\(\\Psi\\) al mismo tiempo. La parametrización es \\[ \\Phi(t,\\theta) = \\left((R+t\\cdot sen\\frac{\\theta}{2})cos\\theta, (R+t\\cdot sen\\frac{\\theta}{2})sen\\theta, t cos\\frac{\\theta}{2} \\right) \\, t\\in(-1,1),\\theta\\in(0,2\\pi) \\] ¿Estamos seguros de que es una parametrización? Vamos a comprobar el homeomorfismo, Tenemos 2 posibles caminos: Despejamos en función de \\(x,y,z\\) \\(\\frac{y}{x} = tg \\theta\\). Definimos : \\[ \\begin{array}{ccc} \\theta &amp;= arctg\\frac{y}{x} &amp; (1)\\\\ \\theta &amp;= arctg\\frac{y}{x} + \\pi &amp; (2)\\\\ \\theta &amp;= arctg\\frac{y}{x} + 2\\pi &amp; (3) \\end{array} \\] Y comprobar que es continua Escribimos este conjunto como un conjunto de nivel y ver que es una variedad, por lo tanto no hace falta comprobar el homeomorfismo sobre la imagen. \\[\\begin{gather*} \\sqrt{x^2+y^2} = (R + t\\cdot cos\\frac{\\theta}{2})\\\\ \\sqrt{x^2+y^2} - R = \\frac{z}{tg\\frac{&quot;arctg\\frac{y}{x}&quot;}{?}} \\end{gather*}\\] Una clave importante es que cuando \\(\\theta\\rightarrow 0^+ \\implies \\Phi(t,\\theta)\\) pero si \\(\\theta\\rightarrow 2\\pi^{-} \\implies \\Phi(-t,\\theta)\\) \\(\\overrightarrow{n} = T_t\\x T_{\\theta} = ... = \\overrightarrow{n}(t,\\theta)\\) \\[\\begin{gather*} \\overrightarrow{n}(t,\\theta) \\convs[][\\theta\\rightarrow 0^+] (-R,\\frac{t}{2})\\\\ \\overrightarrow{n}(t,\\theta) \\convs[][\\theta\\rightarrow 2\\pi^-] (R,-\\frac{t}{2}) \\end{gather*}\\] Curioso: esta parametrización que no cubre el segmento con el que se empieza. Esta parametrización tiene una propiedad curiosa, al empezar, el vector normal apunta hacia dentro y al final, apunta hacia fuera. Es imposible cubrirla del todo con una parametrización. típico de superficie no orientable es la botella de Klein Conjuntos con frontera en \\(\\real^2\\). Utilizamos la tercera caracterización de subvariedad, que dice que existe un que vamos a llamar \\(\\Phi\\), (que por ser difeomorfismo, existe \\(\\Psi = \\Phi^{-1})\\). Podemos suponer que estamos trabajano con un cuadrado en el plano y su imagen para facilitar las cuentas. Partimos de En U es fácil orientar un cuadrado. ¿Cómo nos “llevamos” la orientación? Con la diferencial, que si queremos que sea la misma orientación \\(\\implies \\det D\\Phi &gt; 0\\). Sea \\(\\mathcal{C} = \\{e_1=(1,0),e_2=(0,1)\\}\\) base canónica en \\(\\real^2\\) (variables (s,t)). Sea \\(\\mathcal{B} = \\{D\\Psi(s_0,0)e_1,D\\Psi(s_0,0)e_2\\}\\) base en \\(\\real^2\\) (variables \\((x,y)\\)) Tenemos también \\(\\sigma(s) = \\Psi(s,0)\\) parametrización de un trozo de \\(dM\\) que contiene a \\(x_0\\). \\[D\\Psi = \\begin{pmatrix} \\displaystyle\\dpa{\\Psi_1}{s} &amp;\\displaystyle\\dpa{\\Psi_1}{t}\\\\ \\displaystyle\\dpa{\\Psi_2}{s} &amp;\\displaystyle\\dpa{\\Psi_2}{t} \\end{pmatrix}\\] En la primera columna lo que tenemos es el vector tangente a la curva. Cuando \\(t=0\\) tenemos la parametrización de la frontera. \\[ D\\Psi|_(s_0,0) = \\begin{pmatrix} \\sigma&#39;(s_0) &amp; \\overrightarrow{v}\\\\ \\downarrow &amp; \\downarrow \\end{pmatrix} \\] Haciendo un desarrollo de Taylor tenemos: \\[ \\Psi(s_0,t) = \\begin{pmatrix} \\Psi_1(s_0,0)\\\\ \\Psi_2(s_0,0) \\end{pmatrix} + t \\underbrace{\\begin{pmatrix} \\dpa{\\Psi_1}{t} (s_0,0)\\\\ \\dpa{\\Psi_2}{t} (s_0,0) \\end{pmatrix}}_{\\gor{v}} + err \\] \\[\\underbrace{\\begin{pmatrix}\\Psi_1(s_0,t)\\\\ \\Psi_2(s_0,t) \\end{pmatrix} }_{\\in M\\, t&gt;0}= \\gor{x}_0 + t\\gv + err \\] Conclusión: \\(\\gv\\) “apunta hacia el interior de \\(M\\)”. \\[\\mathcal{C} = \\{e_1,e_2\\}\\] \\[\\mathcal{B} = \\{D\\Psi(s_0,0)e_1,D\\Psi(s_0,0)e_2\\} = \\{\\sigma&#39;(s_0),\\gv\\}\\] \\[\\mathcal{B} \\text{ orientación positiva } \\dimplies \\det D\\Psi(s_0,0) &gt; 0 \\dimplies \\text{ Ángulo entre } \\sigma&#39;(s_0) \\text{ y } \\gv \\in (0,\\pi)\\] Con un dibujo se llegaría a entender perfectamente la siguiente conclusión (basada en la interpretación geométrica de que el ángulo \\(\\in (0,\\pi)\\)) si recorremos la frontera de la subvariedad con la mano izquierda hacia dentro es la orientación positiva si nuestra subvariedad queda en el interior de esa frontera/curva, si nuestra subvariedad es el exterior de la curva pues será la orientación negativa. \\[\\appl{\\Phi}{\\real^2}{\\real^3}\\] Tenemos $(s) = (s,0) $, parametrización de un trozo de \\(dM\\) que contiene a \\(x_0\\). Siendo \\[D\\Phi(s_0,0) = \\begin{pmatrix} ... \\end{pmatrix} = \\begin{pmatrix} \\sigma&#39;(s_0) &amp; \\overrightarrow{v}\\\\ \\downarrow &amp; \\downarrow \\end{pmatrix}\\] \\[\\mathcal{C} = \\{e_1,e_2,e_3\\}\\] \\[\\mathcal{B} = \\{D\\Phi(s_0,0)e_1,D\\Phi(s_0,0)e_2\\} = \\{\\sigma&#39;(s_0),\\gv\\}\\] Solo se tienen 2 vectores. Anteriormente hemos visto que es el producto vectorial () Es decir \\[\\mathcal{B} = \\{\\sigma&#39;(s_0),\\gv,\\sigma&#39;(s_0)\\x\\gv\\}\\] Con las cuentas vistas antes del ejemplo comprobamos que \\(\\gv\\) apunta hacia el interior de \\(M\\). 8.1 Integral de Riemann-Darboux 8.2 Propiedades de la integral 8.3 Evaluación de la integral 8.4 Fórmula de Stirling 8.5 Teoremas del valor medio, versión integral 8.6 Estimación de la integral 8.7 Integrales impropias 8.8 La integrabilidad según Riemann 8.9 Funciones a variación acotada 8.10 La integral de Riemann-Stieltjes "],
["series-numericas-infinitas.html", "Capítulo 9 Series numéricas infinitas 9.1 Definición y ejemplos 9.2 Series con términos no-negativos 9.3 Criterios de convergencia 9.4 Convergencia condicional y absoluta 9.5 Sucesiones dobles y series", " Capítulo 9 Series numéricas infinitas 9.1 Definición y ejemplos 9.2 Series con términos no-negativos 9.3 Criterios de convergencia 9.4 Convergencia condicional y absoluta 9.5 Sucesiones dobles y series "],
["sucesiones-y-series-de-funciones.html", "Capítulo 10 Sucesiones y series de funciones 10.1 Convergencia de sucesiones de funciones 10.2 Propiedades del límite de funciones 10.3 Convergencia de las series de funciones 10.4 Series de potencias", " Capítulo 10 Sucesiones y series de funciones 10.1 Convergencia de sucesiones de funciones 10.2 Propiedades del límite de funciones 10.3 Convergencia de las series de funciones 10.4 Series de potencias "],
["funciones-en-varias-variables.html", "Capítulo 11 Funciones en varias variables 11.1 Transformaciones lineales 11.2 Diferenciación 11.3 El principio de la contracción 11.4 El teorema de la función inversa 11.5 El teorema de la función implícita 11.6 Teorema del rango 11.7 Determinantes 11.8 Derivadas de orden superior 11.9 Diferenciación de integrales", " Capítulo 11 Funciones en varias variables 11.1 Transformaciones lineales 11.2 Diferenciación 11.3 El principio de la contracción 11.4 El teorema de la función inversa 11.5 El teorema de la función implícita 11.6 Teorema del rango 11.7 Determinantes 11.8 Derivadas de orden superior 11.9 Diferenciación de integrales "],
["integracion-de-formas-diferenciales.html", "Capítulo 12 Integración de formas diferenciales 12.1 Integración 12.2 Aplicaciones primitivas 12.3 Cambio de variables 12.4 Formas diferenciales 12.5 Cadenas y símplices 12.6 Teorema de Stoke 12.7 Formas cerradas y formas exactas 12.8 Análisis vectorial", " Capítulo 12 Integración de formas diferenciales 12.1 Integración 12.2 Aplicaciones primitivas 12.3 Cambio de variables 12.4 Formas diferenciales 12.5 Cadenas y símplices 12.6 Teorema de Stoke 12.7 Formas cerradas y formas exactas 12.8 Análisis vectorial "],
["funciones-especiales.html", "Capítulo 13 Funciones especiales 13.1 Series de potencia 13.2 Funciones exponenciales y logarítmicas 13.3 Funciones trigonométricas 13.4 Completitud algebraica del cuerpo de los complejos 13.5 Series de Fourier", " Capítulo 13 Funciones especiales 13.1 Series de potencia 13.2 Funciones exponenciales y logarítmicas 13.3 Funciones trigonométricas 13.4 Completitud algebraica del cuerpo de los complejos 13.5 Series de Fourier "],
["espacios-lineales-normados.html", "Capítulo 14 Espacios lineales normados 14.1 Normas y seminormas 14.2 Completación de un espacio normado 14.3 Series infinitas en espacios normados 14.4 Sumas no ordenadas en espacios normados 14.5 Trnasformaciones lineales acotadas 14.6 Álgebras de Banach", " Capítulo 14 Espacios lineales normados 14.1 Normas y seminormas 14.2 Completación de un espacio normado 14.3 Series infinitas en espacios normados 14.4 Sumas no ordenadas en espacios normados 14.5 Trnasformaciones lineales acotadas 14.6 Álgebras de Banach "],
["espacios-topologicos.html", "Capítulo 15 Espacios topológicos 15.1 Abiertos y cerrados 15.2 Sistemas de entornos 15.3 Bases de entornos 15.4 Topología relativa 15.5 Nets", " Capítulo 15 Espacios topológicos 15.1 Abiertos y cerrados 15.2 Sistemas de entornos 15.3 Bases de entornos 15.4 Topología relativa 15.5 Nets "],
["continuidad-en-espacios-topologicos.html", "Capítulo 16 Continuidad en espacios topológicos 16.1 Propiedades generales 16.2 Topologías iniciales 16.3 Topología producto 16.4 Topología cociente 16.5 Espacio de funciones contínuas 16.6 Conjuntos F-sigma y G-delta", " Capítulo 16 Continuidad en espacios topológicos 16.1 Propiedades generales 16.2 Topologías iniciales 16.3 Topología producto 16.4 Topología cociente 16.5 Espacio de funciones contínuas 16.6 Conjuntos F-sigma y G-delta "],
["espacios-topologicos-normados.html", "Capítulo 17 Espacios topológicos normados 17.1 Lema de Urysohn 17.2 Teorema de extensión de Tietze", " Capítulo 17 Espacios topológicos normados 17.1 Lema de Urysohn 17.2 Teorema de extensión de Tietze "],
["espacios-topologicos-compactos.html", "Capítulo 18 Espacios topológicos compactos 18.1 Convergencia en espacios compactos 18.2 Compacidad del producto cartesiano 18.3 Continuidad y compacidad", " Capítulo 18 Espacios topológicos compactos 18.1 Convergencia en espacios compactos 18.2 Compacidad del producto cartesiano 18.3 Continuidad y compacidad "],
["espacios-metricos-totalmente-acotados.html", "Capítulo 19 Espacios métricos totalmente acotados", " Capítulo 19 Espacios métricos totalmente acotados "],
["equicontinuidad.html", "Capítulo 20 Equicontinuidad", " Capítulo 20 Equicontinuidad "],
["el-teorema-de-stone-weierstrass.html", "Capítulo 21 El teorema de Stone-Weierstrass", " Capítulo 21 El teorema de Stone-Weierstrass "],
["espacios-toplogicos-localmente-compactos.html", "Capítulo 22 Espacios toplógicos localmente compactos 22.1 Propiedades generales 22.2 Funciones a soporte compacto 22.3 Funciones que se anulan al infinito 22.4 Compactificación a un punto", " Capítulo 22 Espacios toplógicos localmente compactos 22.1 Propiedades generales 22.2 Funciones a soporte compacto 22.3 Funciones que se anulan al infinito 22.4 Compactificación a un punto "],
["espacios-de-hilbert.html", "Capítulo 23 Espacios de Hilbert 23.1 Definición y ejemplos 23.2 Ortogonalidad 23.3 Separación de conjuntos convexos 23.4 Bases ortonormadas 23.5 Convergencia débil 23.6 Operadores contínuos y compactos 23.7 Teorema espectral de Hilbert", " Capítulo 23 Espacios de Hilbert 23.1 Definición y ejemplos 23.2 Ortogonalidad 23.3 Separación de conjuntos convexos 23.4 Bases ortonormadas 23.5 Convergencia débil 23.6 Operadores contínuos y compactos 23.7 Teorema espectral de Hilbert "],
["espacio-de-funciones-diferenciables.html", "Capítulo 24 Espacio de funciones diferenciables", " Capítulo 24 Espacio de funciones diferenciables "],
["particiones-de-la-unidad.html", "Capítulo 25 Particiones de la unidad", " Capítulo 25 Particiones de la unidad "],
["conexidad.html", "Capítulo 26 Conexidad", " Capítulo 26 Conexidad En \\(\\real^3\\) tenemos puntos (dimensión 0), curvas (dimensión 1), superficies (dimensión 2) y abiertos (dimensión 3) sobre los que integrar en los que la imaginación resulta bastante útil. Pero… ¿qué pasa en \\(\\real^N\\)? Entonces en \\(\\real^N\\) tenemos objetos de dimensión \\(0,...,N\\) sobre los que vamos a poder definir propiedades, integrales, etc. Repasamos la idea de que en \\(\\real^3\\) podíamos representar una superficie de varias maneras y cómo cálculabamos su plano tangente. Dada una superficie como una gráfica, simplemente construíamos el vector normal al plano derivando parcialmente: \\[ \\vn = \\left( \\dpa{f}{x},\\dpa{f}{y}, 1 \\right) \\] Con una parametrización \\(\\phi\\) \\[\\begin{align*} \\appl{\\phi}{D\\subset\\real^2 &amp;}{\\real^3} \\\\ \\phi(u,v) &amp;= (x(u,v),y(u,v),z(u,v)) \\end{align*}\\] calculamos el plano tangente tomando los dos vectores directores y calculamos el normal al plano: \\[\\begin{gather*} T_u = \\left( \\dpa{x}{u},\\dpa{y}{u},\\dpa{z}{u}\\right)\\\\ T_v = \\left( \\dpa{x}{v},\\dpa{y}{v},\\dpa{z}{v}\\right)\\\\ \\overrightarrow{n} = T_u\\times T_v\\\\ \\end{gather*}\\] Nos podemos encontrar el problema de que \\(\\overrightarrow{n} = \\overrightarrow{0}\\). Para evitar ese caso, estableceremos que el rango de la matriz de las 6 derivadas tiene que ser 2. Supongamos que nos dan una gráfica definida como un conjunto de nivel \\(F(x,y,z) = 0\\) . Para calcular el plano tangente en este caso tenemos que \\(\\overrightarrow{n} = \\nabla F\\). Nos puede ocurrir que \\(F\\) no sea derivable o que \\(\\nabla F = \\overrightarrow{0}\\). Supongamos que sea diferenciable, ¿cómo preveer que puede salir? Para evitarlo, debemos forzar de nuevo que la matriz de las derivadas tenga rango máximo (en este caso 1). Vamos a ver que pasa con las curvas en \\(\\real^3\\) y cómo calcular la recta tangente Vemos claramente que las condiciones para poder calcular superficies tangentes llegan siempre a obligar a que la matriz tenga (ver sección ). A partir de aquí, podemos crear nuestra definición de subvariedad diferenciable: Veamos ejemplos de si algunos objetos son variedades diferenciables o no: \\[\\appl{F}{U\\subset\\real^3}{\\real}\\] \\[\\{(x,y,z) \\tq F(x,y,z) = 0\\}\\] En este caso tenemos \\(N=2, K=1\\). La condición de rango nos dice que \\(\\nabla F(\\gx) \\neq (0,0,0)\\). Esto es, obliga a en cada punto existe un vector normal \\(\\overrightarrow{n}\\), exactamente la misma condición que habíamos visto antes. \\[\\sigma(t) = (x(t),y(t),z(t))\\equiv S_1 \\cap S_2 = \\] \\[= \\{F_1(x,y,z) = 0\\}\\cap \\{F_2(x,y,z) = 0\\}\\] Si tomamos \\(U\\cap \\sigma = \\{(x,y,z)\\in\\real^3 \\tq F_1 = 0 \\y F_2 = 0\\}\\) Siendo \\[\\appl{F}{U\\subset\\real^3}{\\real^2}, N+K=3,K=2\\]\\[F(x,y,z) = (F_1(x,y,z),F_2(x,y,z))\\] Veamos la condición de rango en este caso: \\[rango \\begin{pmatrix} \\dpa{F_1}{x}&amp;\\dpa{F_1}{y}&amp;\\dpa{F_1}{z}\\\\\\dpa{F_2}{x}&amp;\\dpa{F_2}{y}&amp;\\dpa{F_2}{z}\\end{pmatrix}\\] Para que el rango sea máximo, los vectores tienen que ser no paralelos, es decir, \\(S_1, S_2\\) sean transversales, no paralelas. De nuevo, la misma condición que habíamos visto antes. Podemos definir un punto de una manera un tanto rebuscada: \\[\\begin{gather*} \\appl{F}{\\real^K}{\\real^K}\\\\ \\gx \\rightarrow \\gx - \\ga \\end{gather*}\\] En este caso, la subvariedad \\(M\\) es nuestro punto \\(\\ga\\): \\[M = \\{\\gx \\in \\real^K \\tq F(\\gx) = \\gor{0}\\} = \\{\\ga\\}\\] Tenemos $DF = Id $, que tiene rango máximo y por lo tanto \\(\\{\\ga\\}\\) es una subvariedad diferenciable (dimensión 0, codimensión \\(k\\)). \\(\\appl{F}{\\Omega\\subset\\real^N}{0}\\). En este caso tenemos codimensión \\(0\\) y dimensión \\(N\\). \\[\\begin{gather*} \\appl{F}{\\real^2}{\\real}\\\\ F(x,y) = x^3 - y^6 \\\\ M = \\{(x,y) \\tq x^3-y^6 = 0\\} \\\\ DF = (3x^2-6y^5) \\\\ DF(0,0) = (0,0) \\end{gather*}\\] La matriz diferencial tiene rango \\(0\\) en el punto \\((0,0)\\). ¿Quiere esto decir que \\(M\\) no es una subvariedad diferenciable en el 0? No. Lo que quiere decir que no hemos encontrado la función que cumpla las hipótesis. Podemos operar \\[M = \\{x^3=y^6\\} = \\{x = y^2\\} = \\{x-y^2 = 0\\}\\] Y vemos que si tomamos \\(G(x,y) = x-y^2\\), esta función representa la misma subvariedad \\(M\\), y \\(DG(x,y) = (1,2y)\\) tiene rango 1 en el origen. \\[M = \\{(x,y)\\in \\real^2 \\tq x^2-y^2 = 0\\}\\] Definimos una función \\(F\\): \\[\\appl{F}{\\real^2}{\\real}\\] \\[F(x,y) = x^2-y^2\\] \\(DF = (2x,-2y)\\). La condición de rango falla en \\((0,0)\\). Valoramos si este objeto no es una subvariedad o si tendremos que definir una función de una manera más inteligente, tal y como hicimos en el caso anterior. En este caso, vemos que \\(M = \\{ y=x \\cup y = -x \\}\\). No debería ser una subvariedad porque en el \\((0,0)\\) no es derivable (ver figura ). Intentaremos demostrarlo por reducción al absurdo. Supongamos que \\(M\\) es una subvariedad. Entonces, según la definición () existe un \\(U, (0,0) \\in U\\) y una aplicación \\(\\appl{G}{U\\subset\\real^2}{\\real}\\) con \\(U\\cap M = \\{G(x,y) = 0\\}\\). Entonces tendríamos que \\[\\rango DG(x,y) = 1, \\forall(x,y) \\in U \\implies \\rango \\left(\\dpa{G}{x},\\dpa{G}{y}\\right) = 1\\] Ahí tenemos dos casos, o bien que la primera componente no sea \\(0\\) o que sea la segunda la que no es nula. Supongamos primero que \\(\\dpa{G}{x}(0,0) \\neq 0\\). Podemos aplicar el Teorema de la unción implícita (), que nos dice que podemos despejar \\(x = x(y)\\). En este caso: \\(U \\cap M = \\{x(y)^2-y^2 = 0\\}\\). Si fijamos \\(y=\\varepsilon\\), entonces \\(x(\\varepsilon) = \\pm \\varepsilon\\) No es una función, lo que contradice el Teorema de la función implícita, y por lo tanto es imposible que \\(\\dpa{G}{x}(0,0) \\neq 0\\). Análogamente, tampoco puede cumplirse que \\(\\dpa{G}{y}(0,0) \\neq 0\\). Hemos demostrado por lo tanto que no puede existir una función que defina esto como subvariedad diferencial. Este es el ejemplo de que cualquier objeto que tenga autointersección no puede ser subvariedad. \\(M = \\{(x,y) \\in \\real^2 \\tq x^2-y^2 = 0, y\\ge 0\\}\\) Vamos a suponer que existe una función \\(F \\in C^1\\) que representa ese objeto (que viene definido por 2 condiciones). \\(M = \\{F(x,y) = 0\\}\\) para alguna \\(F\\). Condición de rango: \\(\\left(\\dpa{F}{x},\\dpa{F}{y}\\right) \\neq (0,0)\\). Condición de rango: \\(\\left(\\dpa{F}{x},\\dpa{F}{y}\\right) = (0,0), x=y=0\\). Vamos a ver si es subvariedad o no. En este caso intuimos que no debería serlo. Volvemos a demostrar por reducción al absurdo: Supongamos que \\(M\\) es subvariedad, entonces \\(\\exists G(x,y)\\) tal que \\(\\appl{G}{U\\subset\\real^2}{\\real}, U \\cap M = \\{G(x,y) = 0\\}\\) Supongamos \\(\\displaystyle \\dpa{G}{x}\\neq 0\\). Entonces tenemos \\[ M\\cap U = \\{y(x)^2 - y^2 = 0\\} \\] Si fijamos \\(x=\\varepsilon \\implies y(x) = \\abs{\\varepsilon}\\), pero esto quiere decir que \\(G \\notin C^1\\). De forma análoga con la segunda coordenada, vemos que \\(M\\) no es una subvariedad. Nos quitamos el punto conflictivo del caso anterior, el \\((0,0)\\). \\(N = \\{(x,y)\\in \\real^2 \\tq x^2-y^2 = 0, y&gt;0\\}\\) La lógica nos dice que este caso si debería ser subvariedad diferencial: la definición de la función es local, y siempre podremos encontrar un entorno que no incluya el 0, que es el punto problemático. Comprobamos que efectivamente el rango de \\(\\nabla F\\) es máximo \\(\\forall (x,y)\\in \\real^2\\) y por lo tanto \\(M\\) es una subvariedad. Superficie en \\(\\real^3\\) parametrizada: \\(S = \\{\\Phi(u,v) = (x(u,v),y(u,v),z(u,v))\\}\\). A la hora de trabajar con superficies parametrizadas, nos interesaría poder definir una especie de función inversa que nos permita hacer cambios en el plano y llevarlos a la superficie o al reves, pero… ¡tienen dimensiones distintas! La esperanza que nos queda es que la superficie parametrizada tiene dimensión 2, igual que el plano. \\[\\appl{\\sigma}{[0,2\\pi)}{\\real^2}\\] \\[t \\rightarrow \\sigma(t) = (cos(t),sen(t))\\] Inversa: \\(\\Psi(x,y) = t\\) ángulo de la representación en polares. Vamos a estudiar el problema de la continuidad: Tomamos \\(\\{(X_n,Y_n)\\}\\) con \\(x^2+y^2 = 1 \\tlq (x_n,y_n) \\convs (1,0)\\) Si \\(\\Psi\\) es continua, debe ser \\(\\Psi(x_n,y_n) \\convs (1,0) = 0\\). En este caso no es continua porque: Si tomamos \\[\\begin{align*} P_n &amp;= \\left(cos\\left(2\\pi - \\frac{1}{n}\\right), sen\\left(2\\pi - \\frac{1}{n}\\right) \\right)\\\\ P_n &amp;\\convs (1,0)\\\\ \\Psi(P_n) &amp;= 2\\pi - \\frac{1}{n} \\convs 2\\pi\\neq\\Psi(1,0) \\end{align*}\\] \\[\\appl{f}{(0,1)\\subset\\real}{\\real^2}\\] \\[t \\rightarrow f(t) = (t,g(t)), \\text{continua, con inversa continua}\\] Vamos a definir la inversa: \\[P\\in f(0,1) \\implies P(x,g(x)) \\text{Para algún } x\\in(0,1)\\] \\(P = (x,g(x))\\) \\[\\Psi(P) = t \\in (0,1) \\tlq f(t) = (x,g(x)) \\implies t = x\\] Vamos a estudiar la continuidad: \\[\\{P_n\\} \\subset f(0,1), P_n \\rightarrow P \\in f(0,1)\\] \\[P_i = (x_i,g(x_i)), \\text{ para algún } i \\in (0,1)\\] \\[P_n \\convs P_0 \\dimplies (x_n,g(x_n)) \\rightarrow (x_0,g(x_0)) \\implies x_n (= \\Psi(P_n)) \\rightarrow x_0 (=\\Psi(P_0)) \\] ¿La gráfica de una función es homeomorfismo sobre su imagen? \\[\\appl{\\sigma_3}{(0,4\\pi)}{\\real^2}\\] \\[t \\rightarrow \\sigma(t) = (cos(t),sen(t))\\] No es inyectiva \\(\\implies \\nexists \\Psi\\). \\[\\appl{\\sigma_4)}{(0,2\\pi)}{\\real^2}\\] \\[t \\rightarrow \\sigma(t) = (cos(t),sen(t))\\] La diferencia con el ejemplo 1, es que es cerrado. Hay que percatarse de que la tangente no es inyectiva, entonces… la inversa es algo más complicada que \\(\\arctan\\), así que vamos a definirla a trozos siguiendo el esquema de la figura . \\[\\Psi(x,y) = \\begin{cases} \\arctan\\left(\\frac{y}{x}\\right) &amp; (x,y)\\in 1\\\\ \\arctan\\left(\\frac{y}{x}\\right) + \\pi &amp; (x,y) \\in 2\\\\ \\arctan\\left(\\frac{y}{x}\\right)+2\\pi&amp; (x,y) \\in 3 \\end{cases} \\] Hay que estudiar la continuidad en \\((0,-1)\\) y en \\((0,1)\\). Tomamos \\(\\{P_n\\} \\rightarrow (0,1), P_n \\in S_1\\) Queremos probar que \\(\\Psi(P_n) \\rightarrow \\Psi(0,1) = \\frac{\\pi}{2}\\). Ejercicio para el lector: Sucesiones que se acercan por la derecha o por la izquierda y comprobar que vale lo que tiene que valer. La parametrización es \\[\\sigma_5 (x(t),y(t)) = \\left(\\frac{\\cos(t)}{\\sin^2(t)+1},\\frac{\\sin(t)\\cos(t)}{\\sin^2(t)+1}\\right), t\\in \\left(\\frac{-\\pi}{2},\\frac{3\\pi}{2}\\right)\\] Al origen podemos acercarnos de distintas formas: Sea \\(\\{P_n\\}\\subset \\sigma_5(t) \\rightarrow (0,0)\\). Podemos tomar la sucesión \\(\\{P_n\\} = \\{P_1,P_2,P_3,...\\}\\) cada uno en una región (1,2,3 ciclicamente). \\[\\Psi(P_n) \\begin{cases} \\in 3 &amp; \\text{ si }n \\text{ es múltiplo de }3\\\\ \\in 2 &amp; \\text{ si } n\\equiv 2\\ mod \\ 3\\\\ \\in 1 &amp; \\text{ si } n \\equiv 1\\ mod \\ 3 \\end{cases}\\] Por tanto \\(\\nexists \\displaystyle \\lim_{n\\rightarrow \\infty} \\Psi(P_n)\\) definir “parametrización”. Curva parametrizada en \\(\\real^3\\) \\(\\Gamma = \\{\\sigma(t) = (x(t),y(t),z(t)), t \\in (a,b)\\}\\) Queremos excluir: Superficies de parametrización en \\(\\real^3\\). \\[S = \\{\\Phi(u,v) = (x(u,v),y(u,v),z(u,v)), (u,v)\\in D\\}\\] Queremos evitar: Ya tenemos todo lo necesario para definir parametrización: \\(\\Phi_{1,2}(x,y) = (x,y,\\pm \\sqrt{1-x^2-y^2})\\). Esta nos deja sin definir el ecuador. Para ello tenemos que definir más Estas cartas son expresando x,y en función de las otras 2. En total hacen falta 3 parametrizaciones. otra manera de parametrizar, la proyección estereográfica. El dibujo de la proyección es: \\[(u,v) \\in \\real^2 \\rightarrow (u,v,0) \\rightarrow r\\equiv (0,0,R) + t(u,v,-R) = (tu,tv,R(1-t)\\] Imponiendo \\(\\underbrace{P}_{r(t_0)} \\equiv r\\cap S\\) tenemos: \\[(t_0u)^2+(t_0v)^2 + R^2(1-t)^2 = R^2 \\rightarrow ... \\rightarrow t_0 = \\frac{2R^2}{u^2+v^2+R^2}\\] Conclusión: \\[P = (tu,tv,R(1-t) = \\frac{2R^2}{u^2+v^2+R^2}u,\\frac{2R^2}{u^2+v^2+R^2}v,\\frac{R(u^2+v^2-R^2)}{u^2+v^2+R^2} = \\Phi(u,v)\\]. Vemos que \\(\\Phi\\in C^1, \\Phi(\\real^2) = S_R-\\{(0,0,R)\\}\\) ¿Es una parametrización? Hay que comprobar En .2, llamaremos al conjunto definido por la fórmula y el dominio, \\((\\Phi,\\omega)\\). El conjunto de todas las cartas locales que definen una subvariedad se llama Atlas. La proyección estereográfica tiene 2 cartas, la del polo norte (que excluye ese punto) y la del polo sur. \\(\\ref{eq_1}.1 \\dimplies \\ref{eq_2}.2\\). Si tenemos una subvariedad dada como conjunto de nivel la podemos parametrizar (y viceversa). \\begin{proof} Esquema: \\(1\\implies 2, 2\\implies 3, 3 \\implies 1\\) Sea \\(\\ga \\in M, \\exists U\\subset\\real^{N+K}\\) abierto con \\(\\ga \\in U\\) y una función \\(\\appl{F}{U\\subset \\rnk}{\\rk}, F\\in C^1(U), \\tlq U\\cap M = \\{\\gx \\in U\\tq F(\\gx) = \\gor{0}\\}\\). Además \\(DF\\) tiene rango máximo (K). Queremos demostrar que existe una parametrización \\(\\Phi\\). el orden de las variables, suponemos que el menor de orden \\(K\\) con determinante no nulo corresponde con las \\(K\\) últimas variables. Por el teorema de la función implícita, \\(F(x_1,...,x_n,x_{n+1},...,x_{n+k}) = \\gor{0}\\), con \\(x_{n+1},...,x_{n+k}\\) despejable en función de las \\(\\{x_i,i=1,...,n\\}\\) en un entonro de \\(\\gor{a}\\). Es decir: existen \\(\\omega\\subset \\real^N, \\omega&#39;\\subset\\rk\\) abiertos tales que \\(\\gor{a}\\in \\omega \\times \\omega&#39;\\) y una función \\(\\appl{g}{\\omega\\subset\\real^N}{\\omega&#39;\\subset\\rk}\\) de manera que \\[\\left\\{\\begin{array}{c} g(a_1,...,a_n) = (a_{n+1},...,a_{n+k})\\\\ F(\\gx,g(\\gx)) = \\gor{0}, \\forall \\gx \\in \\omega\\end{array}\\right.\\] Idea: la parametrización es \\(\\Phi (\\gx) = (\\gx,g(\\gx))\\) Vamos a comprobar las condiciones: Es decir, $(x,y) = (x,g(x)) ) (x,g(x)) = (x,y) $. También \\(\\inv{\\Phi}(x,y) = x, \\forall(x,y)\\in (\\omega\\times\\omega&#39;) \\cap M\\) Continua. Vamos a emplear un argumento como el de la demostración del segundo teorema del rango. Buscamos: \\[\\begin{align*} \\appl{\\Phi}{\\rnk}{\\rnk} \\tlq \\Phi(V\\cap M) = (\\ast &amp;,0)\\\\ &amp;\\uparrow\\\\ k \\text{ últimas coor} &amp; \\text{denadas} \\end{align*}\\] Siendo \\(\\Phi\\) un difeomorfismo. existe una parametrización local \\(\\appl{\\Psi}{\\real^N}{\\rnk}\\). Completar el número de variables Definimos \\[\\begin{align*} \\appl{\\alpha}{\\omega\\times\\real^K \\subset \\real^N\\times\\real^K}&amp;{\\real^N\\times\\real^K}\\\\ (\\gor{u},\\gor{v})\\longrightarrow &amp;\\alpha(\\gor{u},\\gor{v}) = \\Psi(\\gor{u}) + (\\gor{0},\\gor{v})\\\\ \\gor{u} \\in \\real^N,\\gor{v} \\in \\real^K, \\Phi(\\gor{u})\\in \\real^{N+K} \\end{align*}\\] Aplicar el teorema de la función inversa () a \\(\\alpha\\). \\[D\\alpha = \\left(t. \\begin{array}{c|c} D\\Psi_k \\rightarrow &amp; 0\\\\ \\hline D\\Psi_{n+i} \\rightarrow &amp; I \\end{array}\\right. \\] En \\(\\real^3\\) tenemos puntos (dimensión 0), curvas (dimensión 1), superficies (dimensión 2) y abiertos (dimensión 3) sobre los que integrar en los que la imaginación resulta bastante útil. Pero… ¿qué pasa en \\(\\real^N\\)? Entonces en \\(\\real^N\\) tenemos objetos de dimensión \\(0,...,N\\) sobre los que vamos a poder definir propiedades, integrales, etc. Repasamos la idea de que en \\(\\real^3\\) podíamos representar una superficie de varias maneras y cómo cálculabamos su plano tangente. Dada una superficie como una gráfica, simplemente construíamos el vector normal al plano derivando parcialmente: \\[ \\vn = \\left( \\dpa{f}{x},\\dpa{f}{y}, 1 \\right) \\] Con una parametrización \\(\\phi\\) \\[\\begin{align*} \\appl{\\phi}{D\\subset\\real^2 &amp;}{\\real^3} \\\\ \\phi(u,v) &amp;= (x(u,v),y(u,v),z(u,v)) \\end{align*}\\] calculamos el plano tangente tomando los dos vectores directores y calculamos el normal al plano: \\[\\begin{gather*} T_u = \\left( \\dpa{x}{u},\\dpa{y}{u},\\dpa{z}{u}\\right)\\\\ T_v = \\left( \\dpa{x}{v},\\dpa{y}{v},\\dpa{z}{v}\\right)\\\\ \\overrightarrow{n} = T_u\\times T_v\\\\ \\end{gather*}\\] Nos podemos encontrar el problema de que \\(\\overrightarrow{n} = \\overrightarrow{0}\\). Para evitar ese caso, estableceremos que el rango de la matriz de las 6 derivadas tiene que ser 2. Supongamos que nos dan una gráfica definida como un conjunto de nivel \\(F(x,y,z) = 0\\) . Para calcular el plano tangente en este caso tenemos que \\(\\overrightarrow{n} = \\nabla F\\). Nos puede ocurrir que \\(F\\) no sea derivable o que \\(\\nabla F = \\overrightarrow{0}\\). Supongamos que sea diferenciable, ¿cómo preveer que puede salir? Para evitarlo, debemos forzar de nuevo que la matriz de las derivadas tenga rango máximo (en este caso 1). Vamos a ver que pasa con las curvas en \\(\\real^3\\) y cómo calcular la recta tangente Vemos claramente que las condiciones para poder calcular superficies tangentes llegan siempre a obligar a que la matriz tenga (ver sección ). A partir de aquí, podemos crear nuestra definición de subvariedad diferenciable: Veamos ejemplos de si algunos objetos son variedades diferenciables o no: \\[\\appl{F}{U\\subset\\real^3}{\\real}\\] \\[\\{(x,y,z) \\tq F(x,y,z) = 0\\}\\] En este caso tenemos \\(N=2, K=1\\). La condición de rango nos dice que \\(\\nabla F(\\gx) \\neq (0,0,0)\\). Esto es, obliga a en cada punto existe un vector normal \\(\\overrightarrow{n}\\), exactamente la misma condición que habíamos visto antes. \\[\\sigma(t) = (x(t),y(t),z(t))\\equiv S_1 \\cap S_2 = \\] \\[= \\{F_1(x,y,z) = 0\\}\\cap \\{F_2(x,y,z) = 0\\}\\] Si tomamos \\(U\\cap \\sigma = \\{(x,y,z)\\in\\real^3 \\tq F_1 = 0 \\y F_2 = 0\\}\\) Siendo \\[\\appl{F}{U\\subset\\real^3}{\\real^2}, N+K=3,K=2\\]\\[F(x,y,z) = (F_1(x,y,z),F_2(x,y,z))\\] Veamos la condición de rango en este caso: \\[rango \\begin{pmatrix} \\dpa{F_1}{x}&amp;\\dpa{F_1}{y}&amp;\\dpa{F_1}{z}\\\\\\dpa{F_2}{x}&amp;\\dpa{F_2}{y}&amp;\\dpa{F_2}{z}\\end{pmatrix}\\] Para que el rango sea máximo, los vectores tienen que ser no paralelos, es decir, \\(S_1, S_2\\) sean transversales, no paralelas. De nuevo, la misma condición que habíamos visto antes. Podemos definir un punto de una manera un tanto rebuscada: \\[\\begin{gather*} \\appl{F}{\\real^K}{\\real^K}\\\\ \\gx \\rightarrow \\gx - \\ga \\end{gather*}\\] En este caso, la subvariedad \\(M\\) es nuestro punto \\(\\ga\\): \\[M = \\{\\gx \\in \\real^K \\tq F(\\gx) = \\gor{0}\\} = \\{\\ga\\}\\] Tenemos $DF = Id $, que tiene rango máximo y por lo tanto \\(\\{\\ga\\}\\) es una subvariedad diferenciable (dimensión 0, codimensión \\(k\\)). \\(\\appl{F}{\\Omega\\subset\\real^N}{0}\\). En este caso tenemos codimensión \\(0\\) y dimensión \\(N\\). \\[\\begin{gather*} \\appl{F}{\\real^2}{\\real}\\\\ F(x,y) = x^3 - y^6 \\\\ M = \\{(x,y) \\tq x^3-y^6 = 0\\} \\\\ DF = (3x^2-6y^5) \\\\ DF(0,0) = (0,0) \\end{gather*}\\] La matriz diferencial tiene rango \\(0\\) en el punto \\((0,0)\\). ¿Quiere esto decir que \\(M\\) no es una subvariedad diferenciable en el 0? No. Lo que quiere decir que no hemos encontrado la función que cumpla las hipótesis. Podemos operar \\[M = \\{x^3=y^6\\} = \\{x = y^2\\} = \\{x-y^2 = 0\\}\\] Y vemos que si tomamos \\(G(x,y) = x-y^2\\), esta función representa la misma subvariedad \\(M\\), y \\(DG(x,y) = (1,2y)\\) tiene rango 1 en el origen. \\[M = \\{(x,y)\\in \\real^2 \\tq x^2-y^2 = 0\\}\\] Definimos una función \\(F\\): \\[\\appl{F}{\\real^2}{\\real}\\] \\[F(x,y) = x^2-y^2\\] \\(DF = (2x,-2y)\\). La condición de rango falla en \\((0,0)\\). Valoramos si este objeto no es una subvariedad o si tendremos que definir una función de una manera más inteligente, tal y como hicimos en el caso anterior. En este caso, vemos que \\(M = \\{ y=x \\cup y = -x \\}\\). No debería ser una subvariedad porque en el \\((0,0)\\) no es derivable (ver figura ). Intentaremos demostrarlo por reducción al absurdo. Supongamos que \\(M\\) es una subvariedad. Entonces, según la definición () existe un \\(U, (0,0) \\in U\\) y una aplicación \\(\\appl{G}{U\\subset\\real^2}{\\real}\\) con \\(U\\cap M = \\{G(x,y) = 0\\}\\). Entonces tendríamos que \\[\\rango DG(x,y) = 1, \\forall(x,y) \\in U \\implies \\rango \\left(\\dpa{G}{x},\\dpa{G}{y}\\right) = 1\\] Ahí tenemos dos casos, o bien que la primera componente no sea \\(0\\) o que sea la segunda la que no es nula. Supongamos primero que \\(\\dpa{G}{x}(0,0) \\neq 0\\). Podemos aplicar el Teorema de la unción implícita (), que nos dice que podemos despejar \\(x = x(y)\\). En este caso: \\(U \\cap M = \\{x(y)^2-y^2 = 0\\}\\). Si fijamos \\(y=\\varepsilon\\), entonces \\(x(\\varepsilon) = \\pm \\varepsilon\\) No es una función, lo que contradice el Teorema de la función implícita, y por lo tanto es imposible que \\(\\dpa{G}{x}(0,0) \\neq 0\\). Análogamente, tampoco puede cumplirse que \\(\\dpa{G}{y}(0,0) \\neq 0\\). Hemos demostrado por lo tanto que no puede existir una función que defina esto como subvariedad diferencial. Este es el ejemplo de que cualquier objeto que tenga autointersección no puede ser subvariedad. \\(M = \\{(x,y) \\in \\real^2 \\tq x^2-y^2 = 0, y\\ge 0\\}\\) Vamos a suponer que existe una función \\(F \\in C^1\\) que representa ese objeto (que viene definido por 2 condiciones). \\(M = \\{F(x,y) = 0\\}\\) para alguna \\(F\\). Condición de rango: \\(\\left(\\dpa{F}{x},\\dpa{F}{y}\\right) \\neq (0,0)\\). Condición de rango: \\(\\left(\\dpa{F}{x},\\dpa{F}{y}\\right) = (0,0), x=y=0\\). Vamos a ver si es subvariedad o no. En este caso intuimos que no debería serlo. Volvemos a demostrar por reducción al absurdo: Supongamos que \\(M\\) es subvariedad, entonces \\(\\exists G(x,y)\\) tal que \\(\\appl{G}{U\\subset\\real^2}{\\real}, U \\cap M = \\{G(x,y) = 0\\}\\) Supongamos \\(\\displaystyle \\dpa{G}{x}\\neq 0\\). Entonces tenemos \\[ M\\cap U = \\{y(x)^2 - y^2 = 0\\} \\] Si fijamos \\(x=\\varepsilon \\implies y(x) = \\abs{\\varepsilon}\\), pero esto quiere decir que \\(G \\notin C^1\\). De forma análoga con la segunda coordenada, vemos que \\(M\\) no es una subvariedad. Nos quitamos el punto conflictivo del caso anterior, el \\((0,0)\\). \\(N = \\{(x,y)\\in \\real^2 \\tq x^2-y^2 = 0, y&gt;0\\}\\) La lógica nos dice que este caso si debería ser subvariedad diferencial: la definición de la función es local, y siempre podremos encontrar un entorno que no incluya el 0, que es el punto problemático. Comprobamos que efectivamente el rango de \\(\\nabla F\\) es máximo \\(\\forall (x,y)\\in \\real^2\\) y por lo tanto \\(M\\) es una subvariedad. Superficie en \\(\\real^3\\) parametrizada: \\(S = \\{\\Phi(u,v) = (x(u,v),y(u,v),z(u,v))\\}\\). A la hora de trabajar con superficies parametrizadas, nos interesaría poder definir una especie de función inversa que nos permita hacer cambios en el plano y llevarlos a la superficie o al reves, pero… ¡tienen dimensiones distintas! La esperanza que nos queda es que la superficie parametrizada tiene dimensión 2, igual que el plano. \\[\\appl{\\sigma}{[0,2\\pi)}{\\real^2}\\] \\[t \\rightarrow \\sigma(t) = (cos(t),sen(t))\\] Inversa: \\(\\Psi(x,y) = t\\) ángulo de la representación en polares. Vamos a estudiar el problema de la continuidad: Tomamos \\(\\{(X_n,Y_n)\\}\\) con \\(x^2+y^2 = 1 \\tlq (x_n,y_n) \\convs (1,0)\\) Si \\(\\Psi\\) es continua, debe ser \\(\\Psi(x_n,y_n) \\convs (1,0) = 0\\). En este caso no es continua porque: Si tomamos \\[\\begin{align*} P_n &amp;= \\left(cos\\left(2\\pi - \\frac{1}{n}\\right), sen\\left(2\\pi - \\frac{1}{n}\\right) \\right)\\\\ P_n &amp;\\convs (1,0)\\\\ \\Psi(P_n) &amp;= 2\\pi - \\frac{1}{n} \\convs 2\\pi\\neq\\Psi(1,0) \\end{align*}\\] \\[\\appl{f}{(0,1)\\subset\\real}{\\real^2}\\] \\[t \\rightarrow f(t) = (t,g(t)), \\text{continua, con inversa continua}\\] Vamos a definir la inversa: \\[P\\in f(0,1) \\implies P(x,g(x)) \\text{Para algún } x\\in(0,1)\\] \\(P = (x,g(x))\\) \\[\\Psi(P) = t \\in (0,1) \\tlq f(t) = (x,g(x)) \\implies t = x\\] Vamos a estudiar la continuidad: \\[\\{P_n\\} \\subset f(0,1), P_n \\rightarrow P \\in f(0,1)\\] \\[P_i = (x_i,g(x_i)), \\text{ para algún } i \\in (0,1)\\] \\[P_n \\convs P_0 \\dimplies (x_n,g(x_n)) \\rightarrow (x_0,g(x_0)) \\implies x_n (= \\Psi(P_n)) \\rightarrow x_0 (=\\Psi(P_0)) \\] ¿La gráfica de una función es homeomorfismo sobre su imagen? \\[\\appl{\\sigma_3}{(0,4\\pi)}{\\real^2}\\] \\[t \\rightarrow \\sigma(t) = (cos(t),sen(t))\\] No es inyectiva \\(\\implies \\nexists \\Psi\\). \\[\\appl{\\sigma_4)}{(0,2\\pi)}{\\real^2}\\] \\[t \\rightarrow \\sigma(t) = (cos(t),sen(t))\\] La diferencia con el ejemplo 1, es que es cerrado. Hay que percatarse de que la tangente no es inyectiva, entonces… la inversa es algo más complicada que \\(\\arctan\\), así que vamos a definirla a trozos siguiendo el esquema de la figura . \\[\\Psi(x,y) = \\begin{cases} \\arctan\\left(\\frac{y}{x}\\right) &amp; (x,y)\\in 1\\\\ \\arctan\\left(\\frac{y}{x}\\right) + \\pi &amp; (x,y) \\in 2\\\\ \\arctan\\left(\\frac{y}{x}\\right)+2\\pi&amp; (x,y) \\in 3 \\end{cases} \\] Hay que estudiar la continuidad en \\((0,-1)\\) y en \\((0,1)\\). Tomamos \\(\\{P_n\\} \\rightarrow (0,1), P_n \\in S_1\\) Queremos probar que \\(\\Psi(P_n) \\rightarrow \\Psi(0,1) = \\frac{\\pi}{2}\\). Ejercicio para el lector: Sucesiones que se acercan por la derecha o por la izquierda y comprobar que vale lo que tiene que valer. La parametrización es \\[\\sigma_5 (x(t),y(t)) = \\left(\\frac{\\cos(t)}{\\sin^2(t)+1},\\frac{\\sin(t)\\cos(t)}{\\sin^2(t)+1}\\right), t\\in \\left(\\frac{-\\pi}{2},\\frac{3\\pi}{2}\\right)\\] Al origen podemos acercarnos de distintas formas: Sea \\(\\{P_n\\}\\subset \\sigma_5(t) \\rightarrow (0,0)\\). Podemos tomar la sucesión \\(\\{P_n\\} = \\{P_1,P_2,P_3,...\\}\\) cada uno en una región (1,2,3 ciclicamente). \\[\\Psi(P_n) \\begin{cases} \\in 3 &amp; \\text{ si }n \\text{ es múltiplo de }3\\\\ \\in 2 &amp; \\text{ si } n\\equiv 2\\ mod \\ 3\\\\ \\in 1 &amp; \\text{ si } n \\equiv 1\\ mod \\ 3 \\end{cases}\\] Por tanto \\(\\nexists \\displaystyle \\lim_{n\\rightarrow \\infty} \\Psi(P_n)\\) definir “parametrización”. Curva parametrizada en \\(\\real^3\\) \\(\\Gamma = \\{\\sigma(t) = (x(t),y(t),z(t)), t \\in (a,b)\\}\\) Queremos excluir: Superficies de parametrización en \\(\\real^3\\). \\[S = \\{\\Phi(u,v) = (x(u,v),y(u,v),z(u,v)), (u,v)\\in D\\}\\] Queremos evitar: Ya tenemos todo lo necesario para definir parametrización: \\(\\Phi_{1,2}(x,y) = (x,y,\\pm \\sqrt{1-x^2-y^2})\\). Esta nos deja sin definir el ecuador. Para ello tenemos que definir más Estas cartas son expresando x,y en función de las otras 2. En total hacen falta 3 parametrizaciones. otra manera de parametrizar, la proyección estereográfica. El dibujo de la proyección es: \\[(u,v) \\in \\real^2 \\rightarrow (u,v,0) \\rightarrow r\\equiv (0,0,R) + t(u,v,-R) = (tu,tv,R(1-t)\\] Imponiendo \\(\\underbrace{P}_{r(t_0)} \\equiv r\\cap S\\) tenemos: \\[(t_0u)^2+(t_0v)^2 + R^2(1-t)^2 = R^2 \\rightarrow ... \\rightarrow t_0 = \\frac{2R^2}{u^2+v^2+R^2}\\] Conclusión: \\[P = (tu,tv,R(1-t) = \\frac{2R^2}{u^2+v^2+R^2}u,\\frac{2R^2}{u^2+v^2+R^2}v,\\frac{R(u^2+v^2-R^2)}{u^2+v^2+R^2} = \\Phi(u,v)\\]. Vemos que \\(\\Phi\\in C^1, \\Phi(\\real^2) = S_R-\\{(0,0,R)\\}\\) ¿Es una parametrización? Hay que comprobar En .2, llamaremos al conjunto definido por la fórmula y el dominio, \\((\\Phi,\\omega)\\). El conjunto de todas las cartas locales que definen una subvariedad se llama Atlas. La proyección estereográfica tiene 2 cartas, la del polo norte (que excluye ese punto) y la del polo sur. \\(\\ref{eq_1}.1 \\dimplies \\ref{eq_2}.2\\). Si tenemos una subvariedad dada como conjunto de nivel la podemos parametrizar (y viceversa). Si \\(M\\) es una variedad diferenciable de dimensión \\(N\\) en \\(\\rnk\\), entonces, reordenando las variables puede escribirse como la gráfica de una función \\(C^1, \\underbrace{M\\cap U}_{(*)} = \\{(\\gx,g(\\gx))\\}\\) \\((*) \\implies\\) un trozo de la variedad. Si existe $C^1, rg(D) = n, ()M y $ no es homeomorfismo sobre su imagen \\(\\implies\\) no es subvariedad. Pregunta: Verdadero o Falso: ¿Con encontrar una aplicación que no sea homeomorfismo basta para asegurar que M no es subvariedad? La prueba de este teorema se deja como ejercicio para el lector. Sea \\(M\\) subvariedad n-dimensional en \\(\\rnk\\). \\(T_{\\gor{a}}M \\leadsto\\) espacio tangente a M en el punto \\(\\gor{a}\\in M\\) Pero… ¿qué es un hiperplano tangente? El concepto no es tan simple como un plano tangente. El espacio tangente será el conjunto de todos los vectores tangentes a cada una de las curvas contenidas en la superficie. A partir de aquí buscaremos una forma más eficiente de construirlo (porque es imposible calcular todas y cada una de las curvas contenidas en la superficie). Cualquier curva de la variedad la podremos ver como una curva en la proyección (con una parametriazación construida cuidadosamente) Es decir, si tenemos una parametrización, la diferencial lleva tangentes en tangentes. El teorema y el corolario son las 2 caracterizaciones del espacio tangente. Pero también podemos definir el espacio tangente con una tercera caracterización: Como gráfica. Sea \\(\\appl{f}{\\real^N}{\\real^{K}}\\) \\[M\\equiv \\left\\{(\\gu,f(\\gu)),\\begin{matrix} (\\gu,f(\\gu))\\in \\rnk\\\\ \\gu \\in \\Omega\\subset\\real^N\\\\ \\appl{f}{\\real^N}{\\real^K}\\end{matrix}\\right\\} \\] Salvo reordenación de las variables. Sea \\(p=(\\ga,f(\\ga))\\) el punto en el que queremos calcular el hiperplano tangente a \\(M\\). La caracterización será \\[T_p(M) = \\{(\\gu,v)\\in\\real^N\\x\\real^{K} \\tq v = Df(a) \\gu\\}\\] la otra alternativa es definir la subvariedad como \\(M = \\{(\\gu)\\in\\real^k \\tq f(\\gu) = f(\\ga)\\) $ M = {(x,y) ^2 x2+y2 = 1}$. Es una subvariedad porque… Vamos a calcular \\[T_{(a,b)}M = \\ker \\{\\begin{pmatrix} 2x &amp; 2y \\end{pmatrix}\\} =\\] \\[ \\{(u,v) \\tq \\begin{pmatrix} 2a &amp; 2b \\end{pmatrix}\\begin{pmatrix} u\\\\v \\end{pmatrix} = 0\\} = ... = \\{(u,v)\\in \\real^2 \\tq \\pesc{(a-0,b-0),(a,b)-(u,v)} = 0\\]. El espacio tangente es el formado por todos los vectores perpendiculares al radio. \\[M = \\{ (x,y,z,t)\\in \\real^4 \\tq x^2+y^2 = 1, z^2+t^2 = 1\\}\\] Vamos a calcular: \\(T_{(1,0,0,1)}M\\) … Tenemos definida la variedad de forma implícita: \\(M = \\{F = 0\\}\\) donde \\[F(x,y,z,t) = (x^2+y^2-1,t^2+z^2-1)\\] \\[DF = \\begin{pmatrix} 2x&amp;2y&amp;0&amp;0\\\\ 0&amp;0&amp;2z&amp;2t \\end{pmatrix}\\] En este caso tenemos \\(rg(DF) = 2, \\forall (x,y,z,t)\\in \\real^4\\). (El único punto que podría dar rango 0 es el origen, que en este caso no pertenece a la subvariedad) Vamos con el espacio tangente: \\[T_{(1,0,0,1)}M = \\ker DF (1,0,0,1) = \\ker \\begin{pmatrix} 2&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;2 \\end{pmatrix} =\\] \\[ \\{ (\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4) \\in \\real^4 \\tq \\begin{pmatrix} 2&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;2 \\end{pmatrix} \\begin{pmatrix} \\alpha_1\\\\ \\alpha_2\\\\ \\alpha_3\\\\ \\alpha_4 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\} \\implies \\left\\{\\begin{array}{cc}2\\alpha_1 &amp;= 0\\\\ 2\\alpha_4 &amp; =0\\end{array}\\right.\\] Hiperplano tangente: \\[ \\begin{pmatrix} 1\\\\0\\\\0\\\\1 \\end{pmatrix} + \\alpha_2 \\begin{pmatrix} 0\\\\1\\\\0\\\\0 \\end{pmatrix} + \\alpha_3 \\begin{pmatrix} 0\\\\0\\\\1\\\\0 \\end{pmatrix}, \\alpha_2,\\alpha_3 \\in \\real \\] Y el hiperplano normal: \\[ \\begin{pmatrix} 1\\\\0\\\\0\\\\1 \\end{pmatrix} + \\mu \\begin{pmatrix} 1\\\\0 \\\\0\\\\0 \\end{pmatrix} + \\lambda \\begin{pmatrix} 0\\\\0\\\\0\\\\1 \\end{pmatrix}, \\lambda,\\mu \\in \\real \\] ¿Una posible parametrización de esto? \\[\\begin{align*} \\Phi : D \\subset \\real^2 \\longrightarrow &amp;\\real^4\\\\ (u,v) \\longrightarrow &amp;\\Phi(u,v) = (x,y,z,t) = \\left(cos(u),sen(u),cos(v),sen(v)\\right)\\\\ (u,v)\\in (-\\pi,\\pi)\\times(0,2\\pi) \\end{align*}\\] ¿¿Por qué \\((u,v)\\in (-\\pi,\\pi)\\times(0,2\\pi)\\) y no \\((u,v)\\in (0,2\\pi)\\times(0,2\\pi)\\)?? Porque el punto que tenemos \\((1,0,0,1) \\notin \\img\\). Vamos a calcular la el hiperplano tangente. Como tenemos una parametrización, calcularemos la imagen de la diferencial () \\[T_{(1,0,0,1)}M = \\img D\\Phi\\left(0,\\frac{\\pi}{2}\\right)\\] \\[D\\Phi = \\begin{pmatrix} -sen(u) &amp; 0\\\\ cos(u) &amp; 0\\\\ 0 &amp; -sen(v)\\\\ 0&amp;cos(v) \\end{pmatrix}\\] \\[D\\Phi\\left(0,\\frac{\\pi}{2}\\right) = \\begin{pmatrix} 0&amp;0\\\\1&amp;0\\\\0&amp;-1\\\\0&amp;0 \\end{pmatrix}\\] Entonces el hiperplano tangente es: \\[\\begin{pmatrix} 1\\\\0\\\\0\\\\1 \\end{pmatrix} + u \\begin{pmatrix} 0\\\\1\\\\0\\\\0 \\end{pmatrix} - v \\begin{pmatrix} 0\\\\0\\\\1\\\\0 \\end{pmatrix} \\] Lógico y normal que nos de el mismo hiperplano tangente si estamos trabajando con el mismo objeto (desde una parametrización o desde un conjunto de nivel). ¿Cierto y verdad que sí? El (una especie de flotador) se produce al girar una circunferencia de radio \\(R_1\\) en el plano \\(XZ\\) con el centro sobre el eje \\(X\\) a \\(R_2\\) del origen alrededor del eje \\(Z\\). \\[ \\Phi(\\theta, \\phi) = ((R_2+R_1\\cos\\phi)\\cos\\theta,(R_2+R_1\\cos\\phi)\\sin \\theta,R1\\sin \\phi)\\;\\; \\theta, \\phi \\in (0, 2\\pi) \\] ¿Es \\(\\Phi\\) una parametrización? Vamos con la opción 1: \\[\\Phi(\\alpha,\\theta) = (x,y,z)\\] \\[¿(\\alpha,\\theta) = \\Phi^{-1} (x,y,z)?\\] \\[z = rsen(\\alpha) \\implies \\alpha = arcsen\\left(\\frac{z}{r}\\right)\\] \\[\\frac{y}{x} = tg(\\theta) \\implies \\theta = arctg \\left(\\frac{y}{x}\\right)\\] %Revisar Esto no es suficiente, porque \\(\\appl{arcsen}{(-1,1)}{\\left(\\frac{-\\pi}{2},\\frac{\\pi}{2}\\right)}\\) no toma todos los posibles valores. Lo mismo con la \\(\\appl{arctg}{\\real}{\\left(\\frac{-\\pi}{2},\\frac{\\pi}{2}\\right)}\\) Tenemos que repetir el argumento de () que ya hemos hecho para la \\(arctg\\) vamos a repetirlo para el \\(arcsen\\). \\[f(x,y,z) =\\left\\{ \\begin{matrix} Si &amp; (x,y,z) \\in 1 &amp;\\implies &amp;\\arcsin\\left(\\frac{z}{r}\\right)\\\\ Si &amp; (x,y,z) \\in 2 &amp;\\implies &amp;\\displaystyle\\frac{\\alpha + \\arcsin\\left(\\frac{z}{r}\\right)}{2} = \\frac{\\pi}{2}\\\\ Si &amp; (x,y,z) \\in 3 &amp;\\implies &amp; \\alpha = \\arcsin\\left(\\frac{z}{r}\\right) + 2\\pi \\end{matrix}\\right. \\] Ahora tenemos que estudiar la continuidad des esta función. (Ejercicio para el lector) En este caso (al tratarse de una diferencial) tendremos que hallar la imagen de \\(D\\Phi\\). Ejercicio propuesto: \\(T_{\\left(\\frac{\\pi}{2},\\frac{\\pi}{2}\\right)}M\\). \\begin{problem}[4.4] Sea \\(\\sigma(t) = \\left(\\cos t,\\sen t,t^2(2\\pi - t)^2\\right), t \\in [0,2\\pi]\\). Siempre hemos hablado de parametrizaciones es espacios abiertos, para asegurar la condición de homeomorfismo en casos como este, en el que los puntos cerca de \\(0\\), la inversa me los lleva a puntos lejanos si nos acercamos a \\(0\\) por el \\(0\\) o por el \\(2\\pi\\). Posibles soluciones "],
["espacios-de-banach.html", "Capítulo 27 Espacios de Banach 27.1 Espacios normados 27.2 Separación de conjuntos convexos 27.3 Teorema de prolongamiento 27.4 Duales de los espacios \\(\\ell^p\\) 27.5 Convergencia débil 27.6 Teorema de Banach-Steinhaus 27.7 Espacios reflexivos 27.8 Operadores contínuos y compactos 27.9 Teorema de Fredholm-Riesz 27.10 Aplicaciones abiertos y grafos cerrados 27.11 Caso complejo", " Capítulo 27 Espacios de Banach 27.1 Espacios normados 27.2 Separación de conjuntos convexos 27.3 Teorema de prolongamiento 27.4 Duales de los espacios \\(\\ell^p\\) 27.5 Convergencia débil 27.6 Teorema de Banach-Steinhaus 27.7 Espacios reflexivos 27.8 Operadores contínuos y compactos 27.9 Teorema de Fredholm-Riesz 27.10 Aplicaciones abiertos y grafos cerrados 27.11 Caso complejo "],
["espacios-convexos.html", "Capítulo 28 Espacios convexos 28.1 Familias de seminormas 28.2 Teorema de separación y de prolongamiento 28.3 Teorema de Krein-Milman", " Capítulo 28 Espacios convexos 28.1 Familias de seminormas 28.2 Teorema de separación y de prolongamiento 28.3 Teorema de Krein-Milman "],
["conjuntos-medibles.html", "Capítulo 29 Conjuntos medibles 29.1 Introducción 29.2 Espacios medibles 29.3 Medidas 29.4 Espacios medibles completos 29.5 Medida externa y medibilidad 29.6 Extensión de una medida 29.7 Medida de Lebesgue 29.8 Medida de Lebesgue Stieltjes 29.9 Conjuntos especiales", " Capítulo 29 Conjuntos medibles 29.1 Introducción 29.2 Espacios medibles 29.3 Medidas 29.4 Espacios medibles completos 29.5 Medida externa y medibilidad 29.6 Extensión de una medida 29.7 Medida de Lebesgue 29.8 Medida de Lebesgue Stieltjes 29.9 Conjuntos especiales "],
["funciones-medibles.html", "Capítulo 30 Funciones medibles 30.1 Transformaciones medibles 30.2 Funciones numéricas medibles 30.3 Funciones simples 30.4 Convergencia de funciones medibles", " Capítulo 30 Funciones medibles 30.1 Transformaciones medibles 30.2 Funciones numéricas medibles 30.3 Funciones simples 30.4 Convergencia de funciones medibles "],
["integracion-1.html", "Capítulo 31 Integración 31.1 Construcción de la integral 31.2 Propiedades básicas de la integral 31.3 Conexión con la integral de Riemann en \\(\\mathbb{R}^n\\) 31.4 Teoremas de convergencia 31.5 Integración sobre una medida producto 31.6 Aplicaciones del teorema de Fubini", " Capítulo 31 Integración 31.1 Construcción de la integral 31.2 Propiedades básicas de la integral 31.3 Conexión con la integral de Riemann en \\(\\mathbb{R}^n\\) 31.4 Teoremas de convergencia 31.5 Integración sobre una medida producto 31.6 Aplicaciones del teorema de Fubini "],
["espacios-lp.html", "Capítulo 32 Espacios \\(L^p\\) 32.1 Definición y propiedades generales 32.2 Aproximación en \\(L^p\\) 32.3 Convergencia en \\(L^p\\) 32.4 Integrabilidad uniforme 32.5 Funciones convexas y desigualdad de Jensen", " Capítulo 32 Espacios \\(L^p\\) 32.1 Definición y propiedades generales 32.2 Aproximación en \\(L^p\\) 32.3 Convergencia en \\(L^p\\) 32.4 Integrabilidad uniforme 32.5 Funciones convexas y desigualdad de Jensen "],
["diferenciacion-2.html", "Capítulo 33 Diferenciación 33.1 Medidas con signo 33.2 Medidas complejas 33.3 Continuidad absoluta de medidas 33.4 Diferenciación de medidas 33.5 Funciones a variación acotada 33.6 Funciones absolutamente contínuas", " Capítulo 33 Diferenciación 33.1 Medidas con signo 33.2 Medidas complejas 33.3 Continuidad absoluta de medidas 33.4 Diferenciación de medidas 33.5 Funciones a variación acotada 33.6 Funciones absolutamente contínuas "],
["analisis-de-fourier-en-mathbbrn.html", "Capítulo 34 Análisis de Fourier en \\(\\mathbb{R}^n\\) 34.1 Convolución de funciones 34.2 La transformada de Fourier 34.3 Funciones de rápido decrecimiento 34.4 Análisis de Fourier de medidas en \\(\\mathbb{R}^n\\)", " Capítulo 34 Análisis de Fourier en \\(\\mathbb{R}^n\\) 34.1 Convolución de funciones 34.2 La transformada de Fourier 34.3 Funciones de rápido decrecimiento 34.4 Análisis de Fourier de medidas en \\(\\mathbb{R}^n\\) "],
["medidas-en-espacios-localmente-compactos.html", "Capítulo 35 Medidas en espacios localmente compactos 35.1 Medidas de Radon 35.2 El teorema de representación de Riesz 35.3 Productos de medidas de Radon 35.4 El operador dual 35.5 Operadores compactos", " Capítulo 35 Medidas en espacios localmente compactos 35.1 Medidas de Radon 35.2 El teorema de representación de Riesz 35.3 Productos de medidas de Radon 35.4 El operador dual 35.5 Operadores compactos "],
["espacios-localmente-convexos.html", "Capítulo 36 Espacios localmente convexos 36.1 Propiedades generales 36.2 Funcionales lineales contínuos 36.3 Teoremas de separación de Hahn-Banach 36.4 Algunas construcciones", " Capítulo 36 Espacios localmente convexos 36.1 Propiedades generales 36.2 Funcionales lineales contínuos 36.3 Teoremas de separación de Hahn-Banach 36.4 Algunas construcciones "],
["topologias-debiles-en-espacios-normados.html", "Capítulo 37 Topologías débiles en espacios normados 37.1 Topología débil 37.2 Topología débil\\(^*\\) 37.3 Espacios reflexivos 37.4 Espacios uniformemente convexos", " Capítulo 37 Topologías débiles en espacios normados 37.1 Topología débil 37.2 Topología débil\\(^*\\) 37.3 Espacios reflexivos 37.4 Espacios uniformemente convexos "],
["espacios-de-hilbert-1.html", "Capítulo 38 Espacios de Hilbert 38.1 Principios generales 38.2 Ortogonalidad 38.3 Bases ortonormales 38.4 El adjunto del espacio de Hilbert", " Capítulo 38 Espacios de Hilbert 38.1 Principios generales 38.2 Ortogonalidad 38.3 Bases ortonormales 38.4 El adjunto del espacio de Hilbert "],
["teoria-de-operadores.html", "Capítulo 39 Teoría de operadores 39.1 Tipos de operadores 39.2 Operadores compactos y de rango finito 39.3 El teorema espectral para operadores normales compactos 39.4 El álgebra del grupo \\(L^1\\) 39.5 Representaciones 39.6 Grupos abelianos localmente compactos", " Capítulo 39 Teoría de operadores 39.1 Tipos de operadores 39.2 Operadores compactos y de rango finito 39.3 El teorema espectral para operadores normales compactos 39.4 El álgebra del grupo \\(L^1\\) 39.5 Representaciones 39.6 Grupos abelianos localmente compactos "],
["analisis-en-semigrupos.html", "Capítulo 40 Análisis en semigrupos 40.1 Semigrupos con topologías 40.2 Funciones debilmente casi periódicas 40.3 La estructura de los semigrupos compactos 40.4 Funciones fuertemente casi periódicas 40.5 Semigrupo de operadores", " Capítulo 40 Análisis en semigrupos 40.1 Semigrupos con topologías 40.2 Funciones debilmente casi periódicas 40.3 La estructura de los semigrupos compactos 40.4 Funciones fuertemente casi periódicas 40.5 Semigrupo de operadores "],
["teoria-de-probabilidades.html", "Capítulo 41 Teoría de probabilidades 41.1 Variables aleatorias 41.2 Independencia 41.3 Esperanza condicional 41.4 Sucesiones de variables aleatorias independientes 41.5 Martingalas a tiempo discreto 41.6 Procesos estocásticos 41.7 Movimiento browniano 41.8 Integración estocástica 41.9 Aplicación a las finanzas", " Capítulo 41 Teoría de probabilidades 41.1 Variables aleatorias 41.2 Independencia 41.3 Esperanza condicional 41.4 Sucesiones de variables aleatorias independientes 41.5 Martingalas a tiempo discreto 41.6 Procesos estocásticos 41.7 Movimiento browniano 41.8 Integración estocástica 41.9 Aplicación a las finanzas "],
["apendice.html", "Capítulo 42 Apéndice", " Capítulo 42 Apéndice "],
["software-tools.html", "A Software Tools A.1 R and R packages A.2 Pandoc A.3 LaTeX", " A Software Tools For those who are not familiar with software packages required for using R Markdown, we give a brief introduction to the installation and maintenance of these packages. A.1 R and R packages R can be downloaded and installed from any CRAN (the Comprehensive R Archive Network) mirrors, e.g., https://cran.rstudio.com. Please note that there will be a few new releases of R every year, and you may want to upgrade R occasionally. To install the bookdown package, you can type this in R: install.packages(&quot;bookdown&quot;) This installs all required R packages. You can also choose to install all optional packages as well, if you do not care too much about whether these packages will actually be used to compile your book (such as htmlwidgets): install.packages(&quot;bookdown&quot;, dependencies = TRUE) If you want to test the development version of bookdown on GitHub, you need to install devtools first: if (!requireNamespace(&#39;devtools&#39;)) install.packages(&#39;devtools&#39;) devtools::install_github(&#39;rstudio/bookdown&#39;) R packages are also often constantly updated on CRAN or GitHub, so you may want to update them once in a while: update.packages(ask = FALSE) Although it is not required, the RStudio IDE can make a lot of things much easier when you work on R-related projects. The RStudio IDE can be downloaded from https://www.rstudio.com. A.2 Pandoc An R Markdown document (*.Rmd) is first compiled to Markdown (*.md) through the knitr package, and then Markdown is compiled to other output formats (such as LaTeX or HTML) through Pandoc. This process is automated by the rmarkdown package. You do not need to install knitr or rmarkdown separately, because they are the required packages of bookdown and will be automatically installed when you install bookdown. However, Pandoc is not an R package, so it will not be automatically installed when you install bookdown. You can follow the installation instructions on the Pandoc homepage (http://pandoc.org) to install Pandoc, but if you use the RStudio IDE, you do not really need to install Pandoc separately, because RStudio includes a copy of Pandoc. The Pandoc version number can be obtained via: rmarkdown::pandoc_version() ## [1] &#39;2.5&#39; If you find this version too low and there are Pandoc features only in a later version, you can install the later version of Pandoc, and rmarkdown will call the newer version instead of its built-in version. A.3 LaTeX LaTeX is required only if you want to convert your book to PDF. The typical choice of the LaTeX distribution depends on your operating system. Windows users may consider MiKTeX (http://miktex.org), Mac OS X users can install MacTeX (http://www.tug.org/mactex/), and Linux users can install TeXLive (http://www.tug.org/texlive). See https://www.latex-project.org/get/ for more information about LaTeX and its installation. Most LaTeX distributions provide a minimal/basic package and a full package. You can install the basic package if you have limited disk space and know how to install LaTeX packages later. The full package is often significantly larger in size, since it contains all LaTeX packages, and you are unlikely to run into the problem of missing packages in LaTeX. LaTeX error messages may be obscure to beginners, but you may find solutions by searching for the error message online (you have good chances of ending up on StackExchange). In fact, the LaTeX code converted from R Markdown should be safe enough and you should not frequently run into LaTeX problems unless you introduced raw LaTeX content in your Rmd documents. The most common LaTeX problem should be missing LaTeX packages, and the error may look like this: ! LaTeX Error: File `titling.sty&#39; not found. Type X to quit or &lt;RETURN&gt; to proceed, or enter new name. (Default extension: sty) Enter file name: ! Emergency stop. &lt;read *&gt; l.107 ^^M pandoc: Error producing PDF Error: pandoc document conversion failed with error 43 Execution halted This means you used a package that contains titling.sty, but it was not installed. LaTeX package names are often the same as the *.sty filenames, so in this case, you can try to install the titling package. Both MiKTeX and MacTeX provide a graphical user interface to manage packages. You can find the MiKTeX package manager from the start menu, and MacTeX’s package manager from the application “TeX Live Utility”. Type the name of the package, or the filename to search for the package and install it. TeXLive may be a little trickier: if you use the pre-built TeXLive packages of your Linux distribution, you need to search in the package repository and your keywords may match other non-LaTeX packages. Personally, I find it frustrating to use the pre-built collections of packages on Linux, and much easier to install TeXLive from source, in which case you can manage packages using the tlmgr command. For example, you can search for titling.sty from the TeXLive package repository: tlmgr search --global --file titling.sty # titling: # texmf-dist/tex/latex/titling/titling.sty Once you have figured out the package name, you can install it by: tlmgr install titling # may require sudo LaTeX distributions and packages are also updated from time to time, and you may consider updating them especially when you run into LaTeX problems. You can find out the version of your LaTeX distribution by: system(&#39;pdflatex --version&#39;) ## pdfTeX 3.14159265-2.6-1.40.18 (TeX Live 2017) ## kpathsea version 6.2.3 ## Copyright 2017 Han The Thanh (pdfTeX) et al. ## There is NO warranty. Redistribution of this software is ## covered by the terms of both the pdfTeX copyright and ## the Lesser GNU General Public License. ## For more information about these matters, see the file ## named COPYING and the pdfTeX source. ## Primary author of pdfTeX: Han The Thanh (pdfTeX) et al. ## Compiled with libpng 1.6.29; using libpng 1.6.29 ## Compiled with zlib 1.2.11; using zlib 1.2.11 ## Compiled with xpdf version 3.04 "],
["referencias.html", "Referencias", " Referencias "]
]
