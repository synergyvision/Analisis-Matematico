[
["index.html", "Análisis, Medida y Probabilidades Ciencia de los Datos Financieros Prefacio ¿Por qué leer este libro? Estructura del libro Información sobre los programas y convenciones Prácticas interactivas con R Agradecimientos", " Análisis, Medida y Probabilidades Ciencia de los Datos Financieros Synergy Vision 2019-04-03 Prefacio La versión en línea de este libro se comparte bajo la licencia Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. ¿Por qué leer este libro? Este libro es el resultado de enfocarnos en proveer la mayor cantidad de material sobre Análisis y teoría de la medida con un desarrollo teórico lo más explícito posible, con el valor agregado de incorporar ejemplos de las finanzas y la programación en R. Finalmente tenemos un libro interactivo que ofrece una experiencia de aprendizaje distinta e innovadora. El un mundo abierto, ya no es tanto el acceso a la información, sino el acceso al conocimiento. Es mucha la literatura, pero son pocas las opciones donde se pueda navegar el libro de forma amigable y además contar con ejemplos en R y ejercicios interactivos, además del contenido multimedia. Esperamos que ésta sea un contribución sobre nuevas prácticas para publicar el contenido y darle vida, crear una experiencia distinta, una experiencia interactiva y visual. El reto es darle vida al contenido asistidos con las herramientas de Internet. Finalmente este es un intento de ofrecer otra visión sobre la enseñanza y la generación de material más accesible. Estamos en un mundo multidisciplinado, es por ello que ahora hay que generar contenido que conjugue en un mismo lugar las matemáticas, estadística, finanzas y la computación. Lo dejamos público ya que las herramientas que usamos para ensamblarlo son abiertas y públicas. Estructura del libro TODO: Describir la estructura Información sobre los programas y convenciones Este libro es posible gracias a una gran cantidad de desarrolladores que contribuyen en la construcción de herramientas para generar documentos enriquecidos e interactivos. En particular al autor de los paquetes Yihui Xie xie2015. Prácticas interactivas con R Vamos a utilizar el paquete Datacamp Tutorial que utiliza la librería en JavaScript Datacamp Light para crear ejercicios y prácticas con R. De esta forma el libro es completamente interactivo y con prácticas incluidas. De esta forma estamos creando una experiencia única de aprendizaje en línea. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6ImIgPC0gNSIsInNhbXBsZSI6IiMgQ3JlYSB1bmEgdmFyaWFibGUgYSwgaWd1YWwgYSA1XG5cblxuIyBNdWVzdHJhIGVsIHZhbG9yIGRlIGEiLCJzb2x1dGlvbiI6IiMgQ3JlYSB1bmEgdmFyaWFibGUgYSwgaWd1YWwgYSA1XG5hIDwtIDVcblxuIyBNdWVzdHJhIGVsIHZhbG9yIGRlIGFcbmEiLCJzY3QiOiJ0ZXN0X29iamVjdChcImFcIilcbnRlc3Rfb3V0cHV0X2NvbnRhaW5zKFwiYVwiLCBpbmNvcnJlY3RfbXNnID0gXCJBc2VnJnVhY3V0ZTtyYXRlIGRlIG1vc3RyYXIgZWwgdmFsb3IgZGUgYGFgLlwiKVxuc3VjY2Vzc19tc2coXCJFeGNlbGVudGUhXCIpIn0= Agradecimientos A todo el equipo de Synergy Vision que no deja de soñar. Hay que hacer lo que pocos hacen, insistir, insistir hasta alcanzar. Lo más importante es concretar las ideas. La idea es sólo el inicio y solo vale cuando se concreta. Synergy Vision, Caracas, Venezuela "],
["acerca-del-autor.html", "Acerca del Autor", " Acerca del Autor Este material es un esfuerzo de equipo en Synergy Vision, (http://synergy.vision/nosotros/). El propósito de este material es ofrecer una experiencia de aprendizaje distinta y enfocada en el estudiante. El propósito es que realmente aprenda y practique con mucha intensidad. La idea es cambiar el modelo de clases magistrales y ofrecer una experiencia más centrada en el estudiante y menos centrado en el profesor. Para los temas más técnicos y avanzados es necesario trabajar de la mano con el estudiante y asistirlo en el proceso de aprendizaje con prácticas guiadas, material en línea e interactivo, videos, evaluación contínua de brechas y entendimiento, entre otros, para procurar el dominio de la materia. Nuestro foco es la Ciencia de los Datos Financieros y para ello se desarrollará material sobre: Probabilidad y Estadística Matemática en R, Programación Científica en R, Mercados, Inversiones y Trading, Datos y Modelos Financieros en R, Renta Fija, Inmunización de Carteras de Renta Fija, Teoría de Riesgo en R, Finanzas Cuantitativas, Ingeniería Financiera, Procesos Estocásticos en R, Series de Tiempo en R, Ciencia de los Datos, Ciencia de los Datos Financieros, Simulación en R, Desarrollo de Aplicaciones Interactivas en R, Minería de Datos, Aprendizaje Estadístico, Estadística Multivariante, Riesgo de Crédito, Riesgo de Liquidez, Riesgo de Mercado, Riesgo Operacional, Riesgo de Cambio, Análisis Técnico, Inversión Visual, Finanzas, Finanzas Corporativas, Valoración, Teoría de Portafolio, entre otros. Nuestra cuenta de Twitter es (https://twitter.com/bysynergyvision) y nuestros repositorios están en GitHub (https://github.com/synergyvision). Somos Científicos de Datos Financieros "],
["introduccion.html", "Capítulo 1 Introducción", " Capítulo 1 Introducción "],
["conjuntos.html", "Capítulo 2 Conjuntos 2.1 Operaciones conjuntistas 2.2 Operaciones conjuntistas 2.3 Sistemas numéricos 2.4 Relaciones 2.5 Cardinalidad", " Capítulo 2 Conjuntos Los términos conjunto, colección y familia suelen ser sinónimos, a pesar de que en algunos contextos una de estas palabras tiene prioridad sobre otra. Por ejemplo, se suele decir colección de conjuntos y familia de funciones. Los conjuntos usualmente se denotan con una letra mayúscula, mientras que para los elementos de un conjunto se utilizan minúsculas. La notación \\(a \\in A\\) indica que el elemento \\(a\\) pertenece al conjunto \\(A\\). Por ejemplo, \\[2 \\in \\{a, 3, 2, b\\}\\] indica que el elemento \\(2\\) pertenece al conjunto \\(\\{a, 3, 2, b\\}\\). Nótese que el orden en que se escriben los elementos de un conjunto es indiferente. Así el conjunto anterior es igual a \\[\\{3,b,a,2\\}.\\] Las llaves \\(\\{ \\ \\}\\) siempre se usarán en la definición de conjuntos. En general, para describir a un conjunto exsisten dos formas de hacerlo. Cuando un conjunto se define dando la lista completa de todos sus elementos, decimo por ejemplo 2.1 Operaciones conjuntistas Si \\(A\\) es un subconjunto de \\(B\\), escribimos \\(A \\subseteq B\\). Si todos los conjuntos con los que se trabaja son subconjuntos de un conjunto \\(X\\), entonces \\(X\\) es llamado el conjunto universal. El conjunto potencia de un conjunto \\(X\\) es la colección \\(\\mathcal{P}(X)\\) de todos los subconjuntos de \\(X\\), es decir: \\[ \\mathcal{P}(X) = \\{Y \\, : \\, Y \\subseteq X\\}.\\] Si \\(A, B \\subseteq X\\), entonces \\(A \\cup B\\), \\(A \\cap B\\) y \\(A \\setminus B\\) denotan la unión, intersección y diferencia relativa de \\(A\\) y \\(B\\), El complemento de \\(A\\), definido como los elementos de \\(X\\) que no están en \\(A\\), se denota por \\(A^c\\). Si \\(\\mathcal{A}\\) es un subconjunto de \\(\\mathcal{P}(X)\\) y \\(B \\in \\mathcal{P}(X)\\), se define la traza de \\(\\mathcal{A}\\) en \\(B\\) como \\[\\mathcal{A}\\cap B := \\{A \\cap B \\, : \\, A \\in \\mathcal{A}\\}.\\] La unión e intersección de una familia indexada \\(\\mathcal{A}=\\{A_i \\, : \\, i \\in \\mathcal{I}\\}\\) se denotan, respectivamente, como \\[\\cap \\mathcal{A}:= \\cap_{i \\, \\in \\, \\mathcal{I}},\\] y \\[\\cup \\mathcal{A}:= \\cup_{i \\, \\in \\, \\mathcal{I}},\\] Si el conjunto de índices \\(\\mathcal{I}\\) es \\(\\{1,2, \\hdots, n\\}\\) o \\(\\{1,2, \\hdots\\}\\), se escribe \\[\\cup_{j=1}^n A_j=A_1 \\cup \\hdots \\cup A_n\\] \\[\\cap_{j=1}^n A_j=A_1 \\cap \\hdots \\cap A_n\\] \\[\\cup_{j \\geq 1} A_j=A_1 \\cup A_2 \\cup \\cdots\\] \\[\\cap_{j \\geq 1} A_j=A_1 \\cap A_2 \\cap \\cdots\\] 2.2 Operaciones conjuntistas Si \\(A\\) es un subconjunto de \\(B\\), escribimos \\(A \\subseteq B\\). Si todos los conjuntos con los que se trabaja son subconjuntos de un conjunto \\(X\\), entonces \\(X\\) es llamado el conjunto universal. El conjunto potencia de un conjunto \\(X\\) es la colección \\(\\mathcal{P}(X)\\) de todos los subconjuntos de \\(X\\), es decir: \\[ \\mathcal{P}(X) = \\{Y \\, : \\, Y \\subseteq X\\}.\\] Si \\(A, B \\subseteq X\\), entonces \\(A \\cup B\\), \\(A \\cap B\\) y \\(A \\setminus B\\) denotan la unión, intersección y diferencia relativa de \\(A\\) y \\(B\\), El complemento de \\(A\\), definido como los elementos de \\(X\\) que no están en \\(A\\), se denota por \\(A^c\\). Si \\(\\mathcal{A}\\) es un subconjunto de \\(\\mathcal{P}(X)\\) y \\(B \\in \\mathcal{P}(X)\\), se define la traza de \\(\\mathcal{A}\\) en \\(B\\) como \\[\\mathcal{A}\\cap B := \\{A \\cap B \\, : \\, A \\in \\mathcal{A}\\}.\\] La unión e intersección de una familia indexada \\(\\mathcal{A}=\\{A_i \\, : \\, i \\in \\mathcal{I}\\}\\) se denotan, respectivamente, como \\[\\cap \\mathcal{A}:= \\cap_{i \\, \\in \\, \\mathcal{I}},\\] y \\[\\cup \\mathcal{A}:= \\cup_{i \\, \\in \\, \\mathcal{I}},\\] Si el conjunto de índices \\(\\mathcal{I}\\) es \\(\\{1,2, \\hdots, n\\}\\) o \\(\\{1,2, \\hdots\\}\\), se escribe \\[\\cup_{j=1}^n A_j=A_1 \\cup \\hdots \\cup A_n\\] \\[\\cap_{j=1}^n A_j=A_1 \\cap \\hdots \\cap A_n\\] \\[\\cup_{j \\geq 1} A_j=A_1 \\cup A_2 \\cup \\cdots\\] \\[\\cap_{j \\geq 1} A_j=A_1 \\cap A_2 \\cap \\cdots\\] \\begin{prop} La unión e intersección de conjuntos satisfacen: \\begin{itemize} {} \\end{prop} Una familia \\(\\{A_i \\, : \\, i \\in \\mathcal{I}\\}\\) de conjuntos se dice {} si \\(A_i \\cap A_j=\\emptyset\\), siempre que \\(i \\neq j\\). En este caso, la uni'on \\(\\bigcup_{i \\in \\mathcal{I}}\\) se dice {}. Una {} (o {}) de un conjunto \\(X\\) es una colecci'on de conuntos disjuntos no vac'ios , cuya uni'on disjunta es todo \\(X\\). Una sucesi'on de conjuntos \\(\\{A_n\\}_{n \\geq 0}\\) se dice {} si \\(A_1 \\leq subseteq A_2 \\subseteq \\cdots\\), en cuyo caso escribiremos \\(A_n \\uparrow\\). Si \\(A_1 \\supseteq A_2 \\supseteq \\cdots\\), se dice que la sucesi'on es {}. Denotaremos por \\(A_n \\downarrow\\) a una sucesi'on que es decreciente. Si \\(\\{A_n\\}_{n \\geq 0}\\) es creciente y \\(A=A_1 \\cup A_2 \\cup \\cdots\\) (respectivamente si \\(\\{A_n\\}_{n \\geq 0}\\) es decreciente y \\(A=A_1 \\cap A_2 \\cap \\cdots\\)), escribiremos \\(A_n \\uparrow A\\) (respectivamente \\(A_n \\downarrow A\\)). El producto cartesiano de sucesiones finitas o infinitas numerables de conjuntos \\(A_1, A_2, \\hdots\\) se denotan, respectivamente, por \\(\\begin{array}[ccc] \\prod_{n=1}^d A_n = A_1 \\times \\cdots \\times A_d &amp;y&amp; \\prod_{n=1}^\\infty A_n = A_1 \\times A_2 \\times \\cdots. \\end{array}\\) Si \\(A_1 = A_2 = \\cdots = A\\), escribiremos \\(A^d\\) para el producto finito y \\(A^\\infty\\) para el producto infinito. El producto cartesiano \\(X:=\\prod_{i \\in \\mathcal{I}}X_i\\) de una familia de conjuntos no vacios \\(X_i\\) se define como la colecci'on de todas las funciones \\(f:\\mathcal{I}\\to \\bigcup_{i \\in \\mathcal{I}}X_i\\) tales que \\(f(i)\\in X_i\\), para cada \\(i\\). N'otese que el producto cartesiano de conjuntos no vac'ios es siempre no vac'io, por el axioma de elecci'on. El valor \\(f(i)\\) se denomina la {}. En el caso particular en que el conjunto de 'indices \\(\\mathcal{I}\\) sea el conjunto \\(\\{1,2, \\hdots, n\\}\\), se tiene que toda funci'on \\(f: \\{1,2, \\hdots, n\\}\\to X\\) est'a completamente determinada por por la \\(n\\)-tupla \\((f(1), f(2), \\dots, f(n))\\). 2.3 Sistemas numéricos Usaremos la siguiente notaci'on para el sistema num'e rico est'andar: Los términos conjuntos, colecciones y familias son sinónimos, a pesar de que en algunos contextos se tenga mas preferencia sobre una frase que otra. Por ejemplo, se suele decir coleccion de objetos y familia de funciones. Los conjuntos se denotan usualmente con una letra mayuscula, y elementos de un conjunto con una letra minuscula. Se usa la notacion \\[a \\in A\\] para indicar que el elemento \\(a\\) pertenece al conjunto \\(A\\). Existen dos maneras de representar a un conjunto (comprehensiva y extensiva, ver notas de Manuel.) Dos subconjuntos de \\(\\mathbb{C}\\) son de particular importancia: En general, el s'imbolo \\(\\mathbb{K}\\) denota a un cuerpo de caracter'istica \\(0\\), como por ejemplo \\(\\mathbb{Q}\\), \\(\\mathbb{R}\\) o \\(\\mathbb{C}\\). Si \\(A \\subseteq \\mathbb{R}\\), escribiremos \\(A^+\\) para denotar al conjunto \\(A \\cap [0, \\infty)\\). Asi, por ejemplo, se tiene que \\(\\mathbb{Z}^+=\\{n \\in \\mathbb{Z} \\, : \\, n \\geq 0\\}=\\mathbb{N}\\). Si \\(A \\subseteq \\mathbb{C}\\), se denota por \\(A_*\\) al conjunto de elementos no nulos de \\(A\\). Sea \\(d\\) un entero positivo. Si \\(A\\) es un conjunto, se define \\[A^d:=A \\times A \\times \\cdots \\times A.\\] Los {} se definen, respectivamente, como \\(\\mathbb{R}^d\\) y \\(\\mathbb{C}^d\\), con \\(d \\in \\mathbb{P}\\). Un {} en \\(\\mathbb{R}^d\\) es un producto cartesiano de intervalos en \\(\\mathbb{R}\\). Es decir, si \\({\\bf a}:=(a_1, a_2, \\hdots, a_d)\\) y \\({\\bf b}:=(b_1, b_2, \\hdots, b_d)\\) son elementos de \\(\\mathbb{R}^d\\), un intervalo \\(d\\)-dimensional es de la forma \\[({\\bf a}, {\\bf b}]:=(a_1, b_1] \\times (a_2, b_2] \\times \\cdots \\times (a_d, b_d].\\] La {} en \\(\\mathbb{K}^d\\) se define como \\[|{\\bf z}|:=\\sqrt{|z_1|^2+|z_2|^2+ \\cdots + |z_d|^2},\\] para cada \\({\\bf z}=(z_1, z_2, \\hdots, z_d) \\in \\mathbb{C}^d\\). El {} es el conjunto \\[\\overline{\\mathbb{R}}:=\\mathbb{R} \\cup \\{\\pm \\infty\\}=[-\\infty, +\\infty].\\] Aqui, \\(-\\infty\\) y \\(+ \\infty\\) denotan dos s'imbolos abstractos, que satisfacen los siguientes axiomas: Los elementos de \\([0, +\\infty]=\\overline{\\mathbb{R}}^+\\) se denominan {}. 2.4 Relaciones Una {} en un conjunto no vac'io \\(X\\) es un conjunto \\(R \\subseteq X \\times X\\), no vac'io. Si \\(R\\) es una relaci'on de \\(X\\) y \\((x,y) \\in R\\), se escribe \\(x \\sim_R y\\), o \\(x \\sim y\\). Una relaci'on de \\(X\\) se dice: Una relaci'on en \\(X\\) que es reflexiva, sim'etrica y transitiva se conoce como {}. La {} es el conjunto \\[[x]:= \\{y \\in X \\, : \\, x \\sim y\\}.\\] La colecci'on de clases de equivalencias distintas de \\(X\\) se denota como \\(X/\\sim\\). As'i: \\[X/\\sim = \\{[x] \\, : \\, x \\in X\\}.\\] Las relaciones de equivalencia de un conjunto no vac'io X y las particiones conjuntistas de \\(X\\) son dos nociones equivalentes. En efecto, dado que \\[\\begin{equation} x \\in [y] \\Longleftrightarrow y \\in [x] \\Longleftrightarrow [x]=[y], \\end{equation}\\] es claro que los elementos de \\(X/\\sim\\) son disjuntos dos a dos. Adem'as, \\[\\begin{equation} \\bigcup_{[x] \\, \\in \\, X/\\sim}[x]=\\{y \\in X \\, : \\, x \\sim y, \\text{ para alg\\&#39;un } x \\in X\\}=X. \\end{equation}\\] As'i, la colecci'on \\(X/\\sim\\) es una partici'on de \\(X\\). Rec'iprocamente, dada una partici'on de \\(X\\), la relaci'on definida por \\[\\begin{equation} x \\sim y \\Longleftrightarrow x \\text{ y } y \\text{ pertenecen al mismo bloque de la partici\\&#39;on } \\end{equation}\\] es una relaci'on de equivalencia para \\(X\\), donde cada clase de equivalencia es precisamente un bloque de la partici'on. Un ejemplo de relaci'on de equivalencia en \\(\\mathbb{R}\\) es el siguiente: la relaci'on \\(\\sim\\) est'a dada por \\[\\begin{equation} x\\sim y \\Longleftrightarrow x-y \\in \\mathbb{Q}. \\end{equation}\\] En este caso, existe solo una clase \\([x] \\in \\mathbb{R}/\\sim\\) formada por n'umeros racionales. De hecho, no es dif'icil demostrar que \\([x]=\\mathbb{Q}\\). Cualquier otra clase \\([y]\\) de \\(\\mathbb{R}/\\sim\\) est'a formada por n'umeros irracionales. Una relaci'on que es reflexiva, antisim'etrica y transitiva es llamada un {}. En este caso, se escribe \\(x \\leq y\\) para denotar que el par \\((x,y)\\) pertenecen a este tipo de relaci'on. Un conjunto con un orden parcial se denomina {}. Una {} (respectivamente, {}) de un subconjunto \\(Y\\) de un conjunto parcialmente ordenado \\((X, \\leq)\\) es un elemento \\(x \\in X\\) tal que \\(y \\leq x\\) (respectivamente, \\(x \\leq y\\)), para todo \\(y \\in Y\\). Un {} (respectivamente, un {})de \\(Y\\) es una cota superior (respectivamente, inferior) \\(x_0\\) de \\(Y\\) tal que \\(x_0 \\leq x\\) (respectivamente, \\(x_0 \\geq x\\)) para toda cota superior (respectivamente, inferior) \\(x\\) de \\(Y\\). Por ejemplo, el par \\((\\mathcal{P}(X), \\subseteq)\\) forma un conjunto parcialmente ordenado, para cada conjunto \\(X\\). Si \\(Y \\in \\mathcal{P}(X)\\), entonces los subconjuntos \\(\\cap_{y \\, \\in \\, Y}y\\) y \\(\\cup_{y \\, \\in \\, Y}y\\) corresponden al 'infimo y al supremo de \\(Y\\), respectivamente. Un elemento \\(x\\) de \\(X\\) se dice {} si para cada \\(y \\in X\\) tal que \\(x \\leq y\\), se tiene que \\(x=y\\). Del mismo modo, un elemento \\(x\\) de \\(X\\) se dice {} si para cada \\(y \\in X\\) tal que \\(x \\geq y\\), se tiene que \\(x=y\\). Un subconjunto no vac'io \\(Y\\) de un conjunto parcialmente ordenado \\((X, \\leq)\\) se dice {} si, para cada \\(y_1, y_2 \\in Y\\) distintos se satisface \\(y_1 \\leq y_2\\) o \\(y_2 \\leq y_1\\). Un conjunto totalmente ordendao tambi'en suele llamarse una {}. La noci'on de cadena aparece en el siguiente resultado importante, que se prueba utilizando directamente el axioma de elecci'on: Lema de Zorn: Sea \\(X\\) un conjunto parcialmente ordenado, tal que cada cadena posea una cota superior en \\(X\\). Entonces \\(X\\) posee un elemento maximal. ##Funciones Sean \\(X\\) y \\(Y\\) conjuntos. Una {funci'on} de \\(X\\) en \\(Y\\) es (completar). Una funci'on \\(f\\) cuyo conjunto de partida es \\(X\\) y conjunto de llegada es \\(Y\\) se denota por \\(f: X \\to Y\\). La colecci'on de todas las funciones de \\(X\\) en \\(Y\\) se denota como \\(Y^X\\). La {} de \\(A \\subseteq X\\) y la {preimagen} de \\(B \\subseteq Y\\) bajo la funci'on \\(f:X \\to Y\\) se definen, respectivamente, como \\[f(A):=\\{f(x) \\in B \\, : \\, x \\in A\\},\\] y \\[f^{-1}(A):=\\{x \\in A \\, : \\, f(x) \\in B\\}.\\] Una funci'on \\(f:X \\to Y\\) es {} si \\(f(X)=Y\\), e {} si \\(x_1 \\neq x_2\\) implica que \\(f(x_1)\\neq f(x_2)\\). Una {} (respectivamente, una {}) es una funci'on que es sobreyectiva (respectivamente, inyetiva). Una funci'on que es a la vez sobreyectiva e inyectiva se dice {}. Un ejemplo importante de funci'on sobreyectiva es la {funci'on cociente} \\(C_\\sim: X \\to X/\\sim\\), asociada a una relaci'on de equivalencia \\(\\sim\\) de un conjunto no vac'io \\(X\\). Por definici'on, \\(C_\\sim(x):=[x]\\). En particular, la preimagen de un subconjunto \\(B\\) de \\(X/\\sim\\) bajo \\(C_\\sim\\) es la uni'on de todas las clases de equivalencias \\([x]\\) in \\(B\\). Si \\(X\\), \\(Y\\) son conjuntos y \\(\\{X_i \\, : \\, i \\in \\mathcal{I}\\}\\), \\(\\{Y_j \\, : \\, j \\in \\mathcal{J}\\}\\) son subconjuntos de \\(X\\) y \\(Y\\), respectivamente, se tiene que: Las igualdades en \\((d)\\) y \\((h)\\) se satisfacen si \\(f\\) es inyectiva, mientras que las igualdades de \\((f)\\) y \\((g)\\) suceden si \\(f\\) es sobreyectiva. Para cada \\(f:X \\to Y\\), \\(\\mathcal{A} \\subseteq \\mathcal{P}(X)\\) y \\(\\mathcal{B} \\subseteq \\mathcal{P}(X)\\), se definen las colecciones \\[f(\\mathcal{A})=\\{f(A) \\, : \\, A \\in \\mathcal{A}\\} \\subseteq \\mathcal{P}(y)\\] \\[f^{-1}(\\mathcal{B})=\\{f^{-1}(B) \\, : \\, B \\in \\mathcal{B}\\}\\subseteq \\mathcal{P}(y)\\] Si \\(f:X \\to Y\\) y \\(g: Y \\to Z\\) son funciones, donde \\(f(X)\\subseteq Y\\), la función \\(g \\circ f:X \\to Z\\) definida por \\[(g\\circ f)(x)=g(f(x))\\], para todo \\(x \\in X\\), se denomina {}. Si \\(A \\subseteq Z\\), nótese que se tiene la siguiente relación: \\[(g \\circ f)^{-1}=f^{-1}(g^{-1}(A))\\]. La {} \\(\\text{id}_X\\) de un conjunto \\(X\\) se define como \\(\\text{id}_X(x)=x\\), para cada \\(x \\in X\\). Si \\(A \\subseteq X\\), la restricción de \\(\\text{id}_X\\) a \\(A\\) se denomina {inclusión} o {morfina de inclusión} y se denota como \\(\\iota_A : A \\hookuparrow X\\). Si \\(f:X \\to Y\\) es biyectiva, la {} \\(f^{-1}:Y \\to X\\) es la función definida como \\[f^{-1}(y)=x \\Longleftrigtharrow y=f(x).\\] Se tiene que \\[\\begin{array}[ccc] f^{-1}\\circ f=\\text{id}_X &amp;y&amp; f \\circ f^{-1}=\\text{id}_Y. \\end{array}\\] Si \\(X\\) es un conjunto universal y \\(A \\subseteq X\\), la {} es la función definida por \\[\\begin{equation} {\\bf 1}_A(x)= \\left\\{ \\begin{array}{lcc} 1 &amp; \\text{ si } &amp; x \\in A \\\\ 0 &amp; \\text{ si } &amp; x \\in A^c \\end{array} \\right. \\end{equation}\\] Toda función indicatriz satisface las siguientes propiedades: Un caso particular de función indicatriz es la función {}: \\[\\begin{equation} \\delta_{x,y}= \\left\\{ \\begin{array}{lcc} 1 &amp; \\text{ si } &amp; x=y \\\\ 0 &amp; \\text{ si } &amp; x \\neq y \\end{array} \\right. \\end{equation}\\] En efecto, si \\(A=\\{(x,x) \\, : \\, x \\in X\\}\\), entonces \\(\\delta_{x,y}={\\bf 1}_A(x,y)\\). Las funciones \\(x^+\\) y \\(x^-\\) en \\(\\mathbb{R}\\) se definen como \\[\\begin{array}{ccc} x^+:=\\max\\{x,0\\} &amp; \\text{ y } &amp; x^-:=\\max\\{-x,0\\}. \\end{array}\\] De estas definiciones se desprende fácilmente que \\[\\begin{array}{ccc} x=x^+-x^- &amp; \\text{ y } &amp; |x|=x^+ + x^-. \\end{array}\\] La parte real y la parte imaginaria de un número complejo \\(z\\) se denotan, respectívamente, por \\(\\Re(z)\\) y \\(\\Im(z)\\). Asimismo, la conjugada de \\(z\\) y el módulo de \\(z\\) se denotan por \\(\\overline{z}\\) y \\(|z|\\). Se tiene que \\[\\begin{array}{cccccc} z=\\Re(z) + i\\Im(z) &amp;,&amp; \\overline{z}=\\Re(z)-i\\Im(z) &amp; \\text{ y } &amp; |z|=\\sqrt{(\\Re(z))^2+(\\Im(z))^2}. \\end{array}\\] El signo \\(\\sgn(z)\\) de una variable compleja \\(z\\) se define como \\[\\begin{equation} \\sgn(z)= \\left\\{ \\begin{array}{lcc} \\frac{|z|}{z} &amp; \\text{ si } &amp; z \\neq 0 \\\\ 0 &amp; \\text{ si } &amp; z = 0. \\end{array} \\right. \\end{equation}\\] Así, \\(|z|=z\\sgn(z)\\) para todo \\(z \\in \\mathbb{C}\\) y \\(|\\sgn(z)|=1\\) siempre que \\(z\\neq 0\\). Si \\(F \\subseteq Y^X\\), el funcional de evaluación en \\(x\\) es la función \\(\\text{ev}_x:F \\to Y\\) dada por \\[\\begin{equation} \\text{ev}_x(f):=f(x), \\qquad f \\in F. \\end{equation}\\] El adjunto o dual de una función \\(\\phi:Z \\to X\\) (con respecto a \\(F\\)) es la función \\(\\phi^*:F \\to Y^Z\\) definida como \\[\\begin{equation} \\phi^*(f):=f \\circ \\phi, \\qquad f \\in F. \\end{equation}\\] Las siguientes notaciones se utilizarán en los capítulos siguientes, para cada función a valores reales: \\[\\begin{array}{cc} f^+(x):=\\max{f(x),0} &amp; f^-:= (-f)^+\\\\ (f_1 \\vedge \\dots \\vedge f_n)(x):=\\max_{1 \\, \\leq \\, k \\,\\leq \\, n}f_k(x) &amp; (f_1 \\wedge \\dots \\wedge f_n)(x):=\\min_{1 \\, \\leq \\, k \\,\\leq \\, n}f_k(x)\\\\ (\\sup_n f_n)(x):=\\sup_n f_n(x) &amp; (\\inf_n f_n)(x):=\\inf_n f_n(x)\\\\ (\\lim_n f_n)(x):=\\lim_n f_n(x) &amp; (_n f_n)(x):= _n f_n(x). \\end{array}\\] 2.5 Cardinalidad Dos conjuntos \\(A\\) y \\(B\\) se dicen tener la misma cardinalidad si existe una biyección de \\(A\\) a \\(B\\).Un conjunto \\(A\\) se dice finito si \\(A\\) tiene la misma cardinalidad que \\(\\{1,2, \\hdots, n\\}\\), para algún \\(n \\in \\mathbb{N}\\). En este caso, los elementos de \\(A\\) se pueden indexar utilizando el conjunto \\(\\{1,2, \\hdots, n\\}\\), de modo que se puede escribir \\(A=\\{a_1, a_2, \\hdots, a_n\\}\\). Un conjunto \\(A\\) es infinito numerable si tiene la misma cardinalidad que el conjunto \\(\\mathbb{N}\\) de los n, en cuyo caso se escribe \\(A=\\{a_0, a_1, a_2, \\hdots\\}\\). Un conjunto es numerable si es finito o infinito numerable; en caso contrario, se dice que el conjunto es no-numerable. Los conjuntos \\(\\mathbb{Z}\\) y \\(\\mathbb{Q}\\) son numerables. El conjunto \\(\\mathbb{R}\\) es no numerable, asi como cualquier intervalo de números reales. La cardinalidad de \\(\\mathbb{R}\\) se denota por \\(\\mathfrak{c}\\), mientras que la cardinalidad de \\(\\mathbb{N}\\) se denota por \\(\\aleph_0\\). "],
["sistema-numerico-real-y-complejo.html", "Capítulo 3 Sistema numérico real y complejo 3.1 Introducción 3.2 Propiedades algebraicas de \\(\\mathbb{R}\\) 3.3 Estructura de orden de \\(\\mathbb{R}\\) 3.4 Propiedades de completitud de \\(\\mathbb{R}\\) 3.5 Inducción matemática 3.6 Espacios euclídeos", " Capítulo 3 Sistema numérico real y complejo 3.1 Introducción En este capítulo se introducen las nociones esenciales para el estudio del sistema numérico real, el cual proveerá el basamento teórico para introducir la noción de límite. Por sistema numérico real entendemos a un conjunto no vacío \\(\\mathbb{R}\\), junto con dos operaciones binarias, llamadas suma y producto, y una relación de orden total, satisfaciendo tres tipos de axiomas: los axiomas de cuerpo, los axiomas de orden y el axioma de completitud. Existen varias maneras de construir reigurosamente al conjunto de los números reales. Una de ellas consiste primero en definir al conjunto de los números naturales \\(\\mathbb{N}\\), luego a los enteros \\(\\mathbb{Z}\\), siguiendo con los racionales \\(\\mathbb{Q}\\) y finalmente llegando a los números reales \\(\\mathbb{R}\\). En esta construcción, se pide que el conjunto de los números naturales satisfagan una serie de axiomas, llamados axiomas de Peano, los cuales permiten definir las operaciones de suma y producto entre naturales. La resta se introduce luego, agrandando el sistema numérico de los naturales a \\(\\mathbb{Z}\\), y permitiendo resolver ecuaciones de la forma \\(x+m=n\\) para una indeterminada \\(x\\), y \\(m,n \\in \\mathbb{N}\\). Para obtener la división, se identifica el cociente \\(m/n\\), para \\(m,n \\in \\mathbb{Z}\\) y \\(n \\neq 0\\), con la clase \\([(m,n)]\\) definida a partir de la relación \\(\\sim\\) en \\(\\mathbb{Z} \\times (\\mathbb{Z}\\setminus\\{0\\})\\) dada por \\[\\begin{equation} (m,n) \\sim (m&#39;,n&#39;) \\Longleftrightarrow mn&#39;=nm&#39;. \\end{equation}\\] En este sistema numérico, las ecuaciones del tipo \\(ax+b=c\\), con \\(a,b,c \\in \\mathbb{Z}\\) y \\(a\\neq 0\\), poseen solución. El último paso, la construcción de \\(\\mathbb{R}\\) a partir de \\(\\mathbb{Q}\\), se realiza (informalmente) llenando los vacíos entre números racionales, mediante la incorporación de los números irracionales. Este es el principio del axioma de completitud. 3.2 Propiedades algebraicas de \\(\\mathbb{R}\\) 3.3 Estructura de orden de \\(\\mathbb{R}\\) 3.4 Propiedades de completitud de \\(\\mathbb{R}\\) 3.5 Inducción matemática 3.6 Espacios euclídeos "],
["estructuras-algebraicas.html", "Capítulo 4 Estructuras algebraicas 4.1 Semigrupos y grupos 4.2 Espacios vectoriales 4.3 Transformaciones lineales 4.4 Espacios vectoriales cocientes 4.5 Álgebras", " Capítulo 4 Estructuras algebraicas 4.1 Semigrupos y grupos 4.2 Espacios vectoriales 4.3 Transformaciones lineales 4.4 Espacios vectoriales cocientes 4.5 Álgebras "],
["sucesiones-numericas.html", "Capítulo 5 Sucesiones numéricas 5.1 Límite de una sucesión 5.2 Sucesiones monótonas 5.3 Subsucesiones y sucesiones de Cauchy 5.4 Límites inferior y superior", " Capítulo 5 Sucesiones numéricas 5.1 Límite de una sucesión 5.2 Sucesiones monótonas 5.3 Subsucesiones y sucesiones de Cauchy 5.4 Límites inferior y superior "],
["sucesiones-y-series.html", "Capítulo 6 Sucesiones y series 6.1 Límite de una función 6.2 Límites inferior y superior 6.3 Funciones contínuas 6.4 Propiedades de las funciones contínuas 6.5 Continuidad uniforme", " Capítulo 6 Sucesiones y series 6.1 Límite de una función 6.2 Límites inferior y superior 6.3 Funciones contínuas 6.4 Propiedades de las funciones contínuas 6.5 Continuidad uniforme "],
["diferenciacion.html", "Capítulo 7 Diferenciación 7.1 Definición y ejemplos 7.2 El teorema del valor medio 7.3 Funciones convexas 7.4 Funciones inversas 7.5 Regla de L’Hospital 7.6 Teorema de Taylor en \\(\\mathbb{R}\\) 7.7 Método de Newton", " Capítulo 7 Diferenciación A lo largo de esta secci'on vamos a trabajar en \\(\\real^n={(x_1,\\dots c, x_n) \\ x_j \\in \\real, j=1,...,N}\\) Durante todo el año denotaremos al vector \\((x_1,x_2,\\dots,x_n)\\) como \\(\\gx\\) por comodidad. En general, un producto escalar es una matriz definida positiva y se opera de la siguiente manera: \\[\\pesc{\\gx,\\gy} = (x_1,x_2,...,x_N) \\cdot \\begin{pmatrix} a_11 &amp;\\cdots&amp; a_1N\\\\ \\vdots&amp; \\ddots &amp; \\vdots\\\\ a_N1 &amp; \\cdots&amp; a_NN \\end{pmatrix} \\cdot \\begin{pmatrix} y_1\\\\ \\vdots\\\\ y_N \\end{pmatrix}\\] Hay tres casos particulares, la norma uno \\[ \\md{\\gx}_1 = |x_1| + |x_2| + ... + |x_n| \\] La norma 2, que es la norma euclídea y la norma infinito \\[\\md{\\gx}_{\\infty} = \\max\\left\\{|x_1|,|x_2|,\\dots,|x_n|\\right\\} \\] Vamos a demostrar que la norma \\(p\\) cumple las 3 propiedades de una norma. Para ello, nos apoyaremos en dos teoremas previos: Una vez probadas las dos desigualdades anteriores, pasamos a probar la desigualdad triangular: EJERCICIO PROPUESTO: Tomamos en el plano el conjunto de los puntos cuya norma es 1. Tomando en la norma p=2 sale la circunferencia. ¿Y en p=3? Propiedades: Con un producto escalar puedo definir una norma y con esa norma puedo definir una distancia. Pero… ¿Podemos definir una norma que no venga de un producto escalar y/o alguna distancia que no provenga de una norma? Sí, por ejemplo \\[ \\tilde{d} (\\gx,\\gy) = \\abs{\\arctg(y) - \\arctg(x)} \\] No cuesta mucho comprobar que cumple las 3 propiedades de una distancia. Además, esta distancia es cuanto menos curiosa porque nunca será mayor de \\(\\pi\\). Podemos comprobar que si existiera una norma que midiese esta distancia tendríamos \\[\\tilde{\\md{\\gx}} = \\tilde{d} (\\gx,\\gor{0}) = \\abs{\\arctg (x)} \\] pero esto no cumple la propiedad: \\(\\tilde{\\md{\\lambda x}} = \\abs{\\arctg \\lambda x} \\neq \\abs{\\lambda}\\abs{\\arctg x} = \\abs{\\lambda x}\\tilde{||x||}\\) ya que ninguna distancia puede ser mayor que \\(\\pi\\) y tomando un \\(\\lambda &gt; \\pi\\) se produciría la contradicción. Esa norma asociada al producto escalar tiene dos propiedades importantes: Sea \\(\\md{\\cdot}\\) una norma en \\(\\real^N\\). Si intento calcular la norma de un vector \\(\\gx\\) \\[ \\md{\\gx} = \\md{\\sum x_i e_i} \\leq \\sum_{i=1}^N \\md{x_1 e_1} = \\sum_{i=1}^N|x_i|\\cdot\\md{e_i} \\] Tenemos: \\(\\md{\\gx} \\leq \\sum_{i=1}^N c_i |x_i|\\) siendo \\(c_i = \\md{e_i}\\). Aplicando Cauchy-Schwarz nos queda \\[ \\sum_{i=1}^N \\left(c_i^2\\right)^\\frac{1}{2} \\cdot \\sum \\left(|x_i|^2\\right)^\\frac{1}{2} \\] Es decir, puedo controlar cualquier norma con una constante y la norma euclídea: \\[|||\\gx||| \\leq C \\md{x}_{2}\\] En particular, \\(0 \\leq |||\\gor{x_n} - \\gx|||\\leq c ||\\gor{x_n}-\\gx||\\). Sea \\(F(\\gx) = |||\\gx|||\\) y \\(F:\\real^N \\rightarrow \\real^N\\) \\[|F(\\gx)-F(\\gy)| = \\left| |||\\gx - \\gy||| \\right| = |||\\gx - \\gy||| \\leq C ||\\gx - \\gy||\\] Utilizando: \\(|||\\ga-\\gb||| \\ge |||\\ga||| - |||\\gb|||\\) Es decir, cualquier norma en \\(\\real^n\\) es respecto de la norma euclídea. Para evitar jaleos, al tratar la distancia vamos a tomar la norma euclídea. Como todas las normas son equivalentes, nos da igual tomar una que otra. \\(F(\\gor{x}) = |||\\gor{x}|||\\) una norma (que ya sabemos que es continua): \\(m \\md{x} \\leq |||\\gor{x}||| \\leq C||\\gor{x}||\\) Al tener definida una norma de vectores podemos definir convergencia y continuidad: A partir de esta definición podemos estudiar qué hacen las funciones continuas con conjuntos abiertos y cerrados. Sea \\(F\\) continua. Contrario a lo que podríamos intuir, Empezamos definiendo en qué consiste una función inversa: A partir de aquí podemos extraer dos conclusiones que sí nos ayudarán a discernir si un conjunto imagen es abierto y cerrado. Este teorema nos sirve para decir fácilmente si un conjunto es abierto o cerrado. Por ejemplo, consideremos \\[ M=\\{(x,y,z) \\in \\real^3 \\tq x^2 + \\cos\\left(x\\abs{y}\\right) - e^z &lt; 1 \\} \\] Podemos definir ahí la función \\(F\\) como \\[ F(x,y,z) = x^2 + \\cos\\left(x\\abs{y}\\right) - e^z\\] que va de \\(\\real^3\\) a cierto conjunto \\(A = \\{ a \\in \\real \\tq a &lt; 1 \\}\\). Podemos reescribir \\(M\\) como \\[M=\\{(x,y,z) \\in \\real^3 \\tq F(x,y,z) \\in A\\}\\] o, de otra forma, \\(M = \\inv{F}(A)\\). Según el teorema (), como \\(A\\) es abierto entonces \\(M\\) también es abierto. Además, toda aplicación lineal se puede escribir en forma de matriz. \\[L(\\gor{x})=A\\gor{x} = \\begin{pmatrix} a_{11} &amp; \\cdots &amp; a_{1n} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; \\cdots &amp; a_{nn} \\end{pmatrix}\\] Consideramos una aplicación continua \\[\\begin{align*} \\appl{F}{\\real^N&amp;}{\\real} \\\\ \\gor{x} &amp;\\longrightarrow F(\\gx) = \\underbrace{||A\\gx||}_{L(\\gx)} \\end{align*}\\] Sabemos que existe \\(C&gt;0\\) tal que \\(||A\\gx|| \\leq C||\\gx||\\), es decir, \\(||A\\gx||\\leq C\\) si \\(||\\gx||=1\\). Queremos saber cuál es la mejor constante que podemos encontrar, la que más se ajuste. Consideramos el conjunto \\(M\\) de todos los vectores de la esfera unidad, es decir \\[ M = \\{||A\\gx|| \\tq ||\\gx||=1\\}\\subset \\real \\] Entonces mejor constante \\(C\\) es la cota superior mínima (supremo) que vamos a llamar \\(\\alpha\\). Al ser \\(F\\) continua y \\(M\\) compacto, sabemos que el supremo \\(\\alpha \\in M\\). Basándonos en lo que hemos obtenido, calculamos la norma uno de \\[A = \\begin{pmatrix} 1&amp;2\\\\-3&amp;1\\\\2&amp;0 \\end{pmatrix}\\] Tenemos que maximizar, sabiendo que \\(|x|+|y| = 1\\): \\[ |x+2y| + |-3x+y| + |2x| \\leq |x|+|2y| + |3x| + |y| + 2|x| = 6|x|+3|y| \\leq 6 (|x|+|y|) =6 \\] ¿Podemos encontrar un vector \\(\\gx = (x_0,y_0)\\) tal que \\(||A(x_0,y_0)^T||_1 = 6\\)?\\ Tomando \\(x_0 = 1\\) y \\(y_0 = 0\\) lo encontramos. Como \\(\\gx\\) está en la esfera unidad y es una cota, es el máximo y por lo tanto la norma que buscamos. Curiosamente, y escoger el más grande. Si tomamos la norma infinito de \\[A = \\begin{pmatrix} 1&amp;2\\\\-3&amp;1\\\\2&amp;0 \\end{pmatrix}\\] Tenemos que . Queremos buscar ahora cuánto vale la norma de una matriz cuando usamos la norma euclídea. Usaremos los dos lemas siguientes para apoyarnos. Tenemos \\(A^TA\\) diagonalizable, de dimensión \\(N \\times N\\). Buscamos cuánto vale \\(\\norm{A\\gx}\\) con \\(\\gx \\in \\real^N\\). Empezamos desarrollando el vector en una base ortonormal de \\(A^T A\\): \\[ \\gx = \\sum \\alpha_i \\gv_i \\] y por lo tanto \\[ ||\\gx|| = \\sum \\alpha_i^2 \\pesc{\\gv_i,\\gv_i}\\]. Desarrollamos el producto \\[\\trans{A}A\\gx = \\trans{A}A\\left(\\sum \\alpha_i \\gv_i\\right) = \\sum \\left(\\alpha_i\\lambda_i\\gor{v}_i\\right) \\] donde \\(\\lambda_i\\) son los autovalores de \\(\\trans{A}A\\). Ahora queremos hallar el máximo de \\(||A\\gx||\\) cuando \\(||\\gx|| = 1\\): \\[ ||A\\gx||^2 = \\pesc{A\\gx,A\\gx} = \\pesc{\\trans{A}A\\gx,\\gx} = \\pesc{\\sum \\lambda_i \\alpha_i \\gv_i,\\sum\\alpha_i \\gv_i} \\] Como la base de \\(\\{\\gv_n\\}\\) es ortonormal, \\(\\pesc{\\gv_i, \\gv_j} = 0\\) si \\(i≠j\\), por lo tanto sólo nos queda \\[ \\pesc{\\sum \\lambda_i \\alpha_i \\gv_i,\\sum\\alpha_i \\gv_i} = \\sum \\lambda_i \\alpha_i^2 \\leq \\lambda_{max} \\left(\\sum \\alpha_i^2\\right) = \\lambda_{max} \\] teniendo en cuenta que \\(\\norm{\\gx} = 1\\) y donde \\(\\lambda_max\\) es el autovalor máximo. Es decir, hemos llegado a que \\[ \\max\\norm{A\\gx} ≤ \\sqrt{\\lambda_{max}} \\] Hemos definido una cota para \\(\\norm{A\\gx}\\). Ahora bien, esa cota se alcanza cuando \\(\\gx\\) es el autovector asociado a \\(\\lambda_{max}\\), por lo tanto la cota es un máximo y la norma de la matriz. Importante el detalle de \\(0 &lt; ||\\gx-\\ga||\\), no es un \\(\\leq\\), porque no se necesita que la función esté siquiera definida en el punto \\(\\ga\\). Idea para el cálculo de límites: Si \\(\\displaystyle\\lim F({t\\gor{v}})\\) toma valores distintos dependiendo de \\(\\gor{v}\\) entonces \\(\\nexists \\displaystyle\\mylim{x}{0}{\\gx}\\) Por otra parte, si \\(\\forall t \\in \\real, \\mylim{x}{a}{t\\gor{v}} = L\\), entonces \\(\\gor{L}\\) es el candidato a ser el límite (no tiene por qué serlo). El siguiente paso sería demostrar con argumentos de comparación (Sandwich) u otros que \\(\\mylim{x}{a}{\\gx} = L\\). El contraejemplo para ver que la existencia del límite por rectas no implica la existencia del límite es estudiar la función \\[ f(x,y) = \\frac{x y^2}{x^2 + y^4}\\] . Veamos por qué. Si os acercamos al límite por medio de rectas: \\[ f(x,y) = f(x,mx) = \\frac{x\\cdot(mx)^2}{x^2 + (mx)^4} = \\frac{m^2x^3}{(1 + x^2m^4)x^2} \\] \\[ \\lim{x\\to 0} (f(x,mx)) \\rightarrow 0, \\forall m \\in \\real \\] Pero si nos acercamos al límite por medio de \\(x = y^2\\) tenemos: \\[ f(x,y) = f(y^2,y) = \\frac{y^2y^2}{y^4+y^4} = \\frac{y^4}{2y^4} = \\frac{1}{2} ≠ 0\\] Por lo tanto, el límite no existe a pesar de que existe cuando nos acercamos por rectas. Aproximación lineal \\(\\sim\\) Diferencial. Matriz jacobiana \\(\\sim\\) Jacobiana. El contraejemplo para demostrar la implicación a la izquierda es el mismo que en los límites a lo largo de rectas. $f(x,y) = { \\[\\begin{matrix} \\displaystyle\\frac{xy^2}{x+y^2} &amp; (x,y) \\neq (0,0) \\\\ 0 &amp; (x,y)=(0,0) \\end{matrix}\\] . $ Según las aplicaciones que tengamos, la diferencial se suele llamar de una u otra forma. Con \\(\\appl{\\delta}{\\real}{\\real^M}\\) utilizamos notación vectorial en vez de matricial porque tendríamos una matriz columna. Por ejemplo: la velocidad (en un instante de tiempo, un punto en el espacio). Consideremos la composición de dos funciones de la siguiente forma: \\(\\appl{F}{\\real^N}{\\real^M}\\). \\(\\appl{G}{\\real^M}{\\real^K}\\). \\(\\appl{H=G\\circ F}{\\real^N}{\\real^K}\\). $ ^N, ^M$ Con \\(F\\) diferenciable en \\(\\ga\\) y \\(G\\) diferenciable en \\(F(\\gor{a})\\). Entonces \\(H=G\\circ F\\) es diferenciable en $$. Además la expresión matricial es: \\[ \\underbrace{DH(\\ga)}_{K\\times N} = \\underbrace{DG(F(\\ga))}_{K\\times M}\\cdot \\underbrace{DF(\\ga)}_{M\\times N} \\] No hace falta calcular toda la matriz si sólo queremos un elemento. Para calcular 1 único elemento de la matriz diferencial (el de la fila \\(i\\), columna \\(j\\)), usamos la siguiente fórmula \\[ \\dpa{H_i}{x_j}(\\ga) = \\sum_{k=1}^M \\dpa{G_i}{y_k}\\cdot\\dpa{F_k}{x_j} \\] Siempre teniendo en cuenta que \\(\\displaystyle\\dpa{G_i}{y_k}\\) está evaluado en \\(F(\\ga)\\) y \\(\\displaystyle\\dpa{F_k}{x_j}\\) está evaluado en \\(\\ga\\). El teorema anterior que vimos del valor medio era para funciones de una variable, y proponía lo siguiente: Después el teorema lo extendimos a funciones de \\(\\real^N\\) en \\(\\real\\): dada \\(\\appl{F}{\\real^N}{\\real}\\) entonces definíamos \\[ \\sigma(t) = t\\gor{b}+(1-t)\\gor{a} \\] y \\(g = F\\circ \\sigma\\) era una aplicación de los reales a los reales, y entonces aplicando el teorema anterior teníamos que \\[ F(\\gor{b}-\\gor{a}) = g(1)-g(0) = g&#39;(s) \\] para algún \\(s\\in[0,1]\\). Sin embargo, al tratar de extrapolar este resultado a una función \\(\\appl{F}{\\real^N}{\\real^2}\\) nos queda que \\[ F(\\gor{b})-F(\\ga) = \\begin{pmatrix} \\pesc{\\nabla F_1(\\gor{c_1}),{\\gor{b}-\\ga}}\\\\ \\pesc{\\nabla F_2(\\gor{c_2}),\\gor{b}-\\ga} \\end{pmatrix} \\] Tenemos 2 \\(c\\) distintos, uno para cada \\(f\\), por lo que este teorema pierde sentido. Hay que buscar una extensión, otra formulación del teorema que nos permita aplicarlo a las funciones que estamos estudiando. Este teorema nos sirve, por ejemplo, para ver que si tenemos \\(\\appl{F}{\\real^N}{\\real^M}, F \\in C^1\\), definida en un conjunto abierto y conexo y \\(DF(\\gx) \\equiv 0\\; \\forall\\gx\\), entonces \\(F\\) es constante. \\(\\appl{F}{\\real^N}{\\real^M}\\) (escalar) \\(\\ga \\sim\\) Recta que pasa por \\(\\ga\\) con dirección \\(\\gor{v}\\). \\(r(t) = \\ga + t\\gor{v}\\). Como una recta tiene infinitos vectores directores (dependiendo de la longitud), siempre tomaremos vectores directores unitarios, con \\(\\norm{\\gor{v}} = 1\\). Vamos a estudiar: \\(g(t) = F(\\ga + t\\gor{v}) = F \\circ r (t)\\). \\(t \\sim 0 \\dimplies \\ga + t\\gor{v} \\sim \\ga\\) La existencia de \\(D_{\\gor{v}}F(\\ga), \\forall \\gor{v}\\in \\real^N\\) NO garantiza que \\(F\\) sea derivable. Si sabemos que \\(F\\) SÍ es diferenciable podemos usar la regla de la cadena obteniendo: \\(D_{\\gor{v}}F(\\ga) = g&#39;(0) = D(F\\circ r)(0) = ... = \\pesc{\\nabla F(\\ga),\\gor{v}}\\) Contraejemplo de la no reciprocidad: \\(f(x) = x^2 sin\\left(\\frac{1}{x}\\right)\\) \\(\\deriv{}{y}\\left(\\deriv{f}{x}\\right) \\equiv \\deriv{^2f}{x \\partial y}\\) Tenemos una función \\(\\appl{F}{\\real^N}{\\real}\\), con \\(F\\in C^k\\) (\\(k\\) veces derivable), y queremos el desarrollo de Taylor de \\(F\\) alrededor de \\(\\ga \\in \\real^N\\). En dimensión 1, el desarrollo era el que sigue \\[ g(x) = g(0) + g&#39;(0)x + \\frac{g&#39;&#39;(0)}{2!}x^2 + ... + \\frac{g^{k)}(0)}{k!}x^k + \\underbrace{\\frac{g^{k+1)}(s)}{(k+1)!}x^{(k+1)}}_{\\text{error}} \\] Tenemos que expandir este desarrollo a más dimensiones. Tomamos \\[ g(t) \\equiv F(t(\\ga + \\gor{h}) + (1-t)\\ga) \\] reduciendo así el cálculo a dimensión 1. Operamos ahora para calcular las derivadas: \\[\\begin{gather*} g&#39;(t) = \\pesc{\\nabla F(a+th),h} = \\sum_{i=1}^N \\deriv{F}{x_i}(a+th)\\cdot h_i \\\\ g&#39;&#39;(t) = \\sum_{i=1}^N\\left(\\sum_{j=1}^N \\deriv{}{x_j}\\deriv{F}{x_i}(\\ga+\\gor{h})\\cdot{h_j}\\right)h_i = \\sum_{i,j = 1}^N \\deriv{^2F}{x_i \\partial x_j}(\\ga+t\\gor{h})h_ih_j \\\\ \\dotsb \\\\ \\frac{g^{s)} (0)}{s!} = \\frac{1}{s!}\\sum_{i_1,i_2,...,i_s=1}^N \\frac{\\partial^s F}{\\partial x_{i_1},x_{i_2},...,x_{i_s}} \\end{gather*}\\] De esta forma, el desarrollo de Taylor de orden \\(k\\) de \\(F\\) en \\(\\ga\\) en general es \\[\\begin{equation} T^k_F(x) = \\sum_{\\alpha = 0}^k \\left( \\frac{1}{\\alpha !} \\sum_{i_1,\\dotsc,i_\\alpha = 0}^N (x_{i_1} - a_{i_1}) \\dotsb (x_{i_\\alpha} - a_{i_\\alpha}) \\frac{\\partial^\\alpha F}{\\partial x_{i_1} \\dotsb \\partial x_{i_\\alpha}} (\\ga) \\right) \\end{equation}\\] Por ejemplo, el desarrollo de Taylor de una función \\(F(x,y)\\) con \\(\\ga = (a,b)\\) queda lo siguiente (con \\(F_{xyz\\dotsc} = \\dfrac{\\partial^nF}{\\partial x \\partial y \\partial z \\dotsb}\\)) \\[\\begin{align*} T^k_{F}(\\gx) &amp;= F(\\ga) \\\\ &amp; + (x - a) F_x(\\ga) + (y - b) F_y(\\ga) \\\\ &amp; +\\frac{1}{2!}\\left[(x-a)^2F_{xx}(\\ga) + 2(x-a)(y-b)F_{xy}(\\ga) + (y-b)^2F_{yy}(\\ga)\\right] \\\\ &amp; +\\frac{1}{3!}\\left[(x-a)^3F_{xxx}(\\ga) + 3(x-a)^2(y-b)F_{xxy}(\\ga) + 3(x-a)(y-b)^2 F_{xyy} (\\ga) + (y-b)^3F_{yyy} (\\ga)\\right] \\\\ &amp; +\\dotsb \\end{align*}\\] Existe una forma más compacta para el desarrollo de Taylor de orden dos usando el producto escalar, el vector gradiente y la matriz hessiana (\\(D^2F\\)) de derivadas segundas: \\[ F(\\gor{a}+\\gor{h}) = F(\\ga) + \\pesc{\\grad F(\\ga),\\gor{h}} + \\frac{1}{2} \\gor{h}^T D^2F(\\ga)\\gor{h} \\] En el mundo lineal tenemos podemos resolver sistemas de dos formas. Con \\(\\appl{F}{\\real^N}{\\real^N}\\) y \\(L(\\gx) = A\\gx\\), siendo \\(A\\) una matriz \\(N\\x N\\); queremos resolver el sistema \\(A\\gx = \\gy\\) sabiendo que \\(A\\gor{0} = \\gor{0}\\). Este sistema tiene solución si y sólo si \\(\\det(A) \\neq 0\\): es la condición para que exista \\(A^{-1}\\). Existe también la posibilidad de tener una función \\(\\appl{F}{\\real^{N+M}}{\\real^N}\\) con \\(L(\\gx) = A\\gx\\), \\(A\\) matriz \\(N\\x (N+M)\\). Para resolver el sistema \\(A\\gx = \\gy\\) parametrizamos \\(M\\) variables. En el mundo , consideramos una función \\(\\appl{F}{\\real^N}{\\real^N}\\). Entonces tenemos un sistema de ecuaciones \\[ \\left.\\begin{matrix} F(x_1,\\dotsc,x_N) = y_1\\\\ F(x_1,\\dotsc,x_N) = y_2\\\\ \\vdots\\\\ F(x_1,\\cdots,x_N) = y_N \\end{matrix} \\right\\} F(\\gx)=\\gy \\] Vamos a intentar resolver este problema utilizando Taylor para aproximar al orden lineal, pero tenemos que pagar un precio: para que taylor funcione tenemos que trabajar cerca del punto. Esto significa que . El teorema de la aplicación contractiva nos sirve, por ejemplo, para comprobar si hay una solución de una ecuación diferencial ordinaria (EDO). \\[\\left.\\begin{matrix}y&#39;(x) = f(x,y(x))\\\\ y(x_0) = y_0 \\end{matrix}\\right\\} \\leftrightarrow y(x) = y_0 + \\int_{x_0}^x f(s,y(s)) ds\\] Podemos definir: \\[\\begin{align*} y_1(x) &amp;= y_0 + \\int_{x_0}^{x} f(s,y_0)ds\\\\ y_2(x) &amp;= y_0 + \\int_{x_0}^x f(s,y_1(s))ds \\equiv T(y_1)\\\\ &amp;\\dots\\\\ y_n &amp;= T(y_{n-1}) = y_0 + \\int_{x_0}^x f(s,y_{n-1}(s))ds\\\\ ¿T(y) &amp;= y? \\end{align*}\\] Aquí es donde entraría la diferencia entre trabajar en \\(\\real^N\\) y un espacio de funciones. Ejercicio propuesto: Aplicar este argumento a iterativo \\(\\left. \\begin{matrix} y&#39; = y\\\\ y(0) = 1 \\equiv y_0 \\end{matrix}\\right\\}\\) En Cálculo I teníamos el siguiente teorema: El teorema nos daba un resultado que asegura que existe la inversa, que es diferenciable y además nos daba su fórmula. Buscamos ahora lo que ocurre en dimensión \\(N\\). Tomamos \\(\\appl{f}{\\real^N}{\\real^N}\\) y supongamos que en algún abierto \\(\\exists F^{-1}\\) y \\(F,F^{-1} \\in C^1\\). Entonces está claro que \\[ (F\\circ \\F)(y) = y \\implies DF(\\F(y))D\\F(y) = Id \\] \\[ (\\F\\circ F)(y) = y \\implies D\\F(F(y))DF(y) = Id \\] Con \\(\\appl{F}{\\Omega\\subset \\real^N}{\\real^N}\\) queremos probar que existe una aplicación \\(\\appl{G}{V}{U}\\) que verifique las siguientes condiciones \\[\\begin{gather*} G\\circ F(\\gx) = \\gx, \\forall\\gx\\in U \\subset \\Omega \\\\ F\\circ G(\\gy) = \\gy, \\forall \\gy \\in V \\\\ G \\text{ diferenciable} \\end{gather*}\\] Sea \\(F(x,y) = (x^2-5y^2,4xy)\\). Tomamos \\((x_0,y_0)\\). ¿\\(F\\) invertible en un entorno de \\(F(x_0,y_0)\\)? Calculamos Df: \\[Df = \\begin{pmatrix} 2x&amp;-10y\\\\ 4y &amp; 4x \\end{pmatrix}\\] \\[\\det(DF) = 8x^2 + 40y^2 \\neq 0 \\text{ si } (x,y) \\neq (0,0)\\] Cuando el determinante sea 0, significa que no puedo aplicar el teorema, por tanto, no sé si la función es invertible o no. Aplicamos los fundamentos. ¿Es inyectiva la función? No es inyectiva en ningún entorno del (0,0). El hecho de que el teorema sea local nos puede llevar a confusiones. Por ejemplo, sea \\[ F (r,\\sigma) = (rcos(\\sigma),r\\sen(\\sigma)), r \\in [0,1], \\sigma \\in [0,4\\pi]\\] Entonces hay dos soluciones para la inversa: \\[ F^{-1} (0,\\frac{1}{2}) = \\left\\{\\begin{matrix}2\\pi+\\frac{\\pi}{2}\\\\\\frac{pi}{2}\\end{matrix}\\right. \\] Lo que hay que hacer es partir de un punto del conjunto de salida. El teorema dice que escogido un punto del conjunto de salida, existe un entorno en el conjunto de llegada en el que se puede definir la función inversa. Otro ejemplo es considerar \\(g(x) = f(x) + \\varepsilon x\\) siendo \\(f(s) =\\left\\{\\begin{matrix}x^2sin\\left(\\frac{1}{x}\\right)&amp; \\text{ si } x\\neq0\\\\0 &amp;\\text{si } x=0\\end{matrix}\\right.\\) Esta función \\(g \\notin C^1\\). \\(g\\) es derivable (\\(g&#39;(0) = \\varepsilon&gt;0\\)) pero la derivada no es continua. Se deja como ejercicio para el lector la comprobación. Tomemos el caso particular de una superficie en \\(\\real^3\\). Puede venir dada de dos formas: ¿Existe alguna forma de expresar \\(F(x,y,z)\\) de la forma \\(z=f(x,y)\\)? Pensando en el ejemplo de la esfera: \\(F(x,y,z) = x^2+y^2+z^2+1\\), se puede expresar de dos formas: \\[ z = \\pm \\sqrt{1 - x^2 - y^2} \\] ¿Cuál es la condición que necesitamos para poder despejar \\(z\\)? Supongamos que sabemos despejar \\(z=f(x,y)\\), entonces tenemos: \\(F(x,y,f(x,y)) = 0\\). Derivando implíctamente \\[ \\underbrace{\\frac{\\partial}{\\partial x} [F(x,y,f(x,y)]}_{\\dpa{F}{x} + \\dpa{F}{z}\\cdot\\dpa{f}{x}}= \\underbrace{\\frac{\\partial}{\\partial y} [F(x,y,f(x,y)]}_{\\dpa{F}{y} + \\dpa{F}{z}\\cdot\\dpa{f}{y}} = 0 \\] Si \\(f\\) es diferenciable tenemos: \\[\\dpa{f}{x}(x,y) = - \\frac{\\dpa{F}{x}(x,y,f(x,y))}{\\dpa{F}{z}(x,y,f(x,y))}\\] \\[\\dpa{f}{y}(x,y) = - \\frac{\\dpa{F}{y}(x,y,f(x,y))}{\\dpa{F}{z}(x,y,f(x,y))}\\] Necesitamos entonces que \\(\\displaystyle\\dpa{F}{z}\\left(x,y,f(x,y)\\right) \\neq 0\\). Veamos cómo extrapolar esto de forma general con tres variables. Estudiar si es posible despejar \\(u(x,y,z), v(x,y,z)\\) en las ecuaciones: \\[\\left\\{\\begin{matrix} xy^2+xzu+yv^2 &amp;= 3\\\\ xyu^3+2xv-u^2v^2 &amp;= 2\\end{matrix}\\right.\\] En un entorno de \\((x,y,z) = (1,1,1)\\) Vamos a tener que definir una \\[\\appl{F}{\\real^5}{\\real^2}\\] \\[(x,y,z,u,v) \\rightarrow F(x,y,z,u,v)= (xy^2+xzu+yv^2-3,xyu^3+2xv-u^2v^2-2)\\] Podemos comprobar fácilmente que \\(F\\in C^{\\infty}\\) y \\(F(1,1,1,1,1) = ... = (0,0)\\). Para poder despejar u,v tenemos que evaluar \\(D_{(u,v)}F\\) en \\((1,1,1,1,1)\\). \\[D_{(u,v)} = \\begin{pmatrix} \\dpa{F_1}{u}&amp;\\dpa{F_1}{v}\\\\ \\dpa{F_2}{u} &amp;\\dpa{F_2}{v}\\end{pmatrix} = \\begin{pmatrix} xz &amp; 2yv\\\\3xyu^2-2uv^2 &amp; 2x-2u^2v \\end{pmatrix} = ... = \\begin{pmatrix} 1&amp;2\\\\ 3&amp;0 \\end{pmatrix}\\] Tenemos \\(\\det D_{(u,v)} = -6 \\neq 0\\). Entonces estamos en las hipótesis para utilizar el teorema y garantizar que en un entorno del punto \\((1,1,1)\\) blablabla. COMPLETAR Vamos a calcular (porque lo pide el enunciado) \\[\\dpa{u}{x},\\dpa{v}{x},\\dpa{v}{z}\\]. Como el teorema garantiza que existe, derivamos implícitamente: Vamos a derivar implícitamente respecto a \\(x\\) el sistema: \\[\\left\\{\\begin{matrix} y^2+zu+xzu_x + y 2v v_x = 0 \\\\ yu^3+xy3u^3u_x + 2v + 2xv_x - 2uu_xv^2-2u^2vv_x = 0 \\end{matrix}\\right.\\] Donde \\(u_x = \\dpa{u}{x}\\). si \\((x,y,z) = (1,1,1) \\implies (u,v) = (1,1)\\) Sustiuyendo: \\[\\left\\{\\begin{matrix}1+1+u_x+2v_x &amp;= 0 \\\\ 1+3u_x+2+2v_x-2u_x-2v_x &amp;= 0\\end{matrix}\\right.\\] \\[\\left\\{\\begin{matrix}u_x(1,1,1) + 2v_x(1,1,1) &amp;= -2\\\\ u_x(1,1,1) &amp;= -3 \\end{matrix}\\right.\\] Faltaría calcular \\(\\dpa{v}{z}\\) Demostrar T.F.Inversa a partir del T.F Implícita Tenemos: \\[\\begin{gather} \\appl{F}{\\real^N}{\\real^N}\\\\ F\\in C^1\\\\ F(\\ga) = \\gb\\\\ \\det DF(\\ga) \\neq 0 \\end{gather}\\] ¿Podemos despejar \\(F(\\gx) = \\gy\\) para \\(\\gx\\) en un entorno de \\(\\ga\\) \\(\\gy\\) en un entorno de \\(\\gb\\)? \\(F(\\gx) = \\gy\\) es lo mismo que \\(H(\\gx,\\gy) = 0\\) con \\(H(\\gx,\\gy) = F(\\gx)-\\gy\\). \\(\\appl{H}{\\real^N\\times \\real^N}{\\real^N}. H\\in C^1\\) y \\(H(\\ga,\\gb) = 0\\). Tenemos el punto de partida en el que anclar el teorema. Queremos hallar \\(\\gx\\) como función de \\(\\gy\\) en la ecuación \\(H(\\gx,\\gy) = 0\\). Necesitamos para aplicar el teorema: \\(\\det D_x H(\\gx,\\gb) \\neq 0\\). \\(D_x H = D_x F \\neq 0\\) (por (4)) \\(\\exists f(\\gy) \\tlq H(f(\\gy),\\gy) = \\gor{0} \\equiv F(f(\\gy)) - \\gy = \\gor{0} \\equiv F(f(\\gy)) = \\gy\\). Ahora tenemos que ver que la composición en el otro sentido también nos da la identidad. \\[f(F(\\underbrace{f(\\gy)}_{v}) = f(\\gy) \\implies f(F(v)) = v\\] ¿Dónde está escondida la indentidad en un sistema de ecuaciones (lineales o no)? \\[\\left\\{\\begin{array}{cc} F_1\\equiv x^2+z^2+2xz-2x-2z+1&amp;=0\\\\ F_2\\equiv x^2+4y^2+4z^2+4xy+4xz+8xy&amp;=0 \\end{array}\\right.\\] ¿Es depejable en función de z en un entorno de \\((x,y) = (0,-1), z=1\\)? Solución: Ver que el punto safisface las ecuaciones. Ese sistema es como definir: \\[\\appl{F}{\\real^3}{\\real^2}, F\\in C^1, F(0,-1,1) = (0,0)\\] Calculamos el determinante de \\(DF(0,-1,1)\\) y vemos que da \\(0\\). Conclusión: no podemos aplicar el teorema, pero pensando un poco vemos que \\[F_1 = (x+z)^2 -2(x+z) + 1 = (x+z-1)^2\\] \\[F_1 = 0 \\dimplies x=1-z\\] \\[F_2(x,y,z) = F_2(1-z,y,z) = (1-z)^2 + 4y^2 + 4z^2 + 4(1-z)y + 4(1-z)z+8yz\\] \\[ =... = a(z)y^2+b(z)y+c(z) \\implies y=\\frac{-b\\pm \\sqrt{...}}{2a}\\] ¿Cual de las 2 soluciones escoger? Sabemos (por el enunciado) que si \\(z=1\\) entonces \\(y=-1\\). Escogeremos la solución a la que dandole el valor \\(z=1\\) nos de \\(-1\\). \\[\\begin{array}{cc} x^3+z^3y^3+z &amp;= 0\\\\ cos(xyz)+sen(z)-1 = 0 \\end{array}\\] ¿Es despejable en función de \\(z\\) en un entorno de \\((x,y) = (0,0), z=0\\). Solución: El punto es solución del sistema. Definimos \\[\\appl{F}{\\real^3}{\\real^2}, F\\in C^1, F(0,0,0)=(0,0)\\] Calculamos el determinante de \\(DF(0,0,0)\\) y vemos que da \\(0\\). Conclusión: no podemos aplicar el teorema\\ Supongamos que si se puede despejar. Entonces tendríamos algo de la forma: \\[\\begin{gather*} \\left\\{\\begin{array}{cc} [x(z)]^3+z^3[y(z)]^3+z &amp;= 0\\\\ cos(x(z)y(z)z)+sen(z)-1 &amp;= 0 \\end{array}\\right\\}\\rightarrow\\\\ \\,\\\\ \\left\\{ \\begin{array}{cc} 3[x(z)]^3x&#39;(z) + 3z^2[y(z)]^2 + z^32y(z)y&#39;(z) + 1 &amp;=0\\\\ -sen(x(z)y(z)z) \\cdot\\{...\\} + cos(z) &amp;= 0 \\end{array}\\right\\} \\equiv\\\\\\,\\\\ \\left\\{ \\begin{array}{cc} 0+1&amp;=0\\\\ 0+1&amp;=0 \\end{array}\\right. \\end{gather*}\\] Esto demuestra que no pueden existir las derivadas. 7.1 Definición y ejemplos 7.2 El teorema del valor medio 7.3 Funciones convexas 7.4 Funciones inversas 7.5 Regla de L’Hospital 7.6 Teorema de Taylor en \\(\\mathbb{R}\\) 7.7 Método de Newton "],
["integracion-de-riemann.html", "Capítulo 8 Integración de Riemann 8.1 Integral de Riemann-Darboux 8.2 Propiedades de la integral 8.3 Evaluación de la integral 8.4 Fórmula de Stirling 8.5 Teoremas del valor medio, versión integral 8.6 Estimación de la integral 8.7 Integrales impropias 8.8 La integrabilidad según Riemann 8.9 Funciones a variación acotada 8.10 La integral de Riemann-Stieltjes", " Capítulo 8 Integración de Riemann En dimensión 1, la teoría de integración la basábamos en la integral de Riemann. Tomábamos una partición de \\([a,b]\\): \\(\\mathcal{P} = a = t_0 &lt; t_1&lt;...&lt;t_k = b\\) y a cada subintervalo de esa partición \\(I_i =[t_i,t_{i+1}]\\) asignábamos dos pares de valores, \\(M_i\\) y \\(m_i\\): \\[\\begin{gather*} M_i = \\sup \\{f(x)\\tq x\\in I_i\\}\\\\ m_i = \\inf \\{f(x)\\tq x\\in I_i\\} \\end{gather*}\\] Definíamos las sumas superiores (\\(\\mathbb{U}\\)) e inferiores (\\(\\mathbb{L}\\)): \\[\\begin{gather*} \\mathbb{U}(f,\\mathcal{P}) = \\sum_{i=0}^{k-1} M_i(t_{i+1}-t_i) \\\\ \\mathbb{L}(f,\\mathcal{P}) = \\sum_{i=0}^{k-1} m_i(t_{i+1}-t_i) \\end{gather*}\\] Esto nos da una aproximación al área debajo de la función como una suma de rectángulos. Tal y como los hemos definido, está claro que \\[ \\mathbb{L}(f,\\mathcal{P}) ≤ \\int_a^b f ≤ \\mathbb{U}(f,\\mathcal{P}) \\] También parece obvio que, cuanto más pequeña sea la anchura de los intervalos, más se aproximarán las sumas superiores e inferiores al valor real. Definimos entonces \\[\\begin{gather*} \\alpha = \\lim_{\\abs{\\mathcal{P}}\\to 0} \\mathbb{U}(f,\\mathcal{P}) \\\\ β = \\lim_{\\abs{\\mathcal{P}}\\to 0} \\mathbb{L}(f,\\mathcal{P}) \\end{gather*}\\] , donde \\(\\abs{\\mathcal{P}} = \\max t_{i+1} - t-i\\), es decir, la anchura máxima de los intervalos que formarn \\(\\mathcal{P}\\). A partir de aquí podemos llegar a la definición de función integrable: El recíproco es . Se ve fácilmente si \\(f\\) es una función escalonada, por ejemplo: podemos calcular el área debajo de ella sin problemas pero no es continua. En dimensión mayor que uno teníamos una serie de generalizaciones, y empezábamos con el teorema de Fubini que nos permite el cambio en el orden de integración: Por ejemplo, si tenemos la siguiente integral (ver figura ) \\[ I_1 = \\int_0^2\\int_{y^2}^{2y} f(x,y)\\,dx\\,dy \\] y queremos invertirla cambiamos los límites de integración: \\[ I_1 = \\int_0^4 \\int_{\\frac{x}{2}}^{\\sqrt{x}} f(x,y)\\, dy\\,dx \\] Otro ejemplo: estudiemos \\[ I_2 = \\int_a^bf(x)\\,dx = \\int_a^b\\int_0^{f(x)}dx\\,dy \\]. Si probamos a cambiar el orden de integración, tenemos que \\[ I_2 = \\int_0^M\\int_{A(y)}dx\\,dy \\] Donde \\(M\\) es el máximo de \\(f\\) y \\(A(y) = \\{ x\\in [a,b] \\tq f(x) ≥ y \\}\\). La longitud de ese conjunto \\(A\\) se denomina la medida, y entonces podemos expresar \\[ I_2 = \\int_0^\\infty \\abs{\\{x \\tq f(x) &gt; y\\}}\\,dy \\] ya que la medida del conjunto será \\(0\\) cuando \\(y &gt; M\\). Curiosamente, hemos pasado de una integral de Riemann a otra con una expresión distinta, la llamada . Esto pertence al campo de la , y permite estudiar conjuntos extraños y más monstruos y engendros varios. En general, podemos expresar nuestro cambio de variable de forma más general para cualquier cambio de variable: Para calcular la longitud aplicamos las ideas básicas del cálculo integral: el intervalo \\([a,b]\\), y aproximamos cada uno de esos trozos por un segmento. Calculamos la suma de la longitud de esos segmentos, hacemos tender la anchura de los y si converge, la longitud se puede medir. Este teorema responde a una idea con respecto al cambio de variable: si tomamos \\(\\Gamma\\) como la curva que queremos integrar, entonces se puede expresar \\[ L(\\sigma) = \\int_\\Gamma 1\\,d\\sigma = \\int_a^b \\md{\\sigma&#39;(t)}\\,dt \\] donde \\(\\md{\\sigma&#39;(t)}\\) es el cambio en la medida correspondiente. en general si la curva es sólo continua \\[\\appl{\\sigma}{[0,1]}{\\real^2}\\] Descripción gráfica: entre \\(\\frac{1}{2^k}\\) y \\(\\frac{1}{2^{k+1}}\\) y formo un triángulo isósceles con esos 2 puntos y de altura hasta la bisectriz. Sea el triangulo K: \\[Longitud =L_k = ... ??? ... = \\frac{1}{2k+1} \\{\\sqrt{\\frac{1}{4k^2}+1} + \\sqrt{\\frac{1}{4k^2+4} +1} \\} \\ge \\frac{1}{2k+1}\\] \\[\\text{Longitud total } = \\sum L_k \\ge \\sum_k \\frac{1}{2k+1} = \\infty\\] Este mostruito no tiene longitud. ¿Y si en vez de rectas tomamos sinusoides? Esa función si debería ser \\(C^1\\) pero la longitud es \\(\\infty\\). Entonces la curva no puede ser \\(C^1\\) (sería un contrajemplo del teorema) \\[ \\appl{\\sigma}{[a,b]}{\\real^n} \\] con \\(\\sigma\\) curva \\(C^1\\) y regular. Entonces \\[ L(s) = \\int_a^s \\md{\\sigma&#39;(t)}\\,dt \\] y por el Teorema Fundamental del Cálculo \\[ L&#39;(s) = \\md{\\sigma&#39;(s)} \\] Al imponer que la curva sea regular, entonces \\(\\sigma&#39;(s)\\neq 0\\) y por lo tanto existe la inversa \\(\\inv{L}\\). Si tenemos entonces un \\(\\tau ∈ [0,L(b)]\\), entonces \\(S=\\inv{L}(\\tau) ∈ [a,b]\\). Definimos \\[\\begin{gather*} \\sigma^\\ast = \\sigma \\circ \\inv{L} \\\\ \\sigma^\\ast = \\sigma(\\inv{L}(\\tau)) \\\\ (\\sigma^\\ast)&#39;(\\tau) = \\sigma&#39;(\\inv{L}(tau)) (\\inv{L}(\\tau))&#39; = \\sigma&#39;(\\inv{L}(\\tau)) \\frac{1}{L&#39;(\\inv{L}(\\tau))} = \\sigma&#39;(s) \\frac{1}{L&#39;(s)} = \\frac{\\sigma&#39;(s)}{\\md{\\sigma&#39;(s)}} \\end{gather*}\\] Es decir, hemos conseguido una parametrización con velocidad constante \\(\\md{(\\sigma^\\ast)&#39;} = 1\\) y por lo tanto \\[ L(\\tau) = \\int_0^\\tau \\md{(\\sigma^\\ast)&#39;}\\,ds = \\int_0^s 1\\,ds = s \\] Supongamos que estamos en \\(\\real^3\\). Podemos hablar de la longitud de una variedad de dimensión 1, del área de una de dimensión 2 o del volumen de una de dimensión 3. Ahora bien, ¿qué ocurre cuando pasamos a dimensiones superiores? La denominación será la siguiente Para calcular esas empezaremos partiendo del área de un parelelepípedo. Definiremos el paralelepípedo como, dados \\(k\\) vectores independientes \\(\\{\\gv_i\\} \\subset \\real^N\\), \\[ P_k = \\sum_{i=1}^k \\lambda_i\\gv_i\\;\\lambda_i \\in [0,1] \\] Elemento de área: \\(P_k \\equiv \\{ \\sum_{i=1}^k \\lambda_i\\gor{v}_i, \\lambda_i \\in [0,1]\\}\\subset \\real^N\\) Definimos una \\[\\begin{gather*} \\appl{\\Psi}{\\real^N}{\\real^N}\\\\ \\gx \\longrightarrow (\\Psi_1(\\gx),...,\\Psi_n(\\gx)) \\end{gather*}\\] Si \\(\\gx,\\gy\\in P_k \\implies \\pesc{\\gx,\\gy} = \\underbrace{\\pesc{\\Psi(\\gx),\\Psi(\\gy)}}_{\\text{Prod } \\real^N} = \\underbrace{{\\tilde{\\Psi}(\\gx),\\tilde{\\Psi}(\\gy)}}_{\\text{Prod } \\real^K}\\) Área \\((P_k)\\) = Área \\((\\tilde{\\Psi}(P_k)) \\equiv \\displaystyle \\int_{\\tilde{\\Psi}(P_k)}d\\gor{s}\\), una integral en \\(\\real^K, K&lt;N\\) El paso que vamos a dar ahora es: Construir una aplicación \\(L\\) tal que \\(L(\\gor{e}_i) = \\tilde{\\Psi}(\\gor{v}_i)\\) Y aplicamos el cambio de variables: \\[\\int_{\\tlps(P_k)} d\\gor{s} = \\int_{[0,1]^K} \\abs{\\det\\left( \\dpa{s}{t} \\right)} d\\gor{t}\\] Tenemos: Con la segunda propiedad podemos construir la aplicación \\(L\\). \\[\\begin{gather*} L = \\begin{pmatrix} \\tlps(\\gv_1) &amp; \\tlps(\\gv_2) &amp; ... &amp; \\tlps(\\gv_k)\\\\ \\downarrow &amp; \\downarrow &amp; \\ddots &amp; \\downarrow \\end{pmatrix} = \\frac{d\\gor{s}}{d\\gor{t}}\\end{gather*}\\] \\[Area(P_k) = \\abs{\\det L} = \\left|\\det \\begin{pmatrix} \\tlps(\\gv_1) &amp; \\tlps(\\gv_2) &amp; ... &amp; \\tlps(\\gv_k)\\\\ \\downarrow &amp; \\downarrow &amp; \\ddots &amp; \\downarrow \\end{pmatrix}\\right|\\] \\[ Area(P_k) = \\left( \\det \\begin{pmatrix} \\tlps(\\gv_1) &amp; \\tlps(\\gv_2) &amp; ... &amp; \\tlps(\\gv_k)\\\\ \\downarrow &amp; \\downarrow &amp; \\ddots &amp; \\downarrow \\end{pmatrix} \\begin{pmatrix} \\tlps(\\gv_1) &amp; \\tlps(\\gv_2) &amp; ... &amp; \\tlps(\\gv_k)\\\\ \\downarrow &amp; \\downarrow &amp; \\ddots &amp; \\downarrow \\end{pmatrix} ^T \\right)^{\\frac{1}{2}}\\] ¿Qué ganamos? \\[\\begin{pmatrix} \\pesc{\\tlps(\\gv_1),\\tlps(\\gv_1)} &amp; \\pesc{\\tlps(\\gv_1),\\tlps(\\gv_2)} &amp; ... &amp; \\pesc{\\tlps(\\gv_1),\\tlps(\\gv_k)}\\\\ \\pesc{\\tlps(\\gv_2),\\tlps(\\gv_1)} &amp; \\pesc{\\tlps(\\gv_2),\\tlps(\\gv_2)} &amp; ... &amp; \\pesc{\\tlps(\\gv_2),\\tlps(\\gv_k)}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ \\pesc{\\tlps(\\gv_k),\\tlps(\\gv_1)} &amp; \\pesc{\\tlps(\\gv_k),\\tlps(\\gv_2)} &amp; ... &amp; \\pesc{\\tlps(\\gv_k),\\tlps(\\gv_k)} \\end{pmatrix} = (\\ast) = \\left(\\det(\\pesc{v_i,v_j})\\right)^{\\frac{1}{2}}\\] (\\(\\ast\\)) Hemos construido las cosas de tal manera que se mantiene el producto escalar. \\(P_2 \\subset \\real^3\\) generado por \\(\\gu,\\gv\\in\\real^2\\) \\(Area(P_2) = \\left(\\det \\begin{pmatrix} \\pesc{\\gu,\\gu} &amp; \\pesc{\\gu,\\gv}\\\\ \\pesc{\\gv,\\gu} &amp; \\pesc{\\gv,\\gv} \\end{pmatrix} \\right)^{\\frac{1}{2}} = \\left(\\pesc{\\gu,\\gu}\\pesc{\\gv,\\gv} - \\pesc{\\gu,\\gv}^2\\right)^{\\frac{1}{2}} =\\left(\\md{\\gu}\\md{\\gv} - \\md{\\gu}\\md{\\gv}(cos(\\theta)^2)\\right)^2 = \\md{u}\\md{v} \\sqrt{1-cos^2(\\theta)} = \\md{u}\\md{v} sen(\\theta) = ||u\\times v||\\) Esto cumple lo que la sabíamos de selectividad. El área de un paralelogramo 2 dimensional en \\(\\real^3\\) es la raiz cuadrada del módulo del producto vectorial. Vamos a aplicar esto para hallar el área de una \\(k-variedad\\) en \\(\\real^N\\). \\(Area(\\Phi(Q_k)) = Area(P_k) + err(h)\\) donde \\(Area(P_k) = \\left(\\det (\\pesc{v_i,v_j})\\right)^{\\frac{1}{2}}, \\{v_j\\}\\) genera \\(P_k\\). \\[\\left(\\det\\left(\\pesc{\\dpa{\\Phi}{s_i},\\dpa{\\Phi}{s_j}}h_ih_j\\right)\\right)^{\\frac{1}{2}}\\] Tomamos \\(h_i = h, \\forall i\\) \\[\\underbrace{\\underbrace{h^k}_{Area(Q_k)} \\left(\\det\\left(\\pesc{\\dpa{\\Phi}{s_i},\\dpa{\\Phi}{s_j}}\\right)\\right)^{\\frac{1}{2}}}_{Area(P_k)}\\] Conclusión: \\[Area(M) = \\sum_n Area(Q_k^n) \\left(\\det\\left(\\pesc{\\dpa{\\Phi}{s_i}(\\gs_0^n),\\dpa{\\Phi}{s_j}(\\gs_0^n)}\\right)\\right)^{\\frac{1}{2}} + err(h)\\] Si llamamos \\(F(s_0^n) = \\left(\\det\\left(\\pesc{\\dpa{\\Phi}{s_i}(\\gs_0^n),\\dpa{\\Phi}{s_j}(\\gs_0^n)}\\right)\\right)^{\\frac{1}{2}}\\) Tenemos: \\[\\sum_n Area(Q_k) F(s_0^n) \\convs[Riemann] \\int_D F(\\gs)d\\gs\\] \\(Area(M) = \\int_D \\left(\\det\\left(\\pesc{\\dpa{\\Phi}{s_i},\\dpa{\\Phi}{s_j}}\\right)\\right)^{\\frac{1}{2}}\\) Lo que en Cálculo 2 llamábamos \\(\\md{T_u\\times T_v}\\). Dada una \\(\\appl{f}{\\real^N}{\\real}\\) podemos definir: \\[\\int_{\\Phi(D)}f dA = \\int_{D}f(\\Phi(s)) \\left(\\det(\\pesc{\\Phi_{s_i},\\Phi_{s_j}})\\right)^{\\frac{1}{2}} ds\\] \\[\\appl{\\sigma}{[a,b]}{\\real^N}\\] Sea \\(\\Gamma = \\{\\sigma(t)\\in\\real^N\\tq t\\in[a,b]\\}\\) El elemento de área sería: \\[\\int_{\\Gamma} fdA = \\int_a^bf(\\sigma(t)) (\\pesc{\\sigma&#39;,\\sigma&#39;})^{\\frac{1}{2}}\\] \\[ \\Phi : D\\subset\\real^2\\rightarrow \\real^3\\] \\[(s_1,s_2)\\rightarrow (x(s_1,s_2),y(s_1,s_2),z(s_1,s_2))\\] Tenemos en este caso: \\[\\int_{\\Phi(D)}fdA = \\int_D f(\\Phi(s_1,s_2)) (\\det(\\pesc{\\Phi_{s_i},\\Phi_{s_j}}))^{\\frac{1}{2}})ds\\] \\[(\\det(\\pesc{\\Phi_{s_i},\\Phi_{s_j}}))^{\\frac{1}{2}}) = (*) = \\md{\\Phi_1 \\times \\Phi_2}\\] \\((*)\\) visto anteriormente. Integrales de sobre y en \\(\\real^3\\). Volvemos a plantearnos el problema: ¿Distintas parametrizaciones nos da el mismo trabajo/flujo? Depende de la de la parametrización. No es lo mismo el trabajo para ir desde abajo de la curva hasta arriba que para bajarla. ¿Este problema traducido a \\(\\real^K\\), flipas o k ase? Recordamos algunos teoremas y definiciones vistos en Calculo 2: Los campos vectoriales son funciones \\(\\appl{F}{\\real^N}{\\real^N}\\), en el que a cada punto se le asigna un vector. Todos estos teoremas son parecidos en cuanto a que a la izquierda se tiene el campo evaluado en la frontera y a la derecha tenemos una integral en el interior de una expresión más o menos compleja en la que aparecen las derivadas. Cabe esperar que sean casos particulares de un teorema superior, cosa que es cierta. Este teorema se le llama de en general. De aquí al final de curso nos dedicaremos a llegar a ese teorema y ver que estos 3 teoremas son casos particulares. Para ello nos adentraremos en el espinoso jardín de la orientación. El tema de la orientación tiene que ver con tener cuidado con el orden. Vamos a ver los ejemplos de pocas dimensiones: \\[\\mathcal{B}_1 = \\{(0,1),(1,0)\\}\\] \\[\\mathcal{B}_2 = \\{(1,0),(0,1)\\}\\] Sea \\(\\gv = (v_1,v_2)_{\\mathcal{B}_1} = (v_2,v_1)_{\\mathcal{B}_2} \\neq \\gv\\) Lo que haremos en este caso será un cambio de base, \\(\\mathcal{B}_1,\\mathcal{B}_2\\) bases en \\(\\real^N\\). Sea \\(\\mathcal{C}_{\\mathcal{B}_1\\to\\mathcal{B}_2}\\) matriz del cambio de base de \\(\\mathcal{B}_1\\) a \\(\\mathcal{B}_2\\). ¿Que pasaría si el determinante de esa matriz de cambio de base sea 0? No puede ser (por definición de cambio de base, que tiene que ser reversible). \\(\\real^3\\). Sean \\[\\mathcal{B}_1 = (\\gu,\\gv,\\gw),\\mathcal{B}_2 = (\\gv,\\gu,\\gw),\\mathcal{B}_3 = (\\gw,\\gu,\\gv)\\] Si tomamos $= \\[\\begin{pmatrix} 1\\\\0\\\\0\\end{pmatrix}\\] _{_1} = \\[\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}\\] _{_2} = \\[\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}\\] _{_3} $ Siguiendo este razonamiento llegamos a la matriz de cambio de base: \\[\\mathcal{C}_{\\mathcal{B}_1\\to\\mathcal{B}_2} = \\begin{pmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}\\] Cuyo determinante es negativo. Lo mismo con \\[\\mathcal{C}_{\\mathcal{B}_1\\to\\mathcal{B}_1} = \\begin{pmatrix} 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{pmatrix}\\] Cuyo determinante es positivo. ¿Porqué al cambiar el orden se cambia la orientación? Aceptamos como orientación positiva la de la base canónica ordenada como Dios manda: \\(\\{(1,0,0,...,0), (0,1,0,0,...,0), (0,0,1,0,0,...,0),...,(0,0,...,0,1)\\}\\) Vamos con algún ejemplo más: (Importante) En \\(\\real^3\\), sean \\(\\gu,\\gv\\) independientes. Construimos \\(\\mathcal{B} = \\{\\gu,\\gv,\\gu\\times\\gv\\}\\). Queremos saber si esta base es de orientación positiva u orientación negativa. \\[\\gu\\times\\gv = \\left| \\begin{array}{ccc} \\overrightarrow{i} &amp; \\overrightarrow{j} &amp; \\overrightarrow{k}\\\\ u_1 &amp; u_2 &amp; u_3 \\\\ v_1 &amp; v_2 &amp; v_3 \\end{array}\\right| = ... = \\left(\\left|\\begin{array}{cc}u_2&amp;v_2\\\\u_3&amp;v_3\\end{array}\\right|,\\left|\\begin{array}{cc}u_1&amp;v_1\\\\u_3&amp;v_3\\end{array}\\right|,-\\left|\\begin{array}{cc}u_1&amp;v_1\\\\u_2&amp;v_2\\end{array}\\right|\\right)\\] [_{} = \\[\\begin{pmatrix} u_1&amp;v_1&amp; \\left|\\begin{array}{cc} u_2&amp;v_2\\\\u_3&amp;v_3 \\end{array}\\right|\\\\ u_2&amp;v_2&amp; -\\left|\\begin{array}{cc} u_1&amp;v_1\\\\u_3&amp;v_3 \\end{array}\\right|\\\\ u_3&amp;v_3&amp; \\left|\\begin{array}{cc} u_1&amp;v_1\\\\u_2&amp;v_2\\end{array}\\right| \\end{pmatrix}\\] ] Cuyo determinante es \\[\\left|\\begin{array}{cc}u_2&amp;v_2\\\\u_3&amp;v_3\\end{array}\\right|^2+\\left|\\begin{array}{cc}u_1&amp;v_1\\\\u_3&amp;v_3\\end{array}\\right|^2+\\left|\\begin{array}{cc}u_1&amp;v_1\\\\u_2&amp;v_2\\end{array}\\right|^2&gt;0\\] \\(\\real^2\\) Sean \\[\\mathcal{C} = \\{(1,0),(0,1)\\}\\] \\[\\mathcal{B} = \\{(u_1,u_2),(v_1,v_2)\\}\\] La matriz fácil de calcular es \\[C_{\\mathcal{B}\\to \\mathcal{C}} = \\begin{pmatrix} u_1&amp;v_1\\\\u_2&amp;v_2\\end{pmatrix}\\] \\[0&lt;\\det \\begin{pmatrix} u_1&amp;v_1\\\\u_2&amp;v_2\\end{pmatrix} = \\pesc{(u_1,u_2),(v_1,v_2)} = \\pesc{(\\lambda,0),(\\beta,-\\alpha)} = \\lambda\\beta\\] Aplicando un giro (de aquí salen \\(\\lambda\\)…) podemos ver que el si el ángulo entre \\(\\gu,\\gv \\in (0,\\pi) \\leadsto +\\) pero si el ángulo entre \\(\\gu,\\gv \\in (\\pi,2\\pi) \\leadsto -\\). ¿Y en \\(\\pi\\)? Entonces los vectores no serían independientes. Vamos a manipular con los vectores de la base aplicandoles una deformación continua para ver que pasa con la discontinuidad de la orientación en \\(\\pi\\) a ver que encontramos. Sea la deformación \\((\\alpha(t),\\beta(t)), t\\in(0,1)\\) continua, donde \\((\\alpha(0),\\beta(0)) = (e_1,e_2)\\) y \\((\\alpha(1),\\beta(1)) = (e_1,e_2)\\) en \\(\\real^3\\). Idea geométrica: Yo tengo 2 bases y quiero pasar de una a otra, si puedo encontrar un camino en el que los 3 vectores nunca pertenezcan al mismo plano entonces la orientación es la misma. . Sea \\[\\mathcal{B} = \\{e_1,e_2,e_3\\} \\hspace*{150pt} \\mathcal{C} = \\{\\gu,\\gv,\\gw\\}\\] No hay ninguna manera de llevar \\(e_3\\) a \\(\\gw\\) sin formar un único plano con los tres vectores. Aunque podríamos llevar \\(e_3 \\to \\gu\\) y \\(e_1 \\to \\gw\\). Si no se mantiene el orden, tendríamos la base \\(\\mathcal{C}* = \\{\\gw,\\gv,\\gu\\}\\neq\\mathcal{C}\\). Ahora que ya tenemos vista la orientación de bases en \\(\\real^N\\) vamos a ver la orientación en \\(T_a M\\) (espacio tangente de una variedad en un punto) Sea \\(\\{e_1,e_2,\\dotsc,e_k\\}\\) la base canónica en \\(\\real^k\\), y \\(\\Phi(u_0)=\\ga\\) Como la variedad viene dada por una parametrización, tomamos \\(\\img D\\Phi\\), es decir: \\[T_{\\ga}M = \\img D\\Phi(u_0) = \\{D\\Phi(u_0)\\gv,\\gv\\in\\real^K\\}\\] Tomando la base euclídea: \\[D\\Phi(u_0) \\{\\sum^k v_i\\gor{e}_i\\}\\equiv \\sum^k v_i\\underbrace{[D\\Phi(u_0)\\gor{e}_i]}{\\in\\real^N} \\] Es decir, la \\(\\img D\\Phi\\) está generada por los k vectores \\[\\begin{equation}\\label{baseTaM} \\{ D\\Phi(u_0)\\gor{e}_1,\\dotsc ,D\\Phi(u_0)\\gor{e}_k \\} \\end{equation}\\] La condición de rango máximo nos asegura que los \\(k\\) vectores son linealmente independientes. El conjunto () es una base de \\(T_{\\ga}M\\) Si tenemos 2 parametrizaciones distintas \\(\\Phi,\\Psi\\) entonces tendremos 2 bases diferentes. Cabe plantearse bajo qué condiciones se mantiene la orientación. (aunque todavía no tengamos muy claro que es la orientación de una variedad) Las orientaciones de una curva en \\(\\real^N\\) es fácil, basta comprobar los sentidos de los vectores tangentes. Vamos a hacer las cuentas: Sean \\(\\sigma,\\beta\\) parametrizacones del mismo trozo de la curva, que parten de orígenes distintos. \\[\\appl{\\sigma}{U}{\\Gamma},s\\to \\sigma(s)\\] \\[\\appl{\\beta}{V}{\\Gamma},t\\to \\beta(t)\\] Tenemos un teorema que nos garantiza que existe un ()g, tal que \\[\\sigma(s) = \\beta(g(s)) \\y \\sigma&#39;(s) = \\beta&#39;(g(s))\\cdot g&#39;(s)\\] En este caso, para que las dos parametrizaciones den la misma orientación, \\(g&#39;&gt;0\\). Las orientaciones de una superficie también, basta comprobar los sentidos de los vectores normales. \\[\\appl{\\Phi}{U}{\\real^3}, (u,v) \\to \\Phi(u,v)\\] \\[\\appl{\\Psi}{V}{\\real^3}, (r,s) \\to \\Psi(r,s)\\] Tenemos un teorema que nos garantiza… tal que \\[g(u,v) = (g_1(u,v),g_2(u,v))\\] En este caso, calculamos \\[\\left.\\begin{array}{c} \\Phi_u\\times\\Phi_v \\\\ \\Psi_r\\times\\Psi_s \\end{array}\\right\\}\\] Tenemos que \\(\\Phi(u,v) = \\Psi(g_1(u,v),g_2(u,v))\\) \\[\\begin{gather*} \\Phi_1(u,v) = \\Psi_1(g_1(u,v),g_2(u,v))\\\\ (\\Phi_1)_{u} = (\\Psi_1)_r r_u + (\\Psi_1)_s s_u = ((\\Psi)_r\\,(\\Psi_1)_s)\\begin{pmatrix} (g_1)_u\\\\(g_2)_v \\end{pmatrix} \\end{gather*}\\] Se calculan las 6 derivadas \\((\\Phi_1,\\Phi_2,\\Phi_3)\\) respecto de \\(u\\) y de \\(v\\) y acabamos llegando a \\[\\begin{pmatrix} (\\Phi_1)_u\\\\(\\Phi_2)_u\\\\(\\Phi_3)_u \\end{pmatrix} = \\begin{pmatrix} (\\Psi_1)_r &amp; (\\Psi_1)_s\\\\ (\\Psi_2)_r &amp; (\\Psi_2)_s\\\\ (\\Psi_3)_r &amp; (\\Psi_3)_s \\end{pmatrix}\\begin{pmatrix} (g_1)_u\\\\ (g_2)_u \\end{pmatrix} \\] Análogamente: \\[\\begin{pmatrix} (\\Phi_1)_v\\\\(\\Phi_2)_v\\\\(\\Phi_3)_v \\end{pmatrix} = \\begin{pmatrix} (\\Psi_1)_r &amp; (\\Psi_1)_s\\\\ (\\Psi_2)_r &amp; (\\Psi_2)_s\\\\ (\\Psi_3)_r &amp; (\\Psi_3)_s \\end{pmatrix}\\begin{pmatrix} (g_1)_v\\\\ (g_2)_v \\end{pmatrix} \\] Ahora procedemos a calcular \\[\\Phi_u\\x\\Phi_v = \\dotsc = \\left( \\left|\\begin{array}{cc} (\\Phi_2)_u &amp; (\\Phi_2)_v\\\\ (\\Phi_3)_u &amp; (\\Phi_3)_v \\end{array} \\right|,- \\left|\\begin{array}{cc} (\\Phi_1)_u &amp; (\\Phi_1)_v\\\\ (\\Phi_3)_u &amp; (\\Phi_3)_v \\end{array} \\right|, \\left|\\begin{array}{cc} (\\Phi_1)_u &amp; (\\Phi_1)_v\\\\ (\\Phi_2)_u &amp; (\\Phi_2)_v \\end{array} \\right| \\right)\\] Tomando el primer elemento calculamos: \\[ \\det\\left( \\begin{array}{cc} (\\Psi_2)_r &amp; (\\Psi_2)_s\\\\ (\\Psi_3)_r &amp; (\\Psi_3)_s \\end{array} \\right)\\begin{pmatrix} (g_1)_u &amp;(g_1)_v \\\\ (g_2)_u &amp; (g_2)_v \\end{pmatrix} = \\left| \\begin{array}{cc} (\\Psi_2)_r &amp; (\\Psi_2)_s\\\\ (\\Psi_3)_r &amp; (\\Psi_3)_s \\end{array}\\right| \\det Dg \\] Repitiendo la cuenta con el resto de los elementos, llegamos a \\[\\Phi_u\\x\\Phi_v = \\Psi_r\\x\\Psi_s\\cdot (\\det Dg)\\] Para mantener la orientación necesitamos \\(\\det Dg &gt; 0\\) Este teorema solo afecta a la zona de la subvariedad \\(M\\) parametrizada por \\(\\Phi\\) y \\(\\Psi\\) al mismo tiempo. La parametrización es \\[ \\Phi(t,\\theta) = \\left((R+t\\cdot sen\\frac{\\theta}{2})cos\\theta, (R+t\\cdot sen\\frac{\\theta}{2})sen\\theta, t cos\\frac{\\theta}{2} \\right) \\, t\\in(-1,1),\\theta\\in(0,2\\pi) \\] ¿Estamos seguros de que es una parametrización? Vamos a comprobar el homeomorfismo, Tenemos 2 posibles caminos: Despejamos en función de \\(x,y,z\\) \\(\\frac{y}{x} = tg \\theta\\). Definimos : \\[ \\begin{array}{ccc} \\theta &amp;= arctg\\frac{y}{x} &amp; (1)\\\\ \\theta &amp;= arctg\\frac{y}{x} + \\pi &amp; (2)\\\\ \\theta &amp;= arctg\\frac{y}{x} + 2\\pi &amp; (3) \\end{array} \\] Y comprobar que es continua Escribimos este conjunto como un conjunto de nivel y ver que es una variedad, por lo tanto no hace falta comprobar el homeomorfismo sobre la imagen. \\[\\begin{gather*} \\sqrt{x^2+y^2} = (R + t\\cdot cos\\frac{\\theta}{2})\\\\ \\sqrt{x^2+y^2} - R = \\frac{z}{tg\\frac{&quot;arctg\\frac{y}{x}&quot;}{?}} \\end{gather*}\\] Una clave importante es que cuando \\(\\theta\\rightarrow 0^+ \\implies \\Phi(t,\\theta)\\) pero si \\(\\theta\\rightarrow 2\\pi^{-} \\implies \\Phi(-t,\\theta)\\) \\(\\overrightarrow{n} = T_t\\x T_{\\theta} = ... = \\overrightarrow{n}(t,\\theta)\\) \\[\\begin{gather*} \\overrightarrow{n}(t,\\theta) \\convs[][\\theta\\rightarrow 0^+] (-R,\\frac{t}{2})\\\\ \\overrightarrow{n}(t,\\theta) \\convs[][\\theta\\rightarrow 2\\pi^-] (R,-\\frac{t}{2}) \\end{gather*}\\] Nota: esta parametrización que no cubre el segmento con el que se empieza. Esta parametrización tiene una propiedad curiosa, al empezar, el vector normal apunta hacia dentro y al final, apunta hacia fuera. Es imposible cubrirla del todo con una parametrización. típico de superficie no orientable es la botella de Klein Conjuntos con frontera en \\(\\real^2\\). Utilizamos la tercera caracterización de subvariedad, que dice que existe un que vamos a llamar \\(\\Phi\\), (que por ser difeomorfismo, existe \\(\\Psi = \\Phi^{-1})\\). Podemos suponer que estamos trabajano con un cuadrado en el plano y su imagen para facilitar las cuentas. Partimos de En U es fácil orientar un cuadrado. ¿Cómo nos “llevamos” la orientación? Con la diferencial, que si queremos que sea la misma orientación \\(\\implies \\det D\\Phi &gt; 0\\). Sea \\(\\mathcal{C} = \\{e_1=(1,0),e_2=(0,1)\\}\\) base canónica en \\(\\real^2\\) (variables (s,t)). Sea \\(\\mathcal{B} = \\{D\\Psi(s_0,0)e_1,D\\Psi(s_0,0)e_2\\}\\) base en \\(\\real^2\\) (variables \\((x,y)\\)) Tenemos también \\(\\sigma(s) = \\Psi(s,0)\\) parametrización de un trozo de \\(dM\\) que contiene a \\(x_0\\). \\[D\\Psi = \\begin{pmatrix} \\displaystyle\\dpa{\\Psi_1}{s} &amp;\\displaystyle\\dpa{\\Psi_1}{t}\\\\ \\displaystyle\\dpa{\\Psi_2}{s} &amp;\\displaystyle\\dpa{\\Psi_2}{t} \\end{pmatrix}\\] En la primera columna lo que tenemos es el vector tangente a la curva. Cuando \\(t=0\\) tenemos la parametrización de la frontera. \\[ D\\Psi|_(s_0,0) = \\begin{pmatrix} \\sigma&#39;(s_0) &amp; \\overrightarrow{v}\\\\ \\downarrow &amp; \\downarrow \\end{pmatrix} \\] Haciendo un desarrollo de Taylor tenemos: \\[ \\Psi(s_0,t) = \\begin{pmatrix} \\Psi_1(s_0,0)\\\\ \\Psi_2(s_0,0) \\end{pmatrix} + t \\underbrace{\\begin{pmatrix} \\dpa{\\Psi_1}{t} (s_0,0)\\\\ \\dpa{\\Psi_2}{t} (s_0,0) \\end{pmatrix}}_{\\gor{v}} + err \\] \\[\\underbrace{\\begin{pmatrix}\\Psi_1(s_0,t)\\\\ \\Psi_2(s_0,t) \\end{pmatrix} }_{\\in M\\, t&gt;0}= \\gor{x}_0 + t\\gv + err \\] Conclusión: \\(\\gv\\) “apunta hacia el interior de \\(M\\)”. \\[\\mathcal{C} = \\{e_1,e_2\\}\\] \\[\\mathcal{B} = \\{D\\Psi(s_0,0)e_1,D\\Psi(s_0,0)e_2\\} = \\{\\sigma&#39;(s_0),\\gv\\}\\] \\[\\mathcal{B} \\text{ orientación positiva } \\dimplies \\det D\\Psi(s_0,0) &gt; 0 \\dimplies \\text{ Ángulo entre } \\sigma&#39;(s_0) \\text{ y } \\gv \\in (0,\\pi)\\] Con un dibujo se llegaría a entender perfectamente la siguiente conclusión (basada en la interpretación geométrica de que el ángulo \\(\\in (0,\\pi)\\)) si recorremos la frontera de la subvariedad con la mano izquierda hacia dentro es la orientación positiva si nuestra subvariedad queda en el interior de esa frontera/curva, si nuestra subvariedad es el exterior de la curva pues será la orientación negativa. \\[\\appl{\\Phi}{\\real^2}{\\real^3}\\] Tenemos $(s) = (s,0) $, parametrización de un trozo de \\(dM\\) que contiene a \\(x_0\\). Siendo \\[D\\Phi(s_0,0) = \\begin{pmatrix} ... \\end{pmatrix} = \\begin{pmatrix} \\sigma&#39;(s_0) &amp; \\overrightarrow{v}\\\\ \\downarrow &amp; \\downarrow \\end{pmatrix}\\] \\[\\mathcal{C} = \\{e_1,e_2,e_3\\}\\] \\[\\mathcal{B} = \\{D\\Phi(s_0,0)e_1,D\\Phi(s_0,0)e_2\\} = \\{\\sigma&#39;(s_0),\\gv\\}\\] Solo se tienen 2 vectores. Anteriormente hemos visto que es el producto vectorial () Es decir \\[\\mathcal{B} = \\{\\sigma&#39;(s_0),\\gv,\\sigma&#39;(s_0)\\x\\gv\\}\\] Con las cuentas vistas antes del ejemplo comprobamos que \\(\\gv\\) apunta hacia el interior de \\(M\\). Hola mundo 8.1 Integral de Riemann-Darboux 8.2 Propiedades de la integral 8.3 Evaluación de la integral 8.4 Fórmula de Stirling 8.5 Teoremas del valor medio, versión integral 8.6 Estimación de la integral 8.7 Integrales impropias 8.8 La integrabilidad según Riemann 8.9 Funciones a variación acotada 8.10 La integral de Riemann-Stieltjes "],
["series-numericas-infinitas.html", "Capítulo 9 Series numéricas infinitas 9.1 Definición y ejemplos 9.2 Series con términos no-negativos 9.3 Criterios de convergencia 9.4 Convergencia condicional y absoluta 9.5 Sucesiones dobles y series", " Capítulo 9 Series numéricas infinitas 9.1 Definición y ejemplos 9.2 Series con términos no-negativos 9.3 Criterios de convergencia 9.4 Convergencia condicional y absoluta 9.5 Sucesiones dobles y series "],
["sucesiones-y-series-de-funciones.html", "Capítulo 10 Sucesiones y series de funciones 10.1 Convergencia de sucesiones de funciones 10.2 Propiedades del límite de funciones 10.3 Convergencia de las series de funciones 10.4 Series de potencias", " Capítulo 10 Sucesiones y series de funciones 10.1 Convergencia de sucesiones de funciones 10.2 Propiedades del límite de funciones 10.3 Convergencia de las series de funciones 10.4 Series de potencias "],
["funciones-en-varias-variables.html", "Capítulo 11 Funciones en varias variables 11.1 Transformaciones lineales 11.2 Diferenciación 11.3 El principio de la contracción 11.4 El teorema de la función inversa 11.5 El teorema de la función implícita 11.6 Teorema del rango 11.7 Determinantes 11.8 Derivadas de orden superior 11.9 Diferenciación de integrales", " Capítulo 11 Funciones en varias variables 11.1 Transformaciones lineales 11.2 Diferenciación 11.3 El principio de la contracción 11.4 El teorema de la función inversa 11.5 El teorema de la función implícita 11.6 Teorema del rango 11.7 Determinantes 11.8 Derivadas de orden superior 11.9 Diferenciación de integrales "],
["integracion-de-formas-diferenciales.html", "Capítulo 12 Integración de formas diferenciales 12.1 Integración 12.2 Aplicaciones primitivas 12.3 Cambio de variables 12.4 Formas diferenciales 12.5 Cadenas y símplices 12.6 Teorema de Stoke 12.7 Formas cerradas y formas exactas 12.8 Análisis vectorial", " Capítulo 12 Integración de formas diferenciales Una forma diferencial es una funcion escalar definida en un abierto de \\(\\real^n\\) \\[\\appl{f}{\\Omega\\subset\\real^N}{\\real}\\] Operaciones habituales: Sea \\(\\mathcal{C} = \\{e_1,e_2,...,e_n\\}\\) la base canónica en \\(\\real^N\\). Sea \\(L\\) una aplicación lineal \\[\\appl{L}{\\real^N}{\\real}\\] Que recordamos que cumplen: \\[ L(\\gx+\\gy) = L(\\gx)+L(\\gy); L(\\lambda\\gx) = \\lambda L(\\gx)\\] Definimos \\(\\gy\\in\\real^N \\leadsto \\gy = \\displaystyle\\sum_1^n y_i e_i\\), con lo que \\[L(\\gy) = \\sum y_i L(e_i)\\] Entonces \\[\\left.\\begin{array}{cc} v_i = L(e_i)\\\\ y_i = P_i(\\gy) \\end{array}\\right\\} \\rightarrow L(\\gy) = \\sum_i v_iP_i(y)\\] Siendo \\(P_i\\) las proyecciones, una base del espacio dual. \\(P_i \\equiv dx_i\\). \\(dx_i[\\gy] \\equiv P_i(\\gy) = y_i\\) Entonces, dado un \\(\\gv\\) podemos construir \\[L \\equiv \\sum_i^N v_idx_i\\] \\[L[\\gy] = \\sum_i^N v_idx_i[\\gy] = \\sum_i^N v_iy_i\\] Indicaremos con paréntesis el punto en el que estamos evaluando, y con corchetes el punto en el que estamso actuando. Supongamos \\(f\\) una función escalar (una 0-forma). \\[\\grad f(\\gx) = \\left( \\dpa{f}{x_i}(\\gx)\\right)\\, i=1,...,N\\] Nos podemos construir una 1-forma desde el gradiente \\[\\dpa{f}{x_i}(\\gx)dx_i \\] A esta 1-forma en particular la llamaremos \\(df(\\gx)\\). ¿Utilidad? Ya la veremos, pero es una forma de escribir el producto escalar. \\[\\pesc{\\grad f(\\gx),\\gy} = df(\\gx)[\\gy]\\] Punto de partida: Aplicaciones \\[\\appl{\\Phi}{\\real^N\\x\\real^N}{\\real}\\] Que cumplen Consecuencias: en \\(\\real^3\\) para facilitar las cuentas. \\[\\Phi(\\gu,\\gv) = \\Phi(u_1e_1+u_2e_2+u_3e_3,v_1e_1+v_2e_2+v_3e_3)\\] Aplicando las propiedades anteriores obtenemos: \\[\\begin{gather*} \\overbrace{u_1v_1\\Phi(e_1,e_1)}^{\\equiv 0} + u_1v_2\\Phi(e_1,e_2) + u_1v_3+\\Phi(e_1,e_3)+\\\\ u_2v_1+\\Phi(e_2,e_1)+u_2v_2+\\Phi(e_2,e_2)+u_2v_3+\\Phi(e_2,e_3)+\\\\ u_3v_1\\Phi(e_3,e_1)+u_3v_2+\\Phi(e_3,e_2)+u_3v_3+\\Phi(e_3,e_3) = \\\\ \\underbrace{(u_1v_2-u_2v_1)}_{\\left|\\begin{matrix} u_1&amp;u_2\\\\v_1&amp;v_2 \\end{matrix}\\right|}\\overbrace{\\Phi(e_1,e_2)}^{C_1}+(u_1v_3-u_3v_1)\\Phi(e_1,e_3)+(u_2v_3-u_3v_2)\\Phi(e_2,e_3) \\end{gather*}\\] Hemos demostrado que \\[\\Phi(\\gu,\\gv) = C_1B_{12}(\\gu,\\gv) + C_2B_{13}(\\gu,\\gv) + C_3B_{23}(\\gu,\\gv)\\] \\(B_{ij} = dx_i\\y dx_j\\) \\[dx\\y dx_j [\\gu,\\gv] = \\det \\begin{pmatrix} u_i&amp;u_j\\\\v_i&amp;v_j \\end{pmatrix} = \\det \\begin{pmatrix} dx_i[\\gu]&amp;dx_j[\\gu]\\\\dx_[\\gv]&amp;dx_j[\\gv] \\end{pmatrix}\\] Vamos a dar una definición general de una k-forma. Elementos básicos: \\[\\dfl{x_{i_1}}{x_{i_k}}[\\gu^1,\\gu^2,...,\\gu^k] = \\det\\begin{pmatrix} u_{i_1}^1 &amp; ... &amp; u_{i_k}^1\\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ u_{i_1}^k &amp; ... &amp; u_{i_k}^k \\end{pmatrix}\\] Esto nos dice que en \\(\\real^N\\), teniendo \\(K\\)-formas (con \\(K&lt;N\\)) tenemos \\(\\comb{N}{K}\\) combinaciones distintas. Si \\(K&gt;N\\) y \\(\\omega\\) es una k-forma, entonces \\(\\omega \\equiv 0\\) En \\(\\real^3\\). Al cambio en la 2-forma, que es \\(dzdx\\). Esto es para seguir el (por temas de la orientación). Esto es \\(x\\to y \\to z \\to x\\) Las las podemos interpretar como 0-formas y como 3-formas. Los los podemos interpretar como 1-formas y también como 2-formas. Para escribir un conjunto de subíndices \\(\\{i_1,i_2,...,i_k\\} \\equiv I\\) También acortaremos \\(\\dfl{x_{i_1}}{x_{i_k}} \\equiv dx_I\\). La definición quedaría \\(\\displaystyle \\sum_I F_I(\\gx)dx_I\\) Siempre se puede multiplicar por 0-formas y sumar formas del mismo orden. Estas operaciones son triviales porque son operaciones internas. Vamos a definir las operaciones externas: Si \\(K+S&gt;N \\implies \\omega\\y\\beta=0\\) Vamos a por un ejemplo de producto exterior en \\(ℝ^3\\). Consideramos las dos siguientes formas diferenciales: \\[\\begin{gather*} \\omega = f_1(x,y,z)\\df x + f_2(x,y,z)\\df y + f_3(x,y,z) \\df z \\\\ \\beta= g_1(x,y,z)\\df x + g_2 (x,y,z) \\df y + f_3 (x,y,z) \\df z \\end{gather*}\\] y calculamos su producto exterior, \\(\\omega\\y\\beta\\) \\[\\begin{multline*} \\omega\\y\\beta = f_1g_1\\df{x,x} + f_1g_2\\df{x,y} + f_1g_3\\df{x,z} + f_2g_1\\df{y,x} +\\\\ + f_2g_2\\df{y,y}+ f_2g_3\\df{y,z} + f_3g_1\\df{z,x}+f_3g_2\\df{z,y}+f_3g_3\\df{z,z} \\end{multline*}\\] Tachamos los que sean 0 (\\(\\df{x,x} = 0\\)) y tenemos cuidado con el orden cíclico, y nos queda \\[ (f_2g_3-f_3g_2)\\df{y,z} + (f_3g_1-f_1g_3)\\df{z,x} + (f_1g_2 - f_2g_1) \\df{x,y} \\] Partiendo de 2 campos vectoriales que eran 1-formas hemos llegado a una 2-forma. Además, si nos fijamos, hemos llegado a la definición de en \\(ℝ^3\\): \\[ \\overrightarrow{F} \\x \\overrightarrow{G} = \\left((f_2g_3-f_3g_2),(f_3g_1-f_1g_3),(f_1g_2 - f_2g_1)\\right) \\] El diferencial exterior trata de ampliar el concepto del diferencial (la matriz de derivadas parciales) más allá de las funciones, de tal forma que podamos aplicarlo a formas diferenciales. Veamos algunos ejemplos, empezando en \\(ℝ^3\\). Partimos de un campo vectorial \\(G = (g_1,g_2,g_3)\\) al que asociamos una 2-forma \\(ω\\): \\[ \\omega = g_1 \\df{y,z} + g_2 \\df{z,x} + g_3 \\df{x,y} \\] Calculamos ahora la diferencial exterior, \\(\\dif\\omega\\). Para calcularla, recordamos que \\(\\df{x,x} = 0\\) y que podemos cambiar el orden del producto exterior si respetamos el orden cíclico (\\(x,y,z\\)). \\[\\begin{align*} \\dif\\omega &amp;= \\df{g_1,y,z} + \\df{g_2,z,x} + \\df{g_3,x,y} = \\\\ &amp;= \\left(\\dpa{g_1}{x} \\df x + \\dpa{g_1}{y} \\df y + \\dpa{g_1}{z}\\df z \\right)\\y \\df{y,z}\\\\ &amp;\\qquad + \\left(\\dpa{g_2}{x} \\df x + \\dpa{g_2}{y} \\df y + \\dpa{g_2}{z}\\df z \\right)\\y \\df{z,x}\\\\ &amp;\\qquad + \\left(\\dpa{g_3}{x} \\df x + \\dpa{g_3}{y} \\df y + \\dpa{g_3}{z}\\df z \\right)\\y \\df{x,y} = \\\\ &amp;= \\dpa{g_1}{x}\\df{x,y,z} + \\dpa{g_2}{y}\\df{y,z,x} + \\dpa{g_3}{z}\\df{z,x,y} = \\\\ &amp;= \\left(\\dpa{g_1}{x} + \\dpa{g_2}{y} + \\dpa{g_3}{z}\\right)\\df{x,y,z} \\end{align*}\\] Hemos llegado a una 3-forma que además es la forma diferencial asociada a la de \\(G\\). Pasamos a otro ejemplo, donde vamos a calcular la diferencial exterior de una 1-forma \\(ω=F_1\\df x + F_2 \\df y + F_3 \\df z\\). Recordamos que si invertimos el orden del producto exterior pagamos con un cambio de signo, esto es, \\(\\df{x,z} = - \\df{z,x}\\). \\[\\begin{align*} \\dif ω &amp;= \\dif\\left(F_1\\df x + F_2\\df y+F_3\\df z\\right) =\\\\ &amp;= \\df{F_1,x} + \\df{F_2,y} + \\df{F_3,z} = \\\\ &amp;= \\left(\\dpa{F_1}{x}\\df x + \\dpa{F_1}{x}\\df y + \\dpa{F_1}{x}\\df z \\right)\\y \\df x + \\left(\\dpa{F_2}{x}\\df x + \\dpa{F_2}{x}\\df y + \\dpa{F_2}{x}\\df z \\right)\\y \\df y + \\\\ &amp; \\qquad \\qquad + \\left(\\dpa{F_3}{x}\\df x + \\dpa{F_3}{x}\\df y + \\dpa{F_3}{x}\\df z \\right)\\y \\df z = \\\\ &amp;= \\left(\\dpa{F_3}{y} - \\dpa{F_2}{z} \\right)\\df{y,z} + \\left(\\dpa{F_1}{z} - \\dpa{F_3}{dx}\\right)\\df{z,x} + \\left(\\dpa{F_2}{x} - \\dpa{F_1}{y}\\right) \\df{x,y} \\end{align*}\\] Nos ha quedado un campo de la forma: \\[\\left(\\left(\\dpa{F_3}{y} - \\dpa{F_2}{z} \\right) ,\\left(\\dpa{F_1}{z} - \\dpa{F_3}{x}\\right),\\left(\\dpa{F_2}{x} - \\dpa{F_1}{y}\\right)\\right)\\] que coincide con el . \\(\\dif(\\omega + \\beta) = \\dif\\omega + \\dif\\beta\\) Siendo \\(\\omega = \\sum_I F_I \\dif x_I\\) una k-forma, \\(f\\) una 0-forma, tenemos que \\[ f\\omega = \\sum_I fF_I\\dif x_I \\] y entonces \\[ \\dif (f\\omega) = \\sum_I \\df{(fF_I),x_I} \\] , donde \\[ \\dif (fF_I) = \\sum_{j=1}^N \\dpa{fF_I}{x_j} \\df x_j = \\sum_{j=1}^N\\dpa{f}{x_j} F_I \\df x_j + \\sum_{j=1}^N\\dpa{F_I}{x_j} f \\df x_j = F_I \\dif f + f \\dif F_I \\] De esta forma, nos queda que \\[\\begin{align*} \\dif (fω) &amp;= \\sum_I \\left(F_I \\dif f + f \\dif F_I\\right)\\y \\dif x_I = \\\\ &amp;= \\sum_I F_I \\df{f,x_I} + \\sum_I f \\df{F_I,x_I} = \\\\ &amp;= \\sum_I \\dif f \\y F_I \\dif x_I + f \\dif ω = \\\\ &amp;= \\dif f \\y \\left(\\sum_I F_I \\dif x_I\\right) + f \\dif ω = \\\\ &amp;= ω \\df{f} + f \\dif ω \\end{align*}\\] Hemos llegado a una expresión similar a la de la derivada de un producto de funciones \\[d(f\\omega) = ω \\df{f} + f \\dif ω \\] Sean dos k-formas diferenciales \\(ω\\) y \\(β\\): \\[ \\omega =\\sum f_i \\df x_i ;\\; \\beta = \\sum_j g_j\\df x_j \\] Calculamos el diferencial de su producto exterior: \\[\\begin{align*} \\dif (\\omega \\y \\beta) &amp;= \\dif\\left(\\sum_{i,j=1}^N f_ig_jdx_y\\y dx_j\\right) =\\\\ &amp;= \\sum_{i,j=1}^N \\df{(f_ig_j),x_i,x_j} = \\\\ &amp;= \\sum_{i,j=1}^N \\left(\\sum_{k=1}^N \\dpa{(f_ig_j)}{x_k} \\dif x_k\\right)\\y \\df{x_i,x_j} = \\\\ &amp;= \\sum_{i,j,k=1}^N \\left(\\dpa{f_i}{x_k}g_j + f_i\\dpa{g_j}{x_k}\\right) \\df{x_k,dx_i,dx_j} = \\\\ &amp;= \\sum_{i,j,k=1}^N \\dpa{f_i}{x_k}g_j \\df{x_k,dx_i,dx_j}+ \\sum_{i,j,k=1}^N f_i\\dpa{g_j}{x_k}\\df{x_k,dx_i,dx_j} \\end{align*}\\] Ahora tratamos de encontrar \\(ω\\) y \\(β\\) en ese engendro para volver a una expresión más agradable: \\[\\begin{align*} \\dif (\\omega \\y \\beta) &amp;= \\sum_{i,j,k=1}^N \\dpa{f_i}{x_k}g_j \\df{x_k,x_i,x_j} + \\sum_{i,j,k=1}^N f_i\\dpa{g_j}{x_k}\\df{x_k,dx_i,dx_j} \\\\ &amp;= \\sum_{i,j,k=1}^N \\dpa{f_i}{x_k}\\df{x_k,x_i}\\y(g_j\\df x_j) + \\sum_{i,j,k=1}^N\\dpa{g_j}{x_k}\\df{x_k} \\y f_i \\df{x_i,dx_j} = \\\\ &amp;= \\sum_{i} \\left(\\sum_k \\dpa{f_i}{x_k}\\df{x_k}\\right) \\y \\dif x_i \\y \\left(\\sum_j g_j\\dif x_j\\right) \\\\ &amp;\\qquad + \\sum_{j} \\left(\\sum_k \\dpa{g_j}{x_k}\\df{x_k}\\right) \\y \\left(\\sum_i f_i\\dif x_i\\right) \\y \\dif x_j = \\\\ &amp;= \\sum_i \\df{f_i,x_i} \\y β + \\sum_j \\df{g_j} \\y ω \\y \\dif x_j = \\\\ &amp;= \\left(\\sum_i \\df{f_i,x_i}\\right) \\y β - ω \\y \\left(\\sum_j \\df{g_j,x_j}\\right) = \\\\ &amp;= \\dif ω \\y β - ω \\y \\dif β \\end{align*}\\] Entonces, si \\(\\omega,\\beta\\) son 1-formas tenemos que \\(\\dif(\\omega \\y \\beta) = \\dif\\omega \\y \\beta - \\omega \\y\\dif\\beta\\). Si por el contrario tuviésemos que \\(\\omega\\) es una k-forma y \\(\\beta\\) una s-forma, repitiendo las cuentas de nuevo llegaríamos a \\[\\dif \\omega \\y \\beta + (-1)^k \\omega \\y \\dif\\beta\\] Sea \\(\\omega\\) k-forma con coeficientes \\(C^2\\). Entonces, \\[ \\dif (\\dif ω) = 0 \\] Resumiendo las propiedades del diferencial: Partiendo de un campo en \\(\\real^3\\), podemos interpretarlo como una 1-forma o como una 2-forma. Queremos saber cómo llevar k-formas de \\(\\real^N\\) a \\(\\real^M\\). Partimos de la base de que existe una transformación \\(\\appl{T}{ℝ^N}{ℝ^M}\\) tal que \\(T(s)=x\\), y buscamos otra aplicación \\(\\appl{\\pb}{\\real^M}{\\real^N}\\). En caso de 0-formas, el es lo mismo que la composición. Es decir, dada una función \\(\\appl{f}{ℝ^M}{ℝ}\\), \\(\\pb f = f \\circ T\\). Basándonos en esta misma idea sobre las funciones escalares, vamos a tratar de hallar el pullback para las formas diferenciales. Supongamos que tenemos una k-forma \\(\\omega\\) en \\(\\real^M\\). Queremos construir \\(T^{\\ast}\\) en términos de \\(T\\) y \\(\\omega\\). Partimos de \\(\\gor{s} \\in \\real^N\\) (el punto donde se evalúa la k-forma), y \\(\\gv_1,\\gv_2,..,\\gv_k\\) los vectores en \\(\\real^N\\) sobre los que actúa \\(ω\\). Entonces \\[ (T^{\\ast}\\omega)(\\gor{s}) [\\gv_1,\\dotsc,\\gv_k] = \\omega(\\underbrace{T(s)}_{\\in\\real^M}) [\\underbrace{DT(s)\\gv_j}_{\\in\\real^M}] \\] Es decir, transformamos el punto en el que se evalúa \\(ω\\), que pasa a ser \\(T(\\gor{s})\\) en lugar de \\(\\gor{s}\\), y los vectores sobre los que actúa también los “movemos” usando la matriz diferencial. Vemos que \\[\\omega(T(s)) [DT(s)\\gv]\\equiv \\sum f_i(T(s))\\dif x_i[DT(s)\\gv]\\] \\[DT(s) = \\begin{pmatrix} \\dpa{T_1}{s_1}(s) &amp; ... &amp; \\dpa{T_1}{s_N}\\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\dpa{T_M}{s_1} &amp; \\dots &amp; \\dpa{T_M}{s_N} \\end{pmatrix} \\cdot \\begin{pmatrix} v_1\\\\ \\downarrow\\\\ v_N \\end{pmatrix}\\] ¿Que significa \\(\\dif x_i[DT(s)\\gv]\\)? Vamos a ver que pasa con el producto de una de las filas de la matriz. \\[\\df{x_i} [DT(s)\\gv] = \\dpa{T_i}{s_1}v_1 + ... + \\dpa{T_i}{s_N}v_n\\] Podemos darnos cuenta de que \\(v_1 = \\dif s_1[\\gv]\\). Con esto tenemos: \\[\\df{x_i} [DT(s)\\gv] = \\left(\\dpa{T_i}{s_1}\\dif s_1 + ... + \\dpa{T_i}{s_N}\\dif s_N \\right)[\\gv] = \\dif T_i [\\gv] \\] y por lo tanto \\[ (\\pb ω)(\\gor{s})[\\sample[\\gor{v}][k]] = \\omega(T(s)) [DT(s)\\gv] = \\sum f_i (T(s)) \\dif T_i [\\gv] \\] Sea \\(f(x)\\dif x_i\\) una 1-forma de la que queremos calcular el \\[T^{\\ast}(f\\dif x_i)(s)[v] = f(T(s)) \\dif x_1[DT(s)\\gv]\\] Supongamos \\(f\\equiv 1\\) \\[T^{\\ast}(fdx_i)(s)[v] = \\dif x_1[DT(s)\\gv] = \\dif T_i[\\gv]\\] \\(T^{\\ast}\\dif x_i = \\dif T_i\\). Sea \\(\\omega = \\sum_i f_i \\dif x_i\\) una 1-forma de la que queremos calcular el \\[ (T^{\\ast} \\omega )(s)[\\gv]= \\sum_i f_i(T(s))\\dif x_i [DT(s)\\gv] = \\sum_i f_i(T(s)) \\dif T_i [\\gv ] = T^{\\ast} (\\sum_i f_i \\dif x_i ) = \\sum_i f_i \\circ T \\dif T_i \\] \\(T^{\\ast} (\\sum_i f_i\\dif x_i) = \\sum_i (f_i \\circ T) \\dif T_i\\) ¿Cómo se comporta con el producto exterior? Vamos a trabajar con \\(f\\equiv 1\\). \\[\\begin{gather*} T^{\\ast}(\\df{x_i,x_j} [\\gu,\\gv] = \\df{x_i,x_j} \\left[DT(s)[\\gu], DT(s)[\\gv]\\right] = \\\\ = \\left|\\begin{matrix} \\dif x_i[DT(s)\\gu] &amp; \\dif x_j[DT(s)\\gu] \\\\ \\dif x_i[DT(s)\\gv] &amp; \\dif x_j[DT(s)\\gv] \\end{matrix}\\right| = \\left| \\begin{matrix} \\dif T_i[\\gu] &amp; \\dif T_j[\\gu]\\\\ \\dif T_i[\\gv] &amp; \\dif T_j[\\gv] \\end{matrix}\\right| \\stackrel{(1)}{=} \\df{T_i,T_j} [\\gu,\\gv] \\end{gather*}\\] (1): Por las propiedades del producto exterior de 1-formas. \\(T^{\\ast} (dx_i \\y dx_j) = dT_i \\y dT_j\\). ¿Qué pasa cuando tenemos el producto de 2-formas generadas? \\[\\omega = \\sum f_idx_i\\,;\\,\\beta=\\sum g_jdx_j\\] Vamos con el producto exterior. \\[\\begin{gather*} T^{\\ast} (\\omega \\y \\beta) (s) [\\gu,\\gv] = \\sum_{i,j} f_i(T(s))g_j(T(s)) \\underbrace{\\df{x_i,x_j} [DT(s)\\gu, DT(s),\\gv]}_{\\text{Calculado justo arriba}}\\\\ = \\sum_{i,j} (f_i\\circ T) = \\dotsb \\\\ = \\left(\\sum (f_i\\circ T)\\dif T_i\\right)\\y\\left(\\sum(g_j \\circ T) \\dif T_j\\right) = T^{\\ast}\\omega \\y T^{\\ast}\\beta \\end{gather*}\\] \\(T^{\\ast}(\\omega \\y \\beta) = T^{\\ast}\\omega \\y T^{\\ast}\\beta\\). Esto es válido para multiíndices \\(I\\), es decir, para \\(\\omega\\) k-forma y \\(\\beta\\) s-forma. Una vez visto cómo se comporta el respecto del producto exterior vamos a ver como se comporta con respecto de la diferencial exterior. \\[\\begin{gather*} \\dif (T^{\\ast} \\omega) = \\dif \\left(\\sum_i f_i(T(s)) \\dif x_i [DT(s)\\gv]\\right) = \\dif\\left(\\sum (f_i \\circ T)(s) \\dif T_i[\\gv]\\right)\\\\ = \\sum_i \\dif(f_i\\circ T)\\y dT_i \\end{gather*}\\] Vamos a ver qué significa \\(\\dif(f_i\\circ T)\\): \\[\\begin{gather*} \\dif (f_i\\circ T) = \\sum_k \\dpa{f_i\\circ T}{s_k} \\dif s_k \\\\ \\dpa{f_i\\circ T}{s_k} = \\sum_j \\dpa{f_i}{x_j}(T(s))\\cdot \\dpa{x_j}{s_k} \\end{gather*}\\] Donde \\(x_j = T_j(s)\\). Juntando todo tenemos: \\[ \\dif (T^{\\ast} \\omega) = \\sum_{i,j,k} \\dpa{f_i}{x_j}(T(s)) \\dpa{T_j(s)}{s_k} \\df{s_k,T_i} = \\sum_{i,j} \\dpa{f_i}{s_j}(T(s)) \\df{T_j,T_i} = T^{\\ast}(d(\\sum f_i \\dif x_i)) \\] \\(\\dif (T^{\\ast}\\omega) = T^{\\ast}(\\dif \\omega)\\) El cambio a coordenadas polares \\((\\rho,\\theta)\\) se define por la siguiente transformación: \\[T(\\rho,\\theta) =\\left( T_1(\\rho,\\theta),T_2(\\rho,\\theta)\\right) = (\\rho \\cos\\theta,\\rho \\sen\\theta)\\] Vamos a calcular los pull-backs: \\[\\begin{gather*} T^{\\ast}(\\dif x) = \\dif T_1 = \\dpa{T_1}{ρ}\\dif ρ + \\dpa{T_1}{θ}\\dif θ = \\cos θ \\dif ρ - ρ \\sen θ \\dif θ \\\\ T^{\\ast}(\\dif y) = \\dif T_2 = \\dpa{T_2}{ρ}\\dif ρ + \\dpa{T_2}{θ}\\dif θ = \\sen θ \\dif ρ + ρ \\cos θ \\dif θ \\end{gather*}\\] Juntando ahora lo que tenemos: \\[\\begin{align*} \\pb(\\df{x,y}) &amp;= (\\pb \\dif x) \\y (\\pb \\dif y) = \\\\ &amp;= (\\cos θ \\dif ρ - ρ \\sen θ \\dif θ) \\y (\\sen θ \\dif ρ + ρ \\cos θ \\dif θ) = \\\\ &amp;= ρ \\cos^2 θ \\df{ρ,θ} - ρ \\sen^2 θ \\df{θ,ρ} = \\\\ &amp;= \\left(ρ \\cos^2 θ + ρ \\sen^2 θ\\right) \\df{ρ,θ} = \\\\ &amp;= ρ \\df{ρ,θ} \\end{align*}\\paragraph{Propiedades fundamentales de la operación}\\] Al final, querremos integrar k-formas en \\(ℝ^N\\) sobre variedades de dimensión \\(k\\). Vamos a partir de un conjunto Ω abierto de \\(\\real^N\\). Sea ω una n-forma definida en un entorno de Ω, es decir \\(\\omega = f(\\gx) \\underbrace{\\dfl{x_1}{x_n}}_{\\text{Elemento de volumen}}\\). Definimos la integral como sigue: Supongamos que tenemos una \\(\\appl{\\Phi}{\\real^K}{\\real^N}\\), tal que \\(\\Phi(\\gor{s}) = \\gx\\). Para que lo de la derecha sea una variedad tenemos que \\(\\Phi\\) tiene que ser regular, homeomorfismo y rango máximo. Supongamos \\(\\omega \\in \\real^N\\), con \\(\\omega\\) una x-forma. ¿Qué pasaría si queremos integrar \\(T^{\\ast}\\omega\\)? Si queremos integrar \\(\\pb{\\omega}\\) en \\(\\real^k, \\omega\\) tiene que ser una k-forma para poder aplicar la definición. Después de haber visto los casos específicos, vamos a ver cómo integrar, de forma genérica, una forma diferencial sobre una variedad diferencial. Sea \\(M\\) una variedad de dimensión k en \\(\\real^N\\). Supongamos que \\((D,\\Phi)\\) es una carta local, es decir: \\(\\appl{\\Phi}{D\\subset\\real^K}{\\real^N}, \\Phi(D)\\subset M\\), siendo Φ una parametrización (\\(\\Phi(\\gor{t}) = \\gx\\)). Sea ω una k-forma definida en \\(ℝ^N\\): \\[\\omega = \\sum_I f_I\\dif x_I; \\quad I=\\{i_1,i_2,...,i_k\\}\\] Por definición: \\[\\begin{equation} \\int_{\\Phi(D)} \\omega = \\int_D \\Phi^{\\ast}\\omega \\label{eqIntFDifVar} \\end{equation}\\] El pull-back nos va a dar una k-forma definida en \\(\\real^k\\). \\[ \\Phi^{\\ast}\\omega =g(\\gor{t})\\dfl{t_1}{t_k} \\] Aplicando esto: \\[ \\int_{\\Phi(d) \\omega} = \\int_D g(\\gor{t})\\dfl{t_1}{t_k} \\] Vamos a identificar la función \\(g\\) remangándonos y haciendo cuentas: \\[ \\Phi^{\\ast} \\omega = \\sum_I f_I\\circ\\Phi \\dif \\Phi_I = \\] Vamos a fijarnos en \\(\\dif \\Phi_I = \\df{\\Phi_{i_1},...,\\Phi_{i_k}}\\), que va a actuar sobre k-vectores, es decir: \\[ \\dif \\Phi_I[\\gv_1,...,\\gv_k] = \\dfl{\\Phi_{i_1}}{\\Phi_{i_k}} [\\gv_1,\\dotsc,\\gv_k] = \\det \\begin{pmatrix} \\dif \\Phi_{i_1}[\\gv_1] &amp; \\cdots &amp; \\dif \\Phi_{i_1}[\\gv_k] \\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ \\dif \\Phi_{i_k}[\\gv_1] &amp; \\cdots &amp; \\dif \\Phi_{i_k}[\\gv_k] \\\\ \\end{pmatrix} \\] Vamos a desarrollar uno de los elementos de la matriz (escribiendo \\(\\gw\\) para generalizar a cualquiera de los vectores sobre los que actúa): \\[ \\dif \\Phi_{i_1}[\\gw] = \\sum_{j=1}^k \\left(\\dpa{\\Phi_{i_1}}{t_j} \\dif t_j\\right)[\\gw] = \\sum_{j=1}^k \\dpa{\\Phi_{i_1}}{t_j}w_j = \\pesc{\\grad \\Phi_{i_1},\\gw} \\] Aplicando esto: \\[\\begin{gather*} \\dif \\Phi_I[\\gv_1,...,\\gv_k] = \\det \\begin{pmatrix} \\pesc{\\grad \\Phi_{i_1},\\gv_1} &amp; \\cdots &amp; \\pesc{\\grad \\Phi_{i_1},\\gv_k}\\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ \\pesc{\\grad \\Phi_{i_k},\\gv_1} &amp; \\cdots &amp; \\pesc{\\grad \\Phi_{i_k},\\gv_k} \\\\ \\end{pmatrix} = \\det \\left(\\begin{pmatrix} \\grad \\Phi_{i_1} \\rightarrow \\\\ \\cdots \\\\ \\grad \\Phi_{i_k} \\rightarrow \\end{pmatrix} \\cdot \\begin{pmatrix} \\gv_1 &amp; \\cdots &amp; \\gv_k\\\\ \\downarrow &amp; \\cdots &amp; \\downarrow \\end{pmatrix}\\right) = \\\\ = \\det\\begin{pmatrix} \\grad \\Phi_{i_1} \\longrightarrow \\\\ \\cdots \\\\ \\grad \\Phi_{i_k} \\longrightarrow \\end{pmatrix} \\cdot \\det \\begin{pmatrix} \\gv_1 &amp; \\cdots &amp; \\gv_k \\\\ \\downarrow &amp; \\cdots &amp; \\downarrow \\end{pmatrix} \\stackrel{(1)}{=} \\det\\begin{pmatrix} \\grad \\Phi_{i_1} \\longrightarrow \\\\ \\cdots \\\\ \\grad \\Phi_{i_k} \\longrightarrow \\end{pmatrix} \\dfl{t_1}{t_k}[\\gv_1,\\dotsc,\\gv_k] \\end{gather*}\\] (1): Aplicamos que \\(\\dif t_1(\\gv)\\) es la primera coordenada del vector \\(\\gv\\). Un paso intermedio es \\[\\det \\begin{pmatrix} \\dif t_1(\\gv_1) &amp; \\cdots &amp; \\dif t_1(\\gv_k)\\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ \\dif t_k(\\gv_1) &amp; \\cdots &amp; \\dif t_k(\\gv_k) \\end{pmatrix} \\] \\[ \\Phi^{\\ast} \\omega = \\sum_I f_I\\circ\\Phi \\dif \\Phi_I = \\overbrace{\\sum_I f_i\\circ\\Phi \\det\\begin{pmatrix} \\grad \\Phi_{i_1} \\longrightarrow \\\\ \\cdots \\\\ \\grad \\Phi_{i_k} \\longrightarrow \\end{pmatrix}}^{\\text{Esta es la g que buscamos}} \\dfl{t_1}{t_k} \\] Aplicando a una integral: \\[\\begin{equation} \\int_{\\Phi(D)} \\omega = \\int_D \\sum_I f_i\\circ\\Phi \\det \\begin{pmatrix} \\grad \\Phi_{i_1} \\longrightarrow \\\\ \\cdots\\\\ \\grad \\Phi_{i_k} \\longrightarrow \\end{pmatrix}\\id{t_1,\\dotsb,t_k} \\label{eqPbIntFDif} \\end{equation}\\] Donde la matriz enorme sería el equivalente al cambio en la medida cuando hacíamos un cambio de coordenadas. Para entender los teoremas de Green, de divergencia (o Gauss) y Stokes en términos de las formas diferenciales, vamos a empezar con el caso básico: el cubo unidad. En \\(ℝ^2\\), el cubo unidad es \\(\\mathcal{Q} = [0,1]\\x[0,1]\\). Para calcular la integral sobre la frontera, tenemos que integrar sobre cada una de las aristas: \\[\\begin{array}{cc} I_{11} =\\{(1,y),y\\in[0,1]\\}&amp;\\text{ Orientación: }\\, +\\\\ I_{21} =\\{(x,1),x\\in[0,1]\\}&amp;\\text{ Orientación: }\\, -\\\\ I_{10} =\\{(0,y),y\\in[0,1]\\}&amp;\\text{ Orientación: }\\, -\\\\ I_{20} =\\{(x,0),x\\in[0,1]\\}&amp;\\text{ Orientación: }\\, +\\\\ \\end{array} \\] Para nombrar las aristas usamos la notación \\(I_{ij}\\), donde \\(i\\) es la variable fija (\\(1=x\\),\\(2=y\\)) y \\(j\\) el valor que toma la variable fija. La orientación está calculada con la regla de la mano izquierda. Me coloco en la frontera mirando hacia la dirección en la que nos movemos. Si la mano izquierda estirada apunta hacia el interior, la orientación es positiva. Si apunta hacia fuera, la orientación es negativa. Sea \\(\\omega = f(x,y)\\dif x + g(x,y)\\dif y\\) la forma diferencial que queremos integrar, cuyo diferencial es \\[\\dif \\omega= \\dpa{f}{y}\\df{y,x} + \\dpa{g}{x}\\df{x,y} = \\left(\\dpa{g}{x} - \\dpa{f}{y}\\right) \\df{x,y}\\] Consideramos \\(\\mathcal{C}^+\\) como la frontera de \\(\\mathcal{Q}\\) orientada positivamente. Vamos a intentar calcular la integral de ω sobre esa frontera. \\[\\begin{align*} \\int_{C^+} \\omega &amp;= \\int_{I_11}\\omega\\, - \\int_{I_20}\\omega\\, - \\int_{I_21}\\omega\\, + \\int_{I_10}\\omega = \\\\ &amp;= \\int_0^1 g(1,y)\\dif y + \\int_0^1 f(x,0)\\dif x - \\int_0^1f(x,1)\\dif x - \\int_0^1 g(0,y)\\dif y = \\\\ &amp;= \\int_0^1 \\left(g(1,y)-g(0,y)\\right) \\dif y - \\int_0^1 \\left( f(x,1)-f(x,0)\\right) \\dif x = \\\\ &amp;= \\int_0^1\\int_0^1 \\dpa{g}{x}(x,y)\\id{x,y} - \\int_0^1\\int_0^1 \\dpa{f}{x}(x,y)\\id{y,x} = \\\\ &amp;= \\iint\\limits_Q\\left( \\dpa{g}{x} - \\dpa{f}{y}\\right)\\id{x,y} = \\\\ &amp;= \\iint\\limits_Q \\dif \\omega \\end{align*}\\] Operando, hemos llegado a que \\[ \\int_{C^{+}} \\omega = \\iint\\limits_Q \\dif \\omega \\] Esto escrito en términos de cálculo II es: \\[ \\int_{C^{+}}(P,Q) = \\iint\\limits_Q \\dpa{Q}{x} - \\dpa{P}{y} \\], que es el . Ahora vamos a hacer algo lo mismo en \\(\\real^3\\), sea \\(Q=[0,1]\\x[0,1]\\x[0,1]\\), trabajando con la normal exterior para las orientaciones. Vamos a distinguir las caras con la notación anterior: \\[\\begin{array}{cc} I_{10} = \\{(0,y,z), y\\in[0,1],z\\in[0,1]\\} &amp; \\text{ Orientación: -} \\\\ I_{11} = \\{(1,y,z), y\\in[0,1],z\\in[0,1]\\} &amp; \\text{ Orientación: +} \\\\ I_{20} = \\{(x,0,z), x\\in[0,1],z\\in[0,1]\\} &amp; \\text{ Orientación: +} \\\\ I_{21} = \\{(x,1,z), x\\in[0,1],z\\in[0,1]\\} &amp; \\text{ Orientación: -} \\\\ I_{30} = \\{(x,y,0), x\\in[0,1],y\\in[0,1]\\} &amp; \\text{ Orientación: -} \\\\ I_{31} = \\{(x,y,1), x\\in[0,1],y\\in[0,1]\\} &amp; \\text{ Orientación: +} \\\\ \\end{array} \\] Las orientaciones están calculadas (según el primer caso) \\[\\left.\\begin{array}{cc} T_y = (0,1,0)\\\\T_z=(0,0,1)\\end{array}\\right\\}\\implies T_y\\x T_z = (1,0,0)\\] Mirando en el dibujo, identificamos que la cara en la que estamos trabajando (en este caso la de detrás) y comprobamos que apunta hacia dentro del cubo (dirección contraria a la normal exterior), y concluimos orientación negativa. Repitiendo el proceso llegamos a la conclusión de: \\[ \\begin{array}{cc} T_y\\x T_z &amp;= (0,0,1)\\\\ T_x\\x T_z &amp;= (0,-1,0)\\\\ T_x\\x T_y &amp;= (1,0,0) \\end{array}\\] Si la suma de los subíndices es par, la orientación es positiva. Si por el contrario es impar, es negativa. Detrás de esta idea hay un teorema que no vamos a ver. Además hay que tener cuidado con el orden en el que se hacen las cosas y se escriben los vectores. Trabajamos con nuestra forma diferencial asociada a un campo \\(\\vf\\): \\[\\omega = F_1(x,y,z)\\df{y,z}+F_2(x,y,z)\\df{y,x}+F_3(x,y,z)\\df{x,y}\\] cuyo diferencial es \\[\\begin{align*} \\dif \\omega &amp;= \\dpa{F_1}{x}\\df{x,y,z} + \\dpa{F_2}{y}\\df{y,x,z} + \\dpa{F_3}{z}\\df{z,x,y} \\\\ &amp;= \\left(\\dpa{F_1}{x}+\\dpa{F_2}{y} + \\dpa{F_3}{z}\\right) \\df{x,y,z} \\end{align*}\\] Vamos a calcular \\[\\int_{dQ^+} \\omega\\] siendo \\(dQ^+\\) la frontera del cubo unidad. \\[\\int_{Q^+} \\omega =\\underbrace{ -\\int_{I_{10}} \\omega + \\int_{I_{11}} \\omega}_{(1)} + \\int_{I_{20}} \\omega - \\int_{I_{21}} \\omega -\\int_{I_{30}} \\omega + \\int_{I_{31}} \\omega\\] Operamos (1) por separado. Aplicando () y la ecuación de la integral de una forma diferencial sobre una superficie () tenemos que \\[\\begin{gather*} -\\int_{I_{10}} \\omega + \\int_{I_{11}} \\omega = \\int_0^1\\int_0^1\\Phi^{\\ast}_{11}\\omega - \\int_0^1\\int_0^1 \\Phi^{\\ast}_{10} \\omega = \\\\ = \\int_0^1\\int_0^1 \\pesc{\\overrightarrow{F}\\circ \\Phi_{11},(1,0,0)}\\id{y,z} -\\int_0^1\\int_0^1 \\pesc{\\overrightarrow{F}\\circ \\Phi_{10},(1,0,0)}\\id{y,z}= \\\\ = \\int_0^1\\int_0^1 F_1(\\Phi_{11}(y,z))-F_1(\\Phi_{10}(y,z))dydz = \\iiint\\limits_Q \\dpa{F_1}{x}\\id{x,y,z} \\end{gather*}\\] Aplicando las mismas cuentas con las que faltan llegamos al para el cubo. \\[\\int_{dQ^+} \\omega = \\int_{Q}\\dif \\omega\\] Las ideas sobre formas diferenciales se traducen en \\(\\real^2\\) en el Teorema de Green y en \\(\\real^3\\) en el Teorema de la divergencia. ¿Dónde queda el Teorema de Stokes? Después de unos ejemplos de aplicación de los teoremas vamos a verlo. Sea \\(\\omega\\) 1-forma en \\(\\real^2\\), y \\(D\\) un conjunto cerrado con frontera orientable. Entonces \\[ \\int_{\\partial D^+} P\\dif x+Q\\dif y = \\iint_D \\dif(P\\dif x+Q\\dif y) = \\int\\int_D \\dpa{Q}{x} - \\dpa{Q}{y}\\id{x,y} \\] Podemos elegir \\((P,Q)\\) de tal modo que \\(\\displaystyle \\dpa{Q}{x}-\\dpa{Q}{y} = 1\\). Entonces el área de \\(D\\) sería \\(\\displaystyle\\int_{\\partial D^+} (P,Q)d\\sigma\\). Podemos aplicar esta idea para hallar el área de la hoja folium de Descartes (imagen ), cuya ecuación es \\[x^3+y^3 = mxy\\] Vamos a parametrizarla, siguiendo la indicación: \\(t = \\frac{y}{x}\\). Se deja como ejercicio para el lector llegar a la fórmula: \\[\\begin{gather*} x=\\frac{mt}{1+t^3}\\\\ y= xt = \\frac{mt^2}{1+t^3} \\end{gather*}\\] Quedando por definir que valores toman los parámetros. En este caso es \\((0,\\infty)\\). Estos valores parametrizan la región cerrada. Podríamos plantearnos para qué valores de \\(t\\) que recorren las ramas que se van a infinito. En \\(t=-1\\), se va a infinito, entonces una de las ramas será \\(t\\in(-1,0)\\) y la otra será \\(t\\in(-\\infty,-1)\\). ¿Que orientación nos da esta parametrización? La idea es ver el vector tangente en el 0. Si es horizontal empezaremos por la rama de abajo. Si es vertical, empezaremos por la rama de arriba \\[\\sigma&#39;(t) = \\left(\\frac{m(1+t^3)-3mt^3}{(1+t^3)^2},\\frac{2mt(1+t^3) - 3mt^4}{(1+t^3)^2}\\right)\\] Podemos comprobar que \\(\\sigma&#39;(t) \\convs[][t][0^+] (m,0)\\). Además, \\(\\sigma&#39;(t) \\convs[][t](0,m)\\), quedando una orientación positiva. \\[ A(D) = \\int_{\\partial D^+} (0,x)d\\sigma = \\int_0^{+\\infty} \\pesc{\\left(0,\\frac{mt}{1+t^3}\\right), \\left(\\ast,\\frac{2mt(1+t^3)-3mt^4}{(1+t^3)^2}\\right)}dt = ... \\] Wolfram dice que el resultado de la integral es \\(\\frac{m^2}{6}\\) y que el área encerrada por el folium de Descartes es también \\(\\frac{m^2}{6}\\) lo que nos hace pensar que está bien planteado y bien resuelto el problema. Hacer lo mismo con la curva \\(x^4+y^4=4xy\\). El área de la hoja contenida en el primer cuadrante. % El teorema decía: \\[ \\int_{\\Gamma^+}\\overrightarrow{F}d\\sigma = \\int \\int_{S^+} \\rot\\overrightarrow{F} dS \\] Siendo \\(S\\) la superficie, \\(\\Gamma\\) la ¿frontera?, tomando la orientación positiva con la normal exterior. \\[ \\left.\\begin{array}{cc} z=x^2+y^2\\\\ z=mx \\end{array} \\right\\} \\equiv \\Gamma \\] Queremos calcular \\(\\displaystyle \\int_{\\Gamma}y \\dif z\\) Esto es lo mismo que calcular la integral del campo \\((0,0,y)\\). El primer paso es \\(\\Gamma\\) Tenemos que la proyección en el plano xy es \\[mx=x^2+y^2 \\equiv \\left(x-\\frac{m}{2}\\right)^2 + y^2 = \\frac{m^2}{4}\\] Viendo los cuadrados lo lógico es pensar en utilizar polares. Llamando \\(x=rsen(\\theta),y=rsen(\\theta)\\) tenemos: \\[\\sigma(\\theta) = \\left(mcos^2(\\theta),mcos(\\theta)sen(\\theta),m^2cos^2(\\theta)\\right)\\,\\,\\,\\theta\\in\\left(\\frac{-\\pi}{2},\\frac{\\pi}{2}\\right)\\] Calculamos el vector tangente para ver en que orientación recorre la curva esta parametrización: \\[\\sigma&#39;(\\theta) = ()\\] \\[\\sigma&#39;(0) = (0,m,0)\\] Suponemos (porque no me lo dicen, que \\(m&gt;0\\)) y nos da la orientación. Como en el enunciado no nos hablan de hacerlo con ninguna orientación, la integral que calculemos será de acuerdo con esta orientación. Vamos con la integral: \\[\\int_{\\Gamma}(0,0,y) d\\sigma =\\int_{\\frac{-\\pi}{2}}^{\\frac{\\pi}{2}}\\pesc{\\underbrace{\\left(0,0,mcos(\\theta)sen(\\theta)\\right)}_{\\overrightarrow{F}(\\sigma(\\theta))},\\ast} d\\theta\\] Como camino alternativo a la fórmula, podemos aplicar el teorema: \\[ = \\int\\int_{D^+} rot(0,0,y)dS\\] Siendo el vector normal el que tenga la tercera componente positiva (razonando geométricamente). Calculamos el rotacional del campo:\\(rot\\overrightarrow{F} =\\left|\\begin{matrix} i&amp;j&amp;k\\\\dx&amp;dy&amp;dz\\\\0&amp;0&amp;y \\end{matrix}\\right| = (1,0,0)\\). Utilizamos la parametrización: \\(S = (x,y,mx), x,y\\in C\\) \\[ = \\int\\int_D^+ rot(0,0,y)dS = \\int \\int_C \\pesc{(1,0,0) ,T_x\\x T_y} dxdy= (1) =\\] \\[ \\int \\int_C \\pesc{(1,0,0),(-m,0,1)} = \\int\\int -m dxdy = -m \\cdot \\, Area (C) = -m\\frac{m^2}{4}\\pi \\] $(1): T_x = (1,0,m); T_y = (0,1,0) $. Otra cosa aplicada es que el vector normal \\((-m,0,1)\\) como la tercera componente es positiva, tenemos que esta parametrización induce la orientación positiva. Partimos, igual que en la sección anterior, del cubo unidad en \\(ℝ^n\\). Contamos además con la aplicación \\(\\appl{\\Phi}{Q}{\\Phi(Q)\\subset\\real^n}\\) que “deforma” ese cubo. Sea \\(\\omega\\) una k-forma en \\(\\real^n\\). Queremos calcular la integral de su diferencial en \\(Φ(Q)\\). \\[ \\int_{\\Phi(Q)} \\dif \\omega = \\int_Q \\Phi^{\\ast}\\dif\\omega = \\int_Q \\dif(\\Phi^{\\ast}\\omega) = \\int_{∂Q^{+}} \\Phi^{\\ast}\\omega \\] Donde \\(∂Q^{+}\\) es la frontera del cubo \\(Q\\) orientada debidamente. El último paso es aplicar el teorema anterior. Sea \\(\\appl{\\sigma}{I}{∂Q}\\). Entonces, \\(\\appl{\\Phi\\circ\\sigma}{I}{\\Gamma}\\), siendo \\(\\Gamma\\) la frontera de \\(\\Phi(Q)\\). Aplicando esto a la integral que estamos calculando: \\[ \\int_{\\Phi(dQ^{+})} \\omega \\equiv \\int{\\Phi\\circ\\sigma(I)} = \\int_I (\\Phi\\circ\\sigma)^{\\ast} \\omega = \\int_I \\sigma^{\\ast}\\left(\\Phi^{\\ast}(\\omega)\\right) = \\int_{\\sigma(I)} \\Phi^{\\ast}\\omega = \\int_{∂Q}\\Phi^{\\ast}\\omega \\] Hemos llegado finalmente a que \\[ \\int_{∂Q^+} \\Phi^{\\ast} \\omega = \\int_{\\Phi(∂Q^+)} \\omega \\] Ahora querríamos dar el siguiente paso: \\[\\int_{\\Phi(∂Q^+)} \\omega = \\int_{∂(\\Phi^{\\ast}(Q))} \\omega \\] Es decir, que la imagen de la frontera sea la frontera de la imagen. Esto no es inmediato: en el cambio a coordenadas polares, por ejemplo, no se cumple a primera vista (ver figura ). Cuando el ángulo (θ) se mantiene fijo y \\(r\\) varía (segmentos azules), la imagen de esa parte de la frontera es un radio de la circunferencia, y cuando \\(r=0\\) y variamos el ángulo (segmento verde) la imagen es un punto. Ahora bien, podemos dividir la región \\(Q\\) en celdas disjuntas, como se puede ver en la figura . De esta forma, las fronteras que sean comunes a varias celdas tendrán orientación incompatible y se anularán al integrar. Esto nos resuelve el problema y podemos enunciar el teorema de Stokes para celdas: Vamos a intentar definir en serio la frontera de objetos en \\(\\real^3\\), que es algo que necesitamos tener realmente muy claro. Hace un tiempo, cuando definíamos una subvariedad, demostramos la existencia de un difeomorfismo Ψ que “aplanaba un trozo” de subvariedad. La frontera de una superficie es el conjunto de los puntos (llamados en clase de Tipo 2) que al aplanar nos quedan en la frontera de un objeto de dimensión 2 (ver imagen ). Una vez aclarado el concepto de frontera, pasamos a demostrar el teorema de Stokes. La versión de este teorema vista en clase de prácticas (que es una versión más práctica) Además, para calcular el potencial se puede utilizar cualquiera de estas 3 fórmulas: \\[\\begin{gather*} \\int_{Γ_1}\\vf=\\int_0^x F_1(t,0,0)\\dif t + \\int_0^y F_2(x,s,0)\\dif s+ \\int_0^z F_3(x,y,r)\\dif r \\\\ \\int_{Γ_2}\\vf = \\int_0^y F_2(0,t,0)\\dif t + \\int_0^z F_3(0,y,s)\\dif s+ \\int_0^x F_1(x,y,r)\\dif r \\\\ \\int_{Γ_3}\\vf = \\int_0^x F_1(t,0,0)\\dif t + \\int_0^z F_3(x,0,s)\\dif s+ \\int_0^y F_1(x,r,z)\\dif r \\\\ \\end{gather*}\\subsection{Ejemplos}\\begin{example} Tenemos un cono de base $D$ y altura $h$, y buscamos integrar el campo $\\vf(x,y,z)= (x,y,z)$. $Ω$ es el espacio del cono, y $∂Ω$, su superficie, se divide en la superficie lateral y la base. \\todo[inline]{Dibujito L1244} Si escribimos el teorema de Gauss, tenemos que \\[ \\iiint_Ω \\dv \\vf \\id{x,y,z} = \\iint_{∂Ω^+} \\vf \\dif S \\] En este caso, tenemos la suerte de que $\\dv \\vf = 3$, y por lo tanto \\[ \\iiint_Ω \\dv \\vf \\id{x,y,z} = 3 \\cdot \\mathrm{Volumen}\\,(Ω) \\] Por lo tanto, para hallar el volumen calculamos la integral sobre la superficie y dividimos entre tres. La idea geométrica es que en la cara lateral, la componente normal de $\\vf$ es 0. Lo vemos fácilmente sabiendo que la recta definida por el vector $(x,y,z)$ y que pasa por el origen (el vértice del cono), tiene exactamente la misma dirección de la generatriz. Entones,\\textbf{ la integral sobre la cara lateral es 0} y por lo tanto podemos ignorarla. Nos centramos sólo en la integral de la base: \\[ \\iint_{\\mathrm{Base}}\\vf \\id S \\] Parametrizar $S$ es sencillo: \\[ S \\equiv \\{ (x,y,h)\\tq (x,y)∈ D \\} \\] Calculamos su vector normal: \\[ \\left.\\begin{matrix} T_x = (1,0,0) \\\\ T_y = (0,1,0) \\end{matrix}\\right\\} T_x × T_y = (0,0,1) \\] y entonces \\[ \\iint_{\\mathrm{Base}}\\vf \\id S = \\iint_D\\pesc{(x,y,h),(0,0,1)} \\id{x,y} = \\iint_D h \\id{x,y} = h \\cdot \\mathrm{Area}\\,(D) \\] Finalmente \\[ \\mathrm{Volumen}\\,(Ω) = \\frac{h \\cdot \\mathrm{Area}\\,(D)}{3} \\] \\end{example}\\begin{example}[Cálculo del campo eléctrico o gravitatorio] La expresión del campo eléctrico es \\[ \\vf(\\vx) = C \\frac{\\vx}{\\norm{\\vx}^3} \\] Calculando las derivadas parciales \\begin{align*} \\dpa{F_1}{x}&amp;=\\frac{y^2+z^2-2x^2}{(x^2+y^2+z^2)^{\\frac{5}{2}}} \\\\ \\dpa{F_2}{y}&amp;=\\frac{x^2+z^2-2y^2}{(x^2+y^2+z^2)^{\\frac{5}{2}}} \\\\ \\dpa{F_3}{z}&amp;=\\frac{x^2+y^2-2z^2}{(x^2+y^2+z^2)^{\\frac{5}{2}}} \\end{align*} nos queda que \\[ \\dv\\vf = 0 \\] Consideramos ahora una bola de radio $R$ centrada en el origen, es decir, $B_R(0,0,0)$. Integramos sobre su superficie: \\[ \\iint\\limits_{∂B_R^+} \\vf \\id{S} = \\iint\\limits_{B_R} \\left(\\frac{x}{R^3},\\frac{y}{R^3},\\frac{z}{R^3} \\right) \\id S = \\frac{1}{R^3} \\iint\\limits_{B_R} (x,y,z)\\id S = \\] Dado que integrar el vector es integrar su componente escalar, la integral nos queda \\[ = \\frac{1}{R^3} \\iint\\limits_{B_R} R\\id{x,y} = \\frac{1}{R^2}\\cdot \\mathrm{Area\\; esfera} = 4π \\] Ahora bien, si no supiésemos ese argumento geométrico, empezaríamos parametrizando la esfera: \\[ Φ(θ,φ) =(R\\cos θ \\sin φ, R\\sin θ\\sin φ, R\\cos φ);\\, θ∈[0,2π], φ∈[0,π] \\] Calculamos los vectores tangentes y el normal: \\[ \\begin{matrix} \\\\ T_θ = \\\\ T_φ= \\end{matrix} \\left| \\begin{matrix} \\vec{i} &amp; \\vec{j} &amp; \\vec{k} \\\\ -R\\sin θ \\sin φ &amp; R\\cos θ \\sin φ &amp; 0 \\\\ R\\cos θ\\cos φ &amp; R\\sin θ \\cos φ &amp; -R\\sin φ \\end{matrix}\\right|: T_θ×T_φ = -R\\sin (R\\cos θ, \\] La normal es interior, así que pagamos con un cambio de signo: \\begin{gather*} \\iint\\limits_{x^2+y^2+z^2 = R^2} \\left(\\frac{x}{R^3},\\frac{y}{R^3},\\frac{z}{R^3} \\right) \\id S = \\\\ - \\int_0^{2π}\\int_0^π\\pesc{\\left(\\frac{R\\cos θ \\sin φ}{R^3},{R\\sin θ \\sin φ}{R^3},{R\\cos φ}{R^3}\\right), (,,)} = \\dotsb \\mathrm{Calculos\\; aqui} \\end{gather*} y al final sale lo mismo que antes pero con cuentas mucho más desagrable. ¿Por qué el teorema de Gauss no funciona? La divergencia es 0 y la superficie es cerrada. Sin embargo, $\\vf$ no es $C^1$ en el origen (no existe en ese punto) así que no podemos aplicarlo. Pero, por otra parte, siempre hemos visto que la integral no se ve afectada por lo que ocurra en un punto. ¿Por qué no funciona el teorema de Gauss sólo por lo que pasa en el origen? En realidad en el origen la divergencia vale $4π$ por la Delta de Dirac (δ). Pero también podemos hacer un apaño. Consideramos un conjunto $Ω$, y una bola $B_ε(0,0,0)$. Entonces \\[ Ω_ε = Ω - B_ε \\] de tal forma que $\\vf∈C^1$ en $Ω_ε$. Aplicando el teorema de Gauss: \\[ 0 = \\iiint\\limits_{Ω_ε}\\dv \\vf \\id{x,y,z} = \\iint\\limits_{∂Ω_ε^+}\\vf \\id{S} \\] La frontera se divide en dos partes: $∂Ω_ε = ∂Ω + ∂B_ε^+$. Entonces \\[ 0 = \\iint\\limits_{∂Ω^+}\\vf \\id S - \\iint\\limits_{∂B_ε^+}\\vf \\id S \\] y por lo tanto, tenemos que \\[ \\iint\\limits_{∂Ω^+}\\vf \\id{S} = 4π\\; ∀Ω \\tq 0 ∈ Ω \\] \\end{example}\\begin{theorem} Dado un campo $\\vf \\in C^1$, se tiene que \\[ \\dv \\vf = 0\\dimplies \\exists \\vg\\in C^2 \\tlq \\rot \\vg = \\vf \\] \\end{theorem}\\begin{proof} Con el lenguaje de las formas diferenciales es muy fácil esta demostración. \\todo[inline]{reescribir bien} $\\vf$ es una 1-forma a la que aplicamos la diferencial exterior y obtenemos una 2-forma, que es su rotacional. Entonces, al volver a hacer la diferencial (para hallar la divergencia) estamos aplicando dos veces el diferencial, y por lo tanto su valor es 0. Sin formas diferenciales tenemos: \\[G=(G_1,G_2,G_3)\\] \\[\\rot G = \\left(\\ast_1,\\ast_2,\\ast_3\\right)\\] \\[\\dv(\\rot G) = \\dpa{}{x}(\\ast_1) + \\dpa{}{y}(\\ast_2) + \\dpa{}{z}(\\ast_3)\\] Por la igualdad de las derivadas cruzadas (toerema de Euler) al ser $G\\in C^2$ se cancela todo. $\\implies$ Buscamos $\\vg$ tal que $(F_1,F_2,F_3) = \\rot \\vg$. Podemos definir el sistema: \\[\\begin{array}{cc} \\dpa{G_3}{y} - \\dpa{G_2}{z} &amp;= F_1\\\\ \\dpa{G_1}{z} - \\dpa{G_3}{x} &amp;= F_2\\\\ \\dpa{G_2}{x} - \\dpa{G_1}{y} &amp;= F_3 \\end{array}\\] Como tenemos varios grados de libertad, supongamos que $G_3\\equiv 0$ \\[\\begin{array}{cc} - \\dpa{G_2}{z} &amp;= F_1\\\\ \\dpa{G_1}{z} - &amp;= F_2\\\\ \\dpa{G_2}{x} - \\dpa{G_1}{y} &amp;= F_3 \\end{array} \\begin{array}{cc} \\longrightarrow &amp; G_2 = -\\int_0^z F_1(x,y,t)dt+A(x,y)\\\\ \\longrightarrow &amp; G_1 = \\int_0^2 F_2(x,y,t)dt + B(x,y)\\\\ \\longrightarrow &amp; F_3 = \\dpa{}{x} \\left\\{-\\int_0^z F_1(x,y,t)dt + A(x,y)\\right\\} - \\dpa{}{y}\\left\\{\\int_0^2 F_2(x,y,t)dt + B(x,y) \\right\\} \\end{array} \\] Vamos a ver que ocurre con $F_3$ \\[-\\dpa{}{x}\\left(\\int_0^z F_1(x,y,t)dt\\right) + \\dpa{A}{x} - \\dpa{}{y}\\int_0^z F_2(x,y,t)dt - \\dpa{B}{y}\\] Por la regularidad de $F_1$ podemos meter dentro la derivada (el argumento que está detrás de esto se ve en teoría de la medida, asíque de momento nos fiamos) \\[-\\left(\\int_0^z \\dpa{}{x}F_1(x,y,t)dt\\right) + \\dpa{A}{x} - \\int_0^z \\dpa{}{y}F_2(x,y,t)dt - \\dpa{B}{y}\\] \\[-\\int_0^z \\left(\\dpa{F_1}{x} + \\dpa{F_2}{y}\\right)(x,y,t)dt + \\dpa{A}{x} - \\dpa{B}{y} \\] Utilizamos que la $\\dv = 0$ que nos dice: \\[\\dpa{F_1}{x} + \\dpa{F_2}{y} + \\dpa{F_3}{z} = 0\\] \\[ = \\int_0^z \\dpa{F_3}{z} (x,y,t)dt + \\dpa{A}{x} - \\dpa{B}{y} = F_3(x,y,z) - F(x,y,z) + \\dpa{A}{x} - \\dpa{B}{y}\\] Hemos llegado a \\[F_3(x,y,z) = F_3(x,y,z) - F(x,y,z) + \\dpa{A}{x} - \\dpa{B}{y}\\] Quedando entonces definidas $A$ y $B$ así: \\[\\dpa{A}{x} - \\dpa{B}{y} = F_3(x,y,0)\\] Y aquí nuevamente tenemos muchas opciones. Tomamos $B=0$ por tomar algo \\[A(x,y) = \\int_0^x F_3(s,y,0)ds + C(y)\\] \\end{proof}\\] 12.1 Integración 12.2 Aplicaciones primitivas 12.3 Cambio de variables 12.4 Formas diferenciales 12.5 Cadenas y símplices 12.6 Teorema de Stoke 12.7 Formas cerradas y formas exactas 12.8 Análisis vectorial "],
["funciones-especiales.html", "Capítulo 13 Funciones especiales 13.1 Series de potencia 13.2 Funciones exponenciales y logarítmicas 13.3 Funciones trigonométricas 13.4 Completitud algebraica del cuerpo de los complejos 13.5 Series de Fourier", " Capítulo 13 Funciones especiales 13.1 Series de potencia 13.2 Funciones exponenciales y logarítmicas 13.3 Funciones trigonométricas 13.4 Completitud algebraica del cuerpo de los complejos 13.5 Series de Fourier "],
["espacios-lineales-normados.html", "Capítulo 14 Espacios lineales normados 14.1 Normas y seminormas 14.2 Completación de un espacio normado 14.3 Series infinitas en espacios normados 14.4 Sumas no ordenadas en espacios normados 14.5 Trnasformaciones lineales acotadas 14.6 Álgebras de Banach", " Capítulo 14 Espacios lineales normados 14.1 Normas y seminormas 14.2 Completación de un espacio normado 14.3 Series infinitas en espacios normados 14.4 Sumas no ordenadas en espacios normados 14.5 Trnasformaciones lineales acotadas 14.6 Álgebras de Banach "],
["espacios-topologicos.html", "Capítulo 15 Espacios topológicos 15.1 Abiertos y cerrados 15.2 Sistemas de entornos 15.3 Bases de entornos 15.4 Topología relativa 15.5 Nets", " Capítulo 15 Espacios topológicos 15.1 Abiertos y cerrados 15.2 Sistemas de entornos 15.3 Bases de entornos 15.4 Topología relativa 15.5 Nets "],
["continuidad-en-espacios-topologicos.html", "Capítulo 16 Continuidad en espacios topológicos 16.1 Propiedades generales 16.2 Topologías iniciales 16.3 Topología producto 16.4 Topología cociente 16.5 Espacio de funciones contínuas 16.6 Conjuntos F-sigma y G-delta", " Capítulo 16 Continuidad en espacios topológicos 16.1 Propiedades generales 16.2 Topologías iniciales 16.3 Topología producto 16.4 Topología cociente 16.5 Espacio de funciones contínuas 16.6 Conjuntos F-sigma y G-delta "],
["espacios-topologicos-normados.html", "Capítulo 17 Espacios topológicos normados 17.1 Lema de Urysohn 17.2 Teorema de extensión de Tietze", " Capítulo 17 Espacios topológicos normados 17.1 Lema de Urysohn 17.2 Teorema de extensión de Tietze "],
["espacios-topologicos-compactos.html", "Capítulo 18 Espacios topológicos compactos 18.1 Convergencia en espacios compactos 18.2 Compacidad del producto cartesiano 18.3 Continuidad y compacidad", " Capítulo 18 Espacios topológicos compactos 18.1 Convergencia en espacios compactos 18.2 Compacidad del producto cartesiano 18.3 Continuidad y compacidad "],
["espacios-metricos-totalmente-acotados.html", "Capítulo 19 Espacios métricos totalmente acotados", " Capítulo 19 Espacios métricos totalmente acotados "],
["equicontinuidad.html", "Capítulo 20 Equicontinuidad", " Capítulo 20 Equicontinuidad "],
["el-teorema-de-stone-weierstrass.html", "Capítulo 21 El teorema de Stone-Weierstrass", " Capítulo 21 El teorema de Stone-Weierstrass "],
["espacios-toplogicos-localmente-compactos.html", "Capítulo 22 Espacios toplógicos localmente compactos 22.1 Propiedades generales 22.2 Funciones a soporte compacto 22.3 Funciones que se anulan al infinito 22.4 Compactificación a un punto", " Capítulo 22 Espacios toplógicos localmente compactos 22.1 Propiedades generales 22.2 Funciones a soporte compacto 22.3 Funciones que se anulan al infinito 22.4 Compactificación a un punto "],
["espacios-de-hilbert.html", "Capítulo 23 Espacios de Hilbert 23.1 Definición y ejemplos 23.2 Ortogonalidad 23.3 Separación de conjuntos convexos 23.4 Bases ortonormadas 23.5 Convergencia débil 23.6 Operadores contínuos y compactos 23.7 Teorema espectral de Hilbert", " Capítulo 23 Espacios de Hilbert 23.1 Definición y ejemplos 23.2 Ortogonalidad 23.3 Separación de conjuntos convexos 23.4 Bases ortonormadas 23.5 Convergencia débil 23.6 Operadores contínuos y compactos 23.7 Teorema espectral de Hilbert "],
["espacio-de-funciones-diferenciables.html", "Capítulo 24 Espacio de funciones diferenciables", " Capítulo 24 Espacio de funciones diferenciables "],
["particiones-de-la-unidad.html", "Capítulo 25 Particiones de la unidad", " Capítulo 25 Particiones de la unidad "],
["conexidad.html", "Capítulo 26 Conexidad", " Capítulo 26 Conexidad En \\(\\real^3\\) tenemos puntos (dimensión 0), curvas (dimensión 1), superficies (dimensión 2) y abiertos (dimensión 3) sobre los que integrar en los que la imaginación resulta bastante útil. Pero… ¿qué pasa en \\(\\real^N\\)? Entonces en \\(\\real^N\\) tenemos objetos de dimensión \\(0,...,N\\) sobre los que vamos a poder definir propiedades, integrales, etc. Repasamos la idea de que en \\(\\real^3\\) podíamos representar una superficie de varias maneras y cómo cálculabamos su plano tangente. Dada una superficie como una gráfica, simplemente construíamos el vector normal al plano derivando parcialmente: \\[ \\vn = \\left( \\dpa{f}{x},\\dpa{f}{y}, 1 \\right) \\] Con una parametrización \\(\\phi\\) \\[\\begin{align*} \\appl{\\phi}{D\\subset\\real^2 &amp;}{\\real^3} \\\\ \\phi(u,v) &amp;= (x(u,v),y(u,v),z(u,v)) \\end{align*}\\] calculamos el plano tangente tomando los dos vectores directores y calculamos el normal al plano: \\[\\begin{gather*} T_u = \\left( \\dpa{x}{u},\\dpa{y}{u},\\dpa{z}{u}\\right)\\\\ T_v = \\left( \\dpa{x}{v},\\dpa{y}{v},\\dpa{z}{v}\\right)\\\\ \\overrightarrow{n} = T_u\\times T_v\\\\ \\end{gather*}\\] Nos podemos encontrar el problema de que \\(\\overrightarrow{n} = \\overrightarrow{0}\\). Para evitar ese caso, estableceremos que el rango de la matriz de las 6 derivadas tiene que ser 2. Supongamos que nos dan una gráfica definida como un conjunto de nivel \\(F(x,y,z) = 0\\) . Para calcular el plano tangente en este caso tenemos que \\(\\overrightarrow{n} = \\nabla F\\). Nos puede ocurrir que \\(F\\) no sea derivable o que \\(\\nabla F = \\overrightarrow{0}\\). Supongamos que sea diferenciable, ¿cómo preveer que puede salir? Para evitarlo, debemos forzar de nuevo que la matriz de las derivadas tenga rango máximo (en este caso 1). Vamos a ver que pasa con las curvas en \\(\\real^3\\) y cómo calcular la recta tangente Vemos claramente que las condiciones para poder calcular superficies tangentes llegan siempre a obligar a que la matriz tenga (ver sección ). A partir de aquí, podemos crear nuestra definición de subvariedad diferenciable: Veamos ejemplos de si algunos objetos son variedades diferenciables o no: \\[\\appl{F}{U\\subset\\real^3}{\\real}\\] \\[\\{(x,y,z) \\tq F(x,y,z) = 0\\}\\] En este caso tenemos \\(N=2, K=1\\). La condición de rango nos dice que \\(\\nabla F(\\gx) \\neq (0,0,0)\\). Esto es, obliga a en cada punto existe un vector normal \\(\\overrightarrow{n}\\), exactamente la misma condición que habíamos visto antes. \\[\\sigma(t) = (x(t),y(t),z(t))\\equiv S_1 \\cap S_2 = \\] \\[= \\{F_1(x,y,z) = 0\\}\\cap \\{F_2(x,y,z) = 0\\}\\] Si tomamos \\(U\\cap \\sigma = \\{(x,y,z)\\in\\real^3 \\tq F_1 = 0 \\y F_2 = 0\\}\\) Siendo \\[\\appl{F}{U\\subset\\real^3}{\\real^2}, N+K=3,K=2\\]\\[F(x,y,z) = (F_1(x,y,z),F_2(x,y,z))\\] Veamos la condición de rango en este caso: \\[rango \\begin{pmatrix} \\dpa{F_1}{x}&amp;\\dpa{F_1}{y}&amp;\\dpa{F_1}{z}\\\\\\dpa{F_2}{x}&amp;\\dpa{F_2}{y}&amp;\\dpa{F_2}{z}\\end{pmatrix}\\] Para que el rango sea máximo, los vectores tienen que ser no paralelos, es decir, \\(S_1, S_2\\) sean transversales, no paralelas. De nuevo, la misma condición que habíamos visto antes. Podemos definir un punto de una manera un tanto rebuscada: \\[\\begin{gather*} \\appl{F}{\\real^K}{\\real^K}\\\\ \\gx \\rightarrow \\gx - \\ga \\end{gather*}\\] En este caso, la subvariedad \\(M\\) es nuestro punto \\(\\ga\\): \\[M = \\{\\gx \\in \\real^K \\tq F(\\gx) = \\gor{0}\\} = \\{\\ga\\}\\] Tenemos $DF = Id $, que tiene rango máximo y por lo tanto \\(\\{\\ga\\}\\) es una subvariedad diferenciable (dimensión 0, codimensión \\(k\\)). \\(\\appl{F}{\\Omega\\subset\\real^N}{0}\\). En este caso tenemos codimensión \\(0\\) y dimensión \\(N\\). \\[\\begin{gather*} \\appl{F}{\\real^2}{\\real}\\\\ F(x,y) = x^3 - y^6 \\\\ M = \\{(x,y) \\tq x^3-y^6 = 0\\} \\\\ DF = (3x^2-6y^5) \\\\ DF(0,0) = (0,0) \\end{gather*}\\] La matriz diferencial tiene rango \\(0\\) en el punto \\((0,0)\\). ¿Quiere esto decir que \\(M\\) no es una subvariedad diferenciable en el 0? No. Lo que quiere decir que no hemos encontrado la función que cumpla las hipótesis. Podemos operar \\[M = \\{x^3=y^6\\} = \\{x = y^2\\} = \\{x-y^2 = 0\\}\\] Y vemos que si tomamos \\(G(x,y) = x-y^2\\), esta función representa la misma subvariedad \\(M\\), y \\(DG(x,y) = (1,2y)\\) tiene rango 1 en el origen. \\[M = \\{(x,y)\\in \\real^2 \\tq x^2-y^2 = 0\\}\\] Definimos una función \\(F\\): \\[\\appl{F}{\\real^2}{\\real}\\] \\[F(x,y) = x^2-y^2\\] \\(DF = (2x,-2y)\\). La condición de rango falla en \\((0,0)\\). Valoramos si este objeto no es una subvariedad o si tendremos que definir una función de una manera más inteligente, tal y como hicimos en el caso anterior. En este caso, vemos que \\(M = \\{ y=x \\cup y = -x \\}\\). No debería ser una subvariedad porque en el \\((0,0)\\) no es derivable (ver figura ). Intentaremos demostrarlo por reducción al absurdo. Supongamos que \\(M\\) es una subvariedad. Entonces, según la definición () existe un \\(U, (0,0) \\in U\\) y una aplicación \\(\\appl{G}{U\\subset\\real^2}{\\real}\\) con \\(U\\cap M = \\{G(x,y) = 0\\}\\). Entonces tendríamos que \\[\\rango DG(x,y) = 1, \\forall(x,y) \\in U \\implies \\rango \\left(\\dpa{G}{x},\\dpa{G}{y}\\right) = 1\\] Ahí tenemos dos casos, o bien que la primera componente no sea \\(0\\) o que sea la segunda la que no es nula. Supongamos primero que \\(\\dpa{G}{x}(0,0) \\neq 0\\). Podemos aplicar el Teorema de la unción implícita (), que nos dice que podemos despejar \\(x = x(y)\\). En este caso: \\(U \\cap M = \\{x(y)^2-y^2 = 0\\}\\). Si fijamos \\(y=\\varepsilon\\), entonces \\(x(\\varepsilon) = \\pm \\varepsilon\\) No es una función, lo que contradice el Teorema de la función implícita, y por lo tanto es imposible que \\(\\dpa{G}{x}(0,0) \\neq 0\\). Análogamente, tampoco puede cumplirse que \\(\\dpa{G}{y}(0,0) \\neq 0\\). Hemos demostrado por lo tanto que no puede existir una función que defina esto como subvariedad diferencial. Este es el ejemplo de que cualquier objeto que tenga autointersección no puede ser subvariedad. \\(M = \\{(x,y) \\in \\real^2 \\tq x^2-y^2 = 0, y\\ge 0\\}\\) Vamos a suponer que existe una función \\(F \\in C^1\\) que representa ese objeto (que viene definido por 2 condiciones). \\(M = \\{F(x,y) = 0\\}\\) para alguna \\(F\\). Condición de rango: \\(\\left(\\dpa{F}{x},\\dpa{F}{y}\\right) \\neq (0,0)\\). Condición de rango: \\(\\left(\\dpa{F}{x},\\dpa{F}{y}\\right) = (0,0), x=y=0\\). Vamos a ver si es subvariedad o no. En este caso intuimos que no debería serlo. Volvemos a demostrar por reducción al absurdo: Supongamos que \\(M\\) es subvariedad, entonces \\(\\exists G(x,y)\\) tal que \\(\\appl{G}{U\\subset\\real^2}{\\real}, U \\cap M = \\{G(x,y) = 0\\}\\) Supongamos \\(\\displaystyle \\dpa{G}{x}\\neq 0\\). Entonces tenemos \\[ M\\cap U = \\{y(x)^2 - y^2 = 0\\} \\] Si fijamos \\(x=\\varepsilon \\implies y(x) = \\abs{\\varepsilon}\\), pero esto quiere decir que \\(G \\notin C^1\\). De forma análoga con la segunda coordenada, vemos que \\(M\\) no es una subvariedad. Nos quitamos el punto conflictivo del caso anterior, el \\((0,0)\\). \\(N = \\{(x,y)\\in \\real^2 \\tq x^2-y^2 = 0, y&gt;0\\}\\) La lógica nos dice que este caso si debería ser subvariedad diferencial: la definición de la función es local, y siempre podremos encontrar un entorno que no incluya el 0, que es el punto problemático. Comprobamos que efectivamente el rango de \\(\\nabla F\\) es máximo \\(\\forall (x,y)\\in \\real^2\\) y por lo tanto \\(M\\) es una subvariedad. Superficie en \\(\\real^3\\) parametrizada: \\(S = \\{\\Phi(u,v) = (x(u,v),y(u,v),z(u,v))\\}\\). A la hora de trabajar con superficies parametrizadas, nos interesaría poder definir una especie de función inversa que nos permita hacer cambios en el plano y llevarlos a la superficie o al reves, pero… ¡tienen dimensiones distintas! La esperanza que nos queda es que la superficie parametrizada tiene dimensión 2, igual que el plano. \\[\\appl{\\sigma}{[0,2\\pi)}{\\real^2}\\] \\[t \\rightarrow \\sigma(t) = (cos(t),sen(t))\\] Inversa: \\(\\Psi(x,y) = t\\) ángulo de la representación en polares. Vamos a estudiar el problema de la continuidad: Tomamos \\(\\{(X_n,Y_n)\\}\\) con \\(x^2+y^2 = 1 \\tlq (x_n,y_n) \\convs (1,0)\\) Si \\(\\Psi\\) es continua, debe ser \\(\\Psi(x_n,y_n) \\convs (1,0) = 0\\). En este caso no es continua porque: Si tomamos \\[\\begin{align*} P_n &amp;= \\left(cos\\left(2\\pi - \\frac{1}{n}\\right), sen\\left(2\\pi - \\frac{1}{n}\\right) \\right)\\\\ P_n &amp;\\convs (1,0)\\\\ \\Psi(P_n) &amp;= 2\\pi - \\frac{1}{n} \\convs 2\\pi\\neq\\Psi(1,0) \\end{align*}\\paragraph{Ejemplo 2}\\] \\[\\appl{f}{(0,1)\\subset\\real}{\\real^2}\\] \\[t \\rightarrow f(t) = (t,g(t)), \\text{continua, con inversa continua}\\] Vamos a definir la inversa: \\[P\\in f(0,1) \\implies P(x,g(x)) \\text{Para algún } x\\in(0,1)\\] \\(P = (x,g(x))\\) \\[\\Psi(P) = t \\in (0,1) \\tlq f(t) = (x,g(x)) \\implies t = x\\] Vamos a estudiar la continuidad: \\[\\{P_n\\} \\subset f(0,1), P_n \\rightarrow P \\in f(0,1)\\] \\[P_i = (x_i,g(x_i)), \\text{ para algún } i \\in (0,1)\\] \\[P_n \\convs P_0 \\dimplies (x_n,g(x_n)) \\rightarrow (x_0,g(x_0)) \\implies x_n (= \\Psi(P_n)) \\rightarrow x_0 (=\\Psi(P_0)) \\] ¿La gráfica de una función es homeomorfismo sobre su imagen? \\[\\appl{\\sigma_3}{(0,4\\pi)}{\\real^2}\\] \\[t \\rightarrow \\sigma(t) = (cos(t),sen(t))\\] No es inyectiva \\(\\implies \\nexists \\Psi\\). \\[\\appl{\\sigma_4)}{(0,2\\pi)}{\\real^2}\\] \\[t \\rightarrow \\sigma(t) = (cos(t),sen(t))\\] La diferencia con el ejemplo 1, es que es cerrado. Hay que percatarse de que la tangente no es inyectiva, entonces… la inversa es algo más complicada que \\(\\arctan\\), así que vamos a definirla a trozos siguiendo el esquema de la figura . \\[\\Psi(x,y) = \\begin{cases} \\arctan\\left(\\frac{y}{x}\\right) &amp; (x,y)\\in 1\\\\ \\arctan\\left(\\frac{y}{x}\\right) + \\pi &amp; (x,y) \\in 2\\\\ \\arctan\\left(\\frac{y}{x}\\right)+2\\pi&amp; (x,y) \\in 3 \\end{cases} \\] Hay que estudiar la continuidad en \\((0,-1)\\) y en \\((0,1)\\). Tomamos \\(\\{P_n\\} \\rightarrow (0,1), P_n \\in S_1\\) Queremos probar que \\(\\Psi(P_n) \\rightarrow \\Psi(0,1) = \\frac{\\pi}{2}\\). Ejercicio para el lector: Sucesiones que se acercan por la derecha o por la izquierda y comprobar que vale lo que tiene que valer. La parametrización es \\[\\sigma_5 (x(t),y(t)) = \\left(\\frac{\\cos(t)}{\\sin^2(t)+1},\\frac{\\sin(t)\\cos(t)}{\\sin^2(t)+1}\\right), t\\in \\left(\\frac{-\\pi}{2},\\frac{3\\pi}{2}\\right)\\] Al origen podemos acercarnos de distintas formas: Sea \\(\\{P_n\\}\\subset \\sigma_5(t) \\rightarrow (0,0)\\). Podemos tomar la sucesión \\(\\{P_n\\} = \\{P_1,P_2,P_3,...\\}\\) cada uno en una región (1,2,3 ciclicamente). \\[\\Psi(P_n) \\begin{cases} \\in 3 &amp; \\text{ si }n \\text{ es múltiplo de }3\\\\ \\in 2 &amp; \\text{ si } n\\equiv 2\\ mod \\ 3\\\\ \\in 1 &amp; \\text{ si } n \\equiv 1\\ mod \\ 3 \\end{cases}\\] Por tanto \\(\\nexists \\displaystyle \\lim_{n\\rightarrow \\infty} \\Psi(P_n)\\) definir “parametrización”. Curva parametrizada en \\(\\real^3\\) \\(\\Gamma = \\{\\sigma(t) = (x(t),y(t),z(t)), t \\in (a,b)\\}\\) Queremos excluir: Superficies de parametrización en \\(\\real^3\\). \\[S = \\{\\Phi(u,v) = (x(u,v),y(u,v),z(u,v)), (u,v)\\in D\\}\\] Queremos evitar: Ya tenemos todo lo necesario para definir parametrización: \\(\\Phi_{1,2}(x,y) = (x,y,\\pm \\sqrt{1-x^2-y^2})\\). Esta nos deja sin definir el ecuador. Para ello tenemos que definir más Estas cartas son expresando x,y en función de las otras 2. En total hacen falta 3 parametrizaciones. otra manera de parametrizar, la proyección estereográfica. El dibujo de la proyección es: \\[(u,v) \\in \\real^2 \\rightarrow (u,v,0) \\rightarrow r\\equiv (0,0,R) + t(u,v,-R) = (tu,tv,R(1-t)\\] Imponiendo \\(\\underbrace{P}_{r(t_0)} \\equiv r\\cap S\\) tenemos: \\[(t_0u)^2+(t_0v)^2 + R^2(1-t)^2 = R^2 \\rightarrow ... \\rightarrow t_0 = \\frac{2R^2}{u^2+v^2+R^2}\\] Conclusión: \\[P = (tu,tv,R(1-t) = \\frac{2R^2}{u^2+v^2+R^2}u,\\frac{2R^2}{u^2+v^2+R^2}v,\\frac{R(u^2+v^2-R^2)}{u^2+v^2+R^2} = \\Phi(u,v)\\]. Vemos que \\(\\Phi\\in C^1, \\Phi(\\real^2) = S_R-\\{(0,0,R)\\}\\) ¿Es una parametrización? Hay que comprobar En .2, llamaremos al conjunto definido por la fórmula y el dominio, \\((\\Phi,\\omega)\\). El conjunto de todas las cartas locales que definen una subvariedad se llama Atlas. La proyección estereográfica tiene 2 cartas, la del polo norte (que excluye ese punto) y la del polo sur. \\(\\ref{eq_1}.1 \\dimplies \\ref{eq_2}.2\\). Si tenemos una subvariedad dada como conjunto de nivel la podemos parametrizar (y viceversa). \\begin{proof} Esquema: \\(1\\implies 2, 2\\implies 3, 3 \\implies 1\\) Sea \\(\\ga \\in M, \\exists U\\subset\\real^{N+K}\\) abierto con \\(\\ga \\in U\\) y una función \\(\\appl{F}{U\\subset \\rnk}{\\rk}, F\\in C^1(U), \\tlq U\\cap M = \\{\\gx \\in U\\tq F(\\gx) = \\gor{0}\\}\\). Además \\(DF\\) tiene rango máximo (K). Queremos demostrar que existe una parametrización \\(\\Phi\\). el orden de las variables, suponemos que el menor de orden \\(K\\) con determinante no nulo corresponde con las \\(K\\) últimas variables. Por el teorema de la función implícita, \\(F(x_1,...,x_n,x_{n+1},...,x_{n+k}) = \\gor{0}\\), con \\(x_{n+1},...,x_{n+k}\\) despejable en función de las \\(\\{x_i,i=1,...,n\\}\\) en un entonro de \\(\\gor{a}\\). Es decir: existen \\(\\omega\\subset \\real^N, \\omega&#39;\\subset\\rk\\) abiertos tales que \\(\\gor{a}\\in \\omega \\times \\omega&#39;\\) y una función \\(\\appl{g}{\\omega\\subset\\real^N}{\\omega&#39;\\subset\\rk}\\) de manera que \\[\\left\\{\\begin{array}{c} g(a_1,...,a_n) = (a_{n+1},...,a_{n+k})\\\\ F(\\gx,g(\\gx)) = \\gor{0}, \\forall \\gx \\in \\omega\\end{array}\\right.\\] Idea: la parametrización es \\(\\Phi (\\gx) = (\\gx,g(\\gx))\\) Vamos a comprobar las condiciones: Es decir, $(x,y) = (x,g(x)) ) (x,g(x)) = (x,y) $. También \\(\\inv{\\Phi}(x,y) = x, \\forall(x,y)\\in (\\omega\\times\\omega&#39;) \\cap M\\) Continua. Vamos a emplear un argumento como el de la demostración del segundo teorema del rango. Buscamos: \\[\\begin{align*} \\appl{\\Phi}{\\rnk}{\\rnk} \\tlq \\Phi(V\\cap M) = (\\ast &amp;,0)\\\\ &amp;\\uparrow\\\\ k \\text{ últimas coor} &amp; \\text{denadas} \\end{align*}\\] Siendo \\(\\Phi\\) un difeomorfismo. existe una parametrización local \\(\\appl{\\Psi}{\\real^N}{\\rnk}\\). Completar el número de variables Definimos \\[\\begin{align*} \\appl{\\alpha}{\\omega\\times\\real^K \\subset \\real^N\\times\\real^K}&amp;{\\real^N\\times\\real^K}\\\\ (\\gor{u},\\gor{v})\\longrightarrow &amp;\\alpha(\\gor{u},\\gor{v}) = \\Psi(\\gor{u}) + (\\gor{0},\\gor{v})\\\\ \\gor{u} \\in \\real^N,\\gor{v} \\in \\real^K, \\Phi(\\gor{u})\\in \\real^{N+K} \\end{align*}\\paragraph{Paso 2)}\\] Aplicar el teorema de la función inversa () a \\(\\alpha\\). \\[D\\alpha = \\left(t. \\begin{array}{c|c} D\\Psi_k \\rightarrow &amp; 0\\\\ \\hline D\\Psi_{n+i} \\rightarrow &amp; I \\end{array}\\right. \\] En \\(\\real^3\\) tenemos puntos (dimensión 0), curvas (dimensión 1), superficies (dimensión 2) y abiertos (dimensión 3) sobre los que integrar en los que la imaginación resulta bastante útil. Pero… ¿qué pasa en \\(\\real^N\\)? Entonces en \\(\\real^N\\) tenemos objetos de dimensión \\(0,...,N\\) sobre los que vamos a poder definir propiedades, integrales, etc. Repasamos la idea de que en \\(\\real^3\\) podíamos representar una superficie de varias maneras y cómo cálculabamos su plano tangente. Dada una superficie como una gráfica, simplemente construíamos el vector normal al plano derivando parcialmente: \\[ \\vn = \\left( \\dpa{f}{x},\\dpa{f}{y}, 1 \\right) \\] Con una parametrización \\(\\phi\\) \\[\\begin{align*} \\appl{\\phi}{D\\subset\\real^2 &amp;}{\\real^3} \\\\ \\phi(u,v) &amp;= (x(u,v),y(u,v),z(u,v)) \\end{align*}\\] calculamos el plano tangente tomando los dos vectores directores y calculamos el normal al plano: \\[\\begin{gather*} T_u = \\left( \\dpa{x}{u},\\dpa{y}{u},\\dpa{z}{u}\\right)\\\\ T_v = \\left( \\dpa{x}{v},\\dpa{y}{v},\\dpa{z}{v}\\right)\\\\ \\overrightarrow{n} = T_u\\times T_v\\\\ \\end{gather*}\\] Nos podemos encontrar el problema de que \\(\\overrightarrow{n} = \\overrightarrow{0}\\). Para evitar ese caso, estableceremos que el rango de la matriz de las 6 derivadas tiene que ser 2. Supongamos que nos dan una gráfica definida como un conjunto de nivel \\(F(x,y,z) = 0\\) . Para calcular el plano tangente en este caso tenemos que \\(\\overrightarrow{n} = \\nabla F\\). Nos puede ocurrir que \\(F\\) no sea derivable o que \\(\\nabla F = \\overrightarrow{0}\\). Supongamos que sea diferenciable, ¿cómo preveer que puede salir? Para evitarlo, debemos forzar de nuevo que la matriz de las derivadas tenga rango máximo (en este caso 1). Vamos a ver que pasa con las curvas en \\(\\real^3\\) y cómo calcular la recta tangente Vemos claramente que las condiciones para poder calcular superficies tangentes llegan siempre a obligar a que la matriz tenga (ver sección ). A partir de aquí, podemos crear nuestra definición de subvariedad diferenciable: Veamos ejemplos de si algunos objetos son variedades diferenciables o no: \\[\\appl{F}{U\\subset\\real^3}{\\real}\\] \\[\\{(x,y,z) \\tq F(x,y,z) = 0\\}\\] En este caso tenemos \\(N=2, K=1\\). La condición de rango nos dice que \\(\\nabla F(\\gx) \\neq (0,0,0)\\). Esto es, obliga a en cada punto existe un vector normal \\(\\overrightarrow{n}\\), exactamente la misma condición que habíamos visto antes. \\[\\sigma(t) = (x(t),y(t),z(t))\\equiv S_1 \\cap S_2 = \\] \\[= \\{F_1(x,y,z) = 0\\}\\cap \\{F_2(x,y,z) = 0\\}\\] Si tomamos \\(U\\cap \\sigma = \\{(x,y,z)\\in\\real^3 \\tq F_1 = 0 \\y F_2 = 0\\}\\) Siendo \\[\\appl{F}{U\\subset\\real^3}{\\real^2}, N+K=3,K=2\\]\\[F(x,y,z) = (F_1(x,y,z),F_2(x,y,z))\\] Veamos la condición de rango en este caso: \\[rango \\begin{pmatrix} \\dpa{F_1}{x}&amp;\\dpa{F_1}{y}&amp;\\dpa{F_1}{z}\\\\\\dpa{F_2}{x}&amp;\\dpa{F_2}{y}&amp;\\dpa{F_2}{z}\\end{pmatrix}\\] Para que el rango sea máximo, los vectores tienen que ser no paralelos, es decir, \\(S_1, S_2\\) sean transversales, no paralelas. De nuevo, la misma condición que habíamos visto antes. Podemos definir un punto de una manera un tanto rebuscada: \\[\\begin{gather*} \\appl{F}{\\real^K}{\\real^K}\\\\ \\gx \\rightarrow \\gx - \\ga \\end{gather*}\\] En este caso, la subvariedad \\(M\\) es nuestro punto \\(\\ga\\): \\[M = \\{\\gx \\in \\real^K \\tq F(\\gx) = \\gor{0}\\} = \\{\\ga\\}\\] Tenemos $DF = Id $, que tiene rango máximo y por lo tanto \\(\\{\\ga\\}\\) es una subvariedad diferenciable (dimensión 0, codimensión \\(k\\)). \\(\\appl{F}{\\Omega\\subset\\real^N}{0}\\). En este caso tenemos codimensión \\(0\\) y dimensión \\(N\\). \\[\\begin{gather*} \\appl{F}{\\real^2}{\\real}\\\\ F(x,y) = x^3 - y^6 \\\\ M = \\{(x,y) \\tq x^3-y^6 = 0\\} \\\\ DF = (3x^2-6y^5) \\\\ DF(0,0) = (0,0) \\end{gather*}\\] La matriz diferencial tiene rango \\(0\\) en el punto \\((0,0)\\). ¿Quiere esto decir que \\(M\\) no es una subvariedad diferenciable en el 0? No. Lo que quiere decir que no hemos encontrado la función que cumpla las hipótesis. Podemos operar \\[M = \\{x^3=y^6\\} = \\{x = y^2\\} = \\{x-y^2 = 0\\}\\] Y vemos que si tomamos \\(G(x,y) = x-y^2\\), esta función representa la misma subvariedad \\(M\\), y \\(DG(x,y) = (1,2y)\\) tiene rango 1 en el origen. \\[M = \\{(x,y)\\in \\real^2 \\tq x^2-y^2 = 0\\}\\] Definimos una función \\(F\\): \\[\\appl{F}{\\real^2}{\\real}\\] \\[F(x,y) = x^2-y^2\\] \\(DF = (2x,-2y)\\). La condición de rango falla en \\((0,0)\\). Valoramos si este objeto no es una subvariedad o si tendremos que definir una función de una manera más inteligente, tal y como hicimos en el caso anterior. En este caso, vemos que \\(M = \\{ y=x \\cup y = -x \\}\\). No debería ser una subvariedad porque en el \\((0,0)\\) no es derivable (ver figura ). Intentaremos demostrarlo por reducción al absurdo. Supongamos que \\(M\\) es una subvariedad. Entonces, según la definición () existe un \\(U, (0,0) \\in U\\) y una aplicación \\(\\appl{G}{U\\subset\\real^2}{\\real}\\) con \\(U\\cap M = \\{G(x,y) = 0\\}\\). Entonces tendríamos que \\[\\rango DG(x,y) = 1, \\forall(x,y) \\in U \\implies \\rango \\left(\\dpa{G}{x},\\dpa{G}{y}\\right) = 1\\] Ahí tenemos dos casos, o bien que la primera componente no sea \\(0\\) o que sea la segunda la que no es nula. Supongamos primero que \\(\\dpa{G}{x}(0,0) \\neq 0\\). Podemos aplicar el Teorema de la unción implícita (), que nos dice que podemos despejar \\(x = x(y)\\). En este caso: \\(U \\cap M = \\{x(y)^2-y^2 = 0\\}\\). Si fijamos \\(y=\\varepsilon\\), entonces \\(x(\\varepsilon) = \\pm \\varepsilon\\) No es una función, lo que contradice el Teorema de la función implícita, y por lo tanto es imposible que \\(\\dpa{G}{x}(0,0) \\neq 0\\). Análogamente, tampoco puede cumplirse que \\(\\dpa{G}{y}(0,0) \\neq 0\\). Hemos demostrado por lo tanto que no puede existir una función que defina esto como subvariedad diferencial. Este es el ejemplo de que cualquier objeto que tenga autointersección no puede ser subvariedad. \\(M = \\{(x,y) \\in \\real^2 \\tq x^2-y^2 = 0, y\\ge 0\\}\\) Vamos a suponer que existe una función \\(F \\in C^1\\) que representa ese objeto (que viene definido por 2 condiciones). \\(M = \\{F(x,y) = 0\\}\\) para alguna \\(F\\). Condición de rango: \\(\\left(\\dpa{F}{x},\\dpa{F}{y}\\right) \\neq (0,0)\\). Condición de rango: \\(\\left(\\dpa{F}{x},\\dpa{F}{y}\\right) = (0,0), x=y=0\\). Vamos a ver si es subvariedad o no. En este caso intuimos que no debería serlo. Volvemos a demostrar por reducción al absurdo: Supongamos que \\(M\\) es subvariedad, entonces \\(\\exists G(x,y)\\) tal que \\(\\appl{G}{U\\subset\\real^2}{\\real}, U \\cap M = \\{G(x,y) = 0\\}\\) Supongamos \\(\\displaystyle \\dpa{G}{x}\\neq 0\\). Entonces tenemos \\[ M\\cap U = \\{y(x)^2 - y^2 = 0\\} \\] Si fijamos \\(x=\\varepsilon \\implies y(x) = \\abs{\\varepsilon}\\), pero esto quiere decir que \\(G \\notin C^1\\). De forma análoga con la segunda coordenada, vemos que \\(M\\) no es una subvariedad. Nos quitamos el punto conflictivo del caso anterior, el \\((0,0)\\). \\(N = \\{(x,y)\\in \\real^2 \\tq x^2-y^2 = 0, y&gt;0\\}\\) La lógica nos dice que este caso si debería ser subvariedad diferencial: la definición de la función es local, y siempre podremos encontrar un entorno que no incluya el 0, que es el punto problemático. Comprobamos que efectivamente el rango de \\(\\nabla F\\) es máximo \\(\\forall (x,y)\\in \\real^2\\) y por lo tanto \\(M\\) es una subvariedad. Superficie en \\(\\real^3\\) parametrizada: \\(S = \\{\\Phi(u,v) = (x(u,v),y(u,v),z(u,v))\\}\\). A la hora de trabajar con superficies parametrizadas, nos interesaría poder definir una especie de función inversa que nos permita hacer cambios en el plano y llevarlos a la superficie o al reves, pero… ¡tienen dimensiones distintas! La esperanza que nos queda es que la superficie parametrizada tiene dimensión 2, igual que el plano. \\[\\appl{\\sigma}{[0,2\\pi)}{\\real^2}\\] \\[t \\rightarrow \\sigma(t) = (cos(t),sen(t))\\] Inversa: \\(\\Psi(x,y) = t\\) ángulo de la representación en polares. Vamos a estudiar el problema de la continuidad: Tomamos \\(\\{(X_n,Y_n)\\}\\) con \\(x^2+y^2 = 1 \\tlq (x_n,y_n) \\convs (1,0)\\) Si \\(\\Psi\\) es continua, debe ser \\(\\Psi(x_n,y_n) \\convs (1,0) = 0\\). En este caso no es continua porque: Si tomamos \\[\\begin{align*} P_n &amp;= \\left(cos\\left(2\\pi - \\frac{1}{n}\\right), sen\\left(2\\pi - \\frac{1}{n}\\right) \\right)\\\\ P_n &amp;\\convs (1,0)\\\\ \\Psi(P_n) &amp;= 2\\pi - \\frac{1}{n} \\convs 2\\pi\\neq\\Psi(1,0) \\end{align*}\\paragraph{Ejemplo 2}\\] \\[\\appl{f}{(0,1)\\subset\\real}{\\real^2}\\] \\[t \\rightarrow f(t) = (t,g(t)), \\text{continua, con inversa continua}\\] Vamos a definir la inversa: \\[P\\in f(0,1) \\implies P(x,g(x)) \\text{Para algún } x\\in(0,1)\\] \\(P = (x,g(x))\\) \\[\\Psi(P) = t \\in (0,1) \\tlq f(t) = (x,g(x)) \\implies t = x\\] Vamos a estudiar la continuidad: \\[\\{P_n\\} \\subset f(0,1), P_n \\rightarrow P \\in f(0,1)\\] \\[P_i = (x_i,g(x_i)), \\text{ para algún } i \\in (0,1)\\] \\[P_n \\convs P_0 \\dimplies (x_n,g(x_n)) \\rightarrow (x_0,g(x_0)) \\implies x_n (= \\Psi(P_n)) \\rightarrow x_0 (=\\Psi(P_0)) \\] ¿La gráfica de una función es homeomorfismo sobre su imagen? \\[\\appl{\\sigma_3}{(0,4\\pi)}{\\real^2}\\] \\[t \\rightarrow \\sigma(t) = (cos(t),sen(t))\\] No es inyectiva \\(\\implies \\nexists \\Psi\\). \\[\\appl{\\sigma_4)}{(0,2\\pi)}{\\real^2}\\] \\[t \\rightarrow \\sigma(t) = (cos(t),sen(t))\\] La diferencia con el ejemplo 1, es que es cerrado. Hay que percatarse de que la tangente no es inyectiva, entonces… la inversa es algo más complicada que \\(\\arctan\\), así que vamos a definirla a trozos siguiendo el esquema de la figura . \\[\\Psi(x,y) = \\begin{cases} \\arctan\\left(\\frac{y}{x}\\right) &amp; (x,y)\\in 1\\\\ \\arctan\\left(\\frac{y}{x}\\right) + \\pi &amp; (x,y) \\in 2\\\\ \\arctan\\left(\\frac{y}{x}\\right)+2\\pi&amp; (x,y) \\in 3 \\end{cases} \\] Hay que estudiar la continuidad en \\((0,-1)\\) y en \\((0,1)\\). Tomamos \\(\\{P_n\\} \\rightarrow (0,1), P_n \\in S_1\\) Queremos probar que \\(\\Psi(P_n) \\rightarrow \\Psi(0,1) = \\frac{\\pi}{2}\\). Ejercicio para el lector: Sucesiones que se acercan por la derecha o por la izquierda y comprobar que vale lo que tiene que valer. La parametrización es \\[\\sigma_5 (x(t),y(t)) = \\left(\\frac{\\cos(t)}{\\sin^2(t)+1},\\frac{\\sin(t)\\cos(t)}{\\sin^2(t)+1}\\right), t\\in \\left(\\frac{-\\pi}{2},\\frac{3\\pi}{2}\\right)\\] Al origen podemos acercarnos de distintas formas: Sea \\(\\{P_n\\}\\subset \\sigma_5(t) \\rightarrow (0,0)\\). Podemos tomar la sucesión \\(\\{P_n\\} = \\{P_1,P_2,P_3,...\\}\\) cada uno en una región (1,2,3 ciclicamente). \\[\\Psi(P_n) \\begin{cases} \\in 3 &amp; \\text{ si }n \\text{ es múltiplo de }3\\\\ \\in 2 &amp; \\text{ si } n\\equiv 2\\ mod \\ 3\\\\ \\in 1 &amp; \\text{ si } n \\equiv 1\\ mod \\ 3 \\end{cases}\\] Por tanto \\(\\nexists \\displaystyle \\lim_{n\\rightarrow \\infty} \\Psi(P_n)\\) definir “parametrización”. Curva parametrizada en \\(\\real^3\\) \\(\\Gamma = \\{\\sigma(t) = (x(t),y(t),z(t)), t \\in (a,b)\\}\\) Queremos excluir: Superficies de parametrización en \\(\\real^3\\). \\[S = \\{\\Phi(u,v) = (x(u,v),y(u,v),z(u,v)), (u,v)\\in D\\}\\] Queremos evitar: Ya tenemos todo lo necesario para definir parametrización: \\(\\Phi_{1,2}(x,y) = (x,y,\\pm \\sqrt{1-x^2-y^2})\\). Esta nos deja sin definir el ecuador. Para ello tenemos que definir más Estas cartas son expresando x,y en función de las otras 2. En total hacen falta 3 parametrizaciones. otra manera de parametrizar, la proyección estereográfica. El dibujo de la proyección es: \\[(u,v) \\in \\real^2 \\rightarrow (u,v,0) \\rightarrow r\\equiv (0,0,R) + t(u,v,-R) = (tu,tv,R(1-t)\\] Imponiendo \\(\\underbrace{P}_{r(t_0)} \\equiv r\\cap S\\) tenemos: \\[(t_0u)^2+(t_0v)^2 + R^2(1-t)^2 = R^2 \\rightarrow ... \\rightarrow t_0 = \\frac{2R^2}{u^2+v^2+R^2}\\] Conclusión: \\[P = (tu,tv,R(1-t) = \\frac{2R^2}{u^2+v^2+R^2}u,\\frac{2R^2}{u^2+v^2+R^2}v,\\frac{R(u^2+v^2-R^2)}{u^2+v^2+R^2} = \\Phi(u,v)\\]. Vemos que \\(\\Phi\\in C^1, \\Phi(\\real^2) = S_R-\\{(0,0,R)\\}\\) ¿Es una parametrización? Hay que comprobar En .2, llamaremos al conjunto definido por la fórmula y el dominio, \\((\\Phi,\\omega)\\). El conjunto de todas las cartas locales que definen una subvariedad se llama Atlas. La proyección estereográfica tiene 2 cartas, la del polo norte (que excluye ese punto) y la del polo sur. \\(\\ref{eq_1}.1 \\dimplies \\ref{eq_2}.2\\). Si tenemos una subvariedad dada como conjunto de nivel la podemos parametrizar (y viceversa). Si \\(M\\) es una variedad diferenciable de dimensión \\(N\\) en \\(\\rnk\\), entonces, reordenando las variables puede escribirse como la gráfica de una función \\(C^1, \\underbrace{M\\cap U}_{(*)} = \\{(\\gx,g(\\gx))\\}\\) \\((*) \\implies\\) un trozo de la variedad. Si existe $C^1, rg(D) = n, ()M y $ no es homeomorfismo sobre su imagen \\(\\implies\\) no es subvariedad. Pregunta: Verdadero o Falso: ¿Con encontrar una aplicación que no sea homeomorfismo basta para asegurar que M no es subvariedad? La prueba de este teorema se deja como ejercicio para el lector. Sea \\(M\\) subvariedad n-dimensional en \\(\\rnk\\). \\(T_{\\gor{a}}M \\leadsto\\) espacio tangente a M en el punto \\(\\gor{a}\\in M\\) Pero… ¿qué es un hiperplano tangente? El concepto no es tan simple como un plano tangente. El espacio tangente será el conjunto de todos los vectores tangentes a cada una de las curvas contenidas en la superficie. A partir de aquí buscaremos una forma más eficiente de construirlo (porque es imposible calcular todas y cada una de las curvas contenidas en la superficie). Cualquier curva de la variedad la podremos ver como una curva en la proyección (con una parametriazación construida cuidadosamente) Es decir, si tenemos una parametrización, la diferencial lleva tangentes en tangentes. El teorema y el corolario son las 2 caracterizaciones del espacio tangente. Pero también podemos definir el espacio tangente con una tercera caracterización: Como gráfica. Sea \\(\\appl{f}{\\real^N}{\\real^{K}}\\) \\[M\\equiv \\left\\{(\\gu,f(\\gu)),\\begin{matrix} (\\gu,f(\\gu))\\in \\rnk\\\\ \\gu \\in \\Omega\\subset\\real^N\\\\ \\appl{f}{\\real^N}{\\real^K}\\end{matrix}\\right\\} \\] Salvo reordenación de las variables. Sea \\(p=(\\ga,f(\\ga))\\) el punto en el que queremos calcular el hiperplano tangente a \\(M\\). La caracterización será \\[T_p(M) = \\{(\\gu,v)\\in\\real^N\\x\\real^{K} \\tq v = Df(a) \\gu\\}\\] la otra alternativa es definir la subvariedad como \\(M = \\{(\\gu)\\in\\real^k \\tq f(\\gu) = f(\\ga)\\) $ M = {(x,y) ^2 x2+y2 = 1}$. Es una subvariedad porque… Vamos a calcular \\[T_{(a,b)}M = \\ker \\{\\begin{pmatrix} 2x &amp; 2y \\end{pmatrix}\\} =\\] \\[ \\{(u,v) \\tq \\begin{pmatrix} 2a &amp; 2b \\end{pmatrix}\\begin{pmatrix} u\\\\v \\end{pmatrix} = 0\\} = ... = \\{(u,v)\\in \\real^2 \\tq \\pesc{(a-0,b-0),(a,b)-(u,v)} = 0\\]. El espacio tangente es el formado por todos los vectores perpendiculares al radio. \\[M = \\{ (x,y,z,t)\\in \\real^4 \\tq x^2+y^2 = 1, z^2+t^2 = 1\\}\\] Vamos a calcular: \\(T_{(1,0,0,1)}M\\) … Tenemos definida la variedad de forma implícita: \\(M = \\{F = 0\\}\\) donde \\[F(x,y,z,t) = (x^2+y^2-1,t^2+z^2-1)\\] \\[DF = \\begin{pmatrix} 2x&amp;2y&amp;0&amp;0\\\\ 0&amp;0&amp;2z&amp;2t \\end{pmatrix}\\] En este caso tenemos \\(rg(DF) = 2, \\forall (x,y,z,t)\\in \\real^4\\). (El único punto que podría dar rango 0 es el origen, que en este caso no pertenece a la subvariedad) Vamos con el espacio tangente: \\[T_{(1,0,0,1)}M = \\ker DF (1,0,0,1) = \\ker \\begin{pmatrix} 2&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;2 \\end{pmatrix} =\\] \\[ \\{ (\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4) \\in \\real^4 \\tq \\begin{pmatrix} 2&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;2 \\end{pmatrix} \\begin{pmatrix} \\alpha_1\\\\ \\alpha_2\\\\ \\alpha_3\\\\ \\alpha_4 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\} \\implies \\left\\{\\begin{array}{cc}2\\alpha_1 &amp;= 0\\\\ 2\\alpha_4 &amp; =0\\end{array}\\right.\\] Hiperplano tangente: \\[ \\begin{pmatrix} 1\\\\0\\\\0\\\\1 \\end{pmatrix} + \\alpha_2 \\begin{pmatrix} 0\\\\1\\\\0\\\\0 \\end{pmatrix} + \\alpha_3 \\begin{pmatrix} 0\\\\0\\\\1\\\\0 \\end{pmatrix}, \\alpha_2,\\alpha_3 \\in \\real \\] Y el hiperplano normal: \\[ \\begin{pmatrix} 1\\\\0\\\\0\\\\1 \\end{pmatrix} + \\mu \\begin{pmatrix} 1\\\\0 \\\\0\\\\0 \\end{pmatrix} + \\lambda \\begin{pmatrix} 0\\\\0\\\\0\\\\1 \\end{pmatrix}, \\lambda,\\mu \\in \\real \\] ¿Una posible parametrización de esto? \\[\\begin{align*} \\Phi : D \\subset \\real^2 \\longrightarrow &amp;\\real^4\\\\ (u,v) \\longrightarrow &amp;\\Phi(u,v) = (x,y,z,t) = \\left(cos(u),sen(u),cos(v),sen(v)\\right)\\\\ (u,v)\\in (-\\pi,\\pi)\\times(0,2\\pi) \\end{align*}\\] ¿¿Por qué \\((u,v)\\in (-\\pi,\\pi)\\times(0,2\\pi)\\) y no \\((u,v)\\in (0,2\\pi)\\times(0,2\\pi)\\)?? Porque el punto que tenemos \\((1,0,0,1) \\notin \\img\\). Vamos a calcular la el hiperplano tangente. Como tenemos una parametrización, calcularemos la imagen de la diferencial () \\[T_{(1,0,0,1)}M = \\img D\\Phi\\left(0,\\frac{\\pi}{2}\\right)\\] \\[D\\Phi = \\begin{pmatrix} -sen(u) &amp; 0\\\\ cos(u) &amp; 0\\\\ 0 &amp; -sen(v)\\\\ 0&amp;cos(v) \\end{pmatrix}\\] \\[D\\Phi\\left(0,\\frac{\\pi}{2}\\right) = \\begin{pmatrix} 0&amp;0\\\\1&amp;0\\\\0&amp;-1\\\\0&amp;0 \\end{pmatrix}\\] Entonces el hiperplano tangente es: \\[\\begin{pmatrix} 1\\\\0\\\\0\\\\1 \\end{pmatrix} + u \\begin{pmatrix} 0\\\\1\\\\0\\\\0 \\end{pmatrix} - v \\begin{pmatrix} 0\\\\0\\\\1\\\\0 \\end{pmatrix} \\] Lógico y normal que nos de el mismo hiperplano tangente si estamos trabajando con el mismo objeto (desde una parametrización o desde un conjunto de nivel). ¿Cierto y verdad que sí? El (una especie de flotador) se produce al girar una circunferencia de radio \\(R_1\\) en el plano \\(XZ\\) con el centro sobre el eje \\(X\\) a \\(R_2\\) del origen alrededor del eje \\(Z\\). \\[ \\Phi(\\theta, \\phi) = ((R_2+R_1\\cos\\phi)\\cos\\theta,(R_2+R_1\\cos\\phi)\\sin \\theta,R1\\sin \\phi)\\;\\; \\theta, \\phi \\in (0, 2\\pi) \\] ¿Es \\(\\Phi\\) una parametrización? Vamos con la opción 1: \\[\\Phi(\\alpha,\\theta) = (x,y,z)\\] \\[¿(\\alpha,\\theta) = \\Phi^{-1} (x,y,z)?\\] \\[z = rsen(\\alpha) \\implies \\alpha = arcsen\\left(\\frac{z}{r}\\right)\\] \\[\\frac{y}{x} = tg(\\theta) \\implies \\theta = arctg \\left(\\frac{y}{x}\\right)\\] %Revisar Esto no es suficiente, porque \\(\\appl{arcsen}{(-1,1)}{\\left(\\frac{-\\pi}{2},\\frac{\\pi}{2}\\right)}\\) no toma todos los posibles valores. Lo mismo con la \\(\\appl{arctg}{\\real}{\\left(\\frac{-\\pi}{2},\\frac{\\pi}{2}\\right)}\\) Tenemos que repetir el argumento de () que ya hemos hecho para la \\(arctg\\) vamos a repetirlo para el \\(arcsen\\). \\[f(x,y,z) =\\left\\{ \\begin{matrix} Si &amp; (x,y,z) \\in 1 &amp;\\implies &amp;\\arcsin\\left(\\frac{z}{r}\\right)\\\\ Si &amp; (x,y,z) \\in 2 &amp;\\implies &amp;\\displaystyle\\frac{\\alpha + \\arcsin\\left(\\frac{z}{r}\\right)}{2} = \\frac{\\pi}{2}\\\\ Si &amp; (x,y,z) \\in 3 &amp;\\implies &amp; \\alpha = \\arcsin\\left(\\frac{z}{r}\\right) + 2\\pi \\end{matrix}\\right. \\] Ahora tenemos que estudiar la continuidad des esta función. (Ejercicio para el lector) En este caso (al tratarse de una diferencial) tendremos que hallar la imagen de \\(D\\Phi\\). Ejercicio propuesto: \\(T_{\\left(\\frac{\\pi}{2},\\frac{\\pi}{2}\\right)}M\\). \\begin{problem}[4.4] Sea \\(\\sigma(t) = \\left(\\cos t,\\sen t,t^2(2\\pi - t)^2\\right), t \\in [0,2\\pi]\\). Siempre hemos hablado de parametrizaciones es espacios abiertos, para asegurar la condición de homeomorfismo en casos como este, en el que los puntos cerca de \\(0\\), la inversa me los lleva a puntos lejanos si nos acercamos a \\(0\\) por el \\(0\\) o por el \\(2\\pi\\). Posibles soluciones "],
["espacios-de-banach.html", "Capítulo 27 Espacios de Banach 27.1 Espacios normados 27.2 Separación de conjuntos convexos 27.3 Teorema de prolongamiento 27.4 Duales de los espacios \\(\\ell^p\\) 27.5 Convergencia débil 27.6 Teorema de Banach-Steinhaus 27.7 Espacios reflexivos 27.8 Operadores contínuos y compactos 27.9 Teorema de Fredholm-Riesz 27.10 Aplicaciones abiertos y grafos cerrados 27.11 Caso complejo", " Capítulo 27 Espacios de Banach 27.1 Espacios normados 27.2 Separación de conjuntos convexos 27.3 Teorema de prolongamiento 27.4 Duales de los espacios \\(\\ell^p\\) 27.5 Convergencia débil 27.6 Teorema de Banach-Steinhaus 27.7 Espacios reflexivos 27.8 Operadores contínuos y compactos 27.9 Teorema de Fredholm-Riesz 27.10 Aplicaciones abiertos y grafos cerrados 27.11 Caso complejo "],
["espacios-convexos.html", "Capítulo 28 Espacios convexos 28.1 Familias de seminormas 28.2 Teorema de separación y de prolongamiento 28.3 Teorema de Krein-Milman", " Capítulo 28 Espacios convexos 28.1 Familias de seminormas 28.2 Teorema de separación y de prolongamiento 28.3 Teorema de Krein-Milman "],
["conjuntos-medibles.html", "Capítulo 29 Conjuntos medibles 29.1 Introducción 29.2 Espacios medibles 29.3 Medidas 29.4 Espacios medibles completos 29.5 Medida externa y medibilidad 29.6 Extensión de una medida 29.7 Medida de Lebesgue 29.8 Medida de Lebesgue Stieltjes 29.9 Conjuntos especiales", " Capítulo 29 Conjuntos medibles 29.1 Introducción 29.2 Espacios medibles 29.3 Medidas 29.4 Espacios medibles completos 29.5 Medida externa y medibilidad 29.6 Extensión de una medida 29.7 Medida de Lebesgue 29.8 Medida de Lebesgue Stieltjes 29.9 Conjuntos especiales "],
["funciones-medibles.html", "Capítulo 30 Funciones medibles 30.1 Transformaciones medibles 30.2 Funciones numéricas medibles 30.3 Funciones simples 30.4 Convergencia de funciones medibles", " Capítulo 30 Funciones medibles 30.1 Transformaciones medibles 30.2 Funciones numéricas medibles 30.3 Funciones simples 30.4 Convergencia de funciones medibles "],
["integracion-1.html", "Capítulo 31 Integración 31.1 Construcción de la integral 31.2 Propiedades básicas de la integral 31.3 Conexión con la integral de Riemann en \\(\\mathbb{R}^n\\) 31.4 Teoremas de convergencia 31.5 Integración sobre una medida producto 31.6 Aplicaciones del teorema de Fubini", " Capítulo 31 Integración 31.1 Construcción de la integral 31.2 Propiedades básicas de la integral 31.3 Conexión con la integral de Riemann en \\(\\mathbb{R}^n\\) 31.4 Teoremas de convergencia 31.5 Integración sobre una medida producto 31.6 Aplicaciones del teorema de Fubini "],
["espacios-lp.html", "Capítulo 32 Espacios \\(L^p\\) 32.1 Definición y propiedades generales 32.2 Aproximación en \\(L^p\\) 32.3 Convergencia en \\(L^p\\) 32.4 Integrabilidad uniforme 32.5 Funciones convexas y desigualdad de Jensen", " Capítulo 32 Espacios \\(L^p\\) 32.1 Definición y propiedades generales 32.2 Aproximación en \\(L^p\\) 32.3 Convergencia en \\(L^p\\) 32.4 Integrabilidad uniforme 32.5 Funciones convexas y desigualdad de Jensen "],
["diferenciacion-3.html", "Capítulo 33 Diferenciación 33.1 Medidas con signo 33.2 Medidas complejas 33.3 Continuidad absoluta de medidas 33.4 Diferenciación de medidas 33.5 Funciones a variación acotada 33.6 Funciones absolutamente contínuas", " Capítulo 33 Diferenciación 33.1 Medidas con signo 33.2 Medidas complejas 33.3 Continuidad absoluta de medidas 33.4 Diferenciación de medidas 33.5 Funciones a variación acotada 33.6 Funciones absolutamente contínuas "],
["analisis-de-fourier-en-mathbbrn.html", "Capítulo 34 Análisis de Fourier en \\(\\mathbb{R}^n\\) 34.1 Convolución de funciones 34.2 La transformada de Fourier 34.3 Funciones de rápido decrecimiento 34.4 Análisis de Fourier de medidas en \\(\\mathbb{R}^n\\)", " Capítulo 34 Análisis de Fourier en \\(\\mathbb{R}^n\\) 34.1 Convolución de funciones 34.2 La transformada de Fourier 34.3 Funciones de rápido decrecimiento 34.4 Análisis de Fourier de medidas en \\(\\mathbb{R}^n\\) "],
["medidas-en-espacios-localmente-compactos.html", "Capítulo 35 Medidas en espacios localmente compactos 35.1 Medidas de Radon 35.2 El teorema de representación de Riesz 35.3 Productos de medidas de Radon 35.4 El operador dual 35.5 Operadores compactos", " Capítulo 35 Medidas en espacios localmente compactos 35.1 Medidas de Radon 35.2 El teorema de representación de Riesz 35.3 Productos de medidas de Radon 35.4 El operador dual 35.5 Operadores compactos "],
["espacios-localmente-convexos.html", "Capítulo 36 Espacios localmente convexos 36.1 Propiedades generales 36.2 Funcionales lineales contínuos 36.3 Teoremas de separación de Hahn-Banach 36.4 Algunas construcciones", " Capítulo 36 Espacios localmente convexos 36.1 Propiedades generales 36.2 Funcionales lineales contínuos 36.3 Teoremas de separación de Hahn-Banach 36.4 Algunas construcciones "],
["topologias-debiles-en-espacios-normados.html", "Capítulo 37 Topologías débiles en espacios normados 37.1 Topología débil 37.2 Topología débil\\(^*\\) 37.3 Espacios reflexivos 37.4 Espacios uniformemente convexos", " Capítulo 37 Topologías débiles en espacios normados 37.1 Topología débil 37.2 Topología débil\\(^*\\) 37.3 Espacios reflexivos 37.4 Espacios uniformemente convexos "],
["espacios-de-hilbert-1.html", "Capítulo 38 Espacios de Hilbert 38.1 Principios generales 38.2 Ortogonalidad 38.3 Bases ortonormales 38.4 El adjunto del espacio de Hilbert", " Capítulo 38 Espacios de Hilbert 38.1 Principios generales 38.2 Ortogonalidad 38.3 Bases ortonormales 38.4 El adjunto del espacio de Hilbert "],
["teoria-de-operadores.html", "Capítulo 39 Teoría de operadores 39.1 Tipos de operadores 39.2 Operadores compactos y de rango finito 39.3 El teorema espectral para operadores normales compactos 39.4 El álgebra del grupo \\(L^1\\) 39.5 Representaciones 39.6 Grupos abelianos localmente compactos", " Capítulo 39 Teoría de operadores 39.1 Tipos de operadores 39.2 Operadores compactos y de rango finito 39.3 El teorema espectral para operadores normales compactos 39.4 El álgebra del grupo \\(L^1\\) 39.5 Representaciones 39.6 Grupos abelianos localmente compactos "],
["analisis-en-semigrupos.html", "Capítulo 40 Análisis en semigrupos 40.1 Semigrupos con topologías 40.2 Funciones debilmente casi periódicas 40.3 La estructura de los semigrupos compactos 40.4 Funciones fuertemente casi periódicas 40.5 Semigrupo de operadores", " Capítulo 40 Análisis en semigrupos 40.1 Semigrupos con topologías 40.2 Funciones debilmente casi periódicas 40.3 La estructura de los semigrupos compactos 40.4 Funciones fuertemente casi periódicas 40.5 Semigrupo de operadores "],
["teoria-de-probabilidades.html", "Capítulo 41 Teoría de probabilidades 41.1 Conjuntos y funciones \\(\\sigma\\)-aditivas} 41.2 Variables aleatorias 41.3 Independencia 41.4 Esperanza condicional 41.5 Sucesiones de variables aleatorias independientes 41.6 Martingalas a tiempo discreto 41.7 Procesos estocásticos 41.8 Movimiento browniano 41.9 Integración estocástica 41.10 Aplicación a las finanzas", " Capítulo 41 Teoría de probabilidades 41.1 Conjuntos y funciones \\(\\sigma\\)-aditivas} Sea un conjunto \\(\\Omega\\), y sea \\(\\mathcal{A}\\) una \\(\\sigma\\)-álgebra sobre \\(\\Omega\\). Se dice que \\(\\mathcal{A}\\) es una \\(\\sigma\\)-álgebra si cumple las siguientes propiedades: Es decir, una \\(\\sigma\\)-álgebra es una clase de conjuntos cerrada para las operaciones complementario y unión numerable. Existen una serie de propiedades inmediatas derivadas de las propiedades que definen a la \\(\\sigma\\)-álgebra:\\ \\(\\emptyset \\in \\mathcal{A}\\)\\ \\(\\mathcal{A}\\) es cerrada para la operación intersección numerable. Al par (\\(\\Omega\\), \\(\\mathcal{A}\\)) se le llama espacio medible, y a los conjuntos que pertenecen a \\(\\mathcal{A}\\) se les denomina conjuntos medibles. Si tomamos la relación de inclusión entre conjuntos (\\(\\subseteq\\)) como una relación de orden, podemos hablar sin ambiguedad de sucesiones monótonas. En este sentido, una sucesión creciente de conjuntos es una sucesión \\(\\{A_n\\}_{n \\in \\mathbb{N}}\\) en la que se cumple que \\(A_i \\subseteq A_{i+1}, \\forall i \\in \\mathbb{N}\\). De forma análoga se puede definir sucesión decreciente de conjuntos.\\ Existe una forma intuitiva de definir el límite de una sucesión monótona de conjuntos. En particular, el límite de una sucesión creciente de conjuntos \\(\\{A_n\\}_{n \\in \\mathbb{N} }\\) se puede definir como \\[\\lim_{n\\to\\infty} A_n = \\bigcup_{k \\in \\mathbb{N}}^{\\infty} A_k\\] Esta definición aprovecha la relación de orden entre los conjuntos de la sucesión para afirmar que si un elemento está en el último conjunto de la sucesión, entonces está en todos los anteriores, y por tanto en todos, por ello se puede utilizar una intersección numerable (que está bien definida) para formalizar el concepto.\\ Con una intuición análoga se define el límite de una sucesión decreciente de conjuntos. Si \\(\\{A_n\\}_{n\\in\\mathbb{N}}\\) es una sucesión decreciente de conjuntos, entonces: \\[ \\lim_{n\\to\\infty} A_n = \\bigcap_{k\\in\\mathbb{N}}^{\\infty} A_k \\] No solo se puede hablar de límites en sucesiones monótonas. Para definir los límites en sucesiones arbitrarias de conjuntos tenemos que recurrir a los conceptos de límite inferior y límite superior. La intuición de estos límites superior e inferior pasan por el concepto de las colas de la sucesión.\\ Dada una sucesión de conjuntos \\(\\{A_n\\}_{n\\in\\mathbb{N} }\\) podemos considerar el conjunto \\[ B_n = \\bigcap_{k=n}^{\\infty} A_k \\] y este conjunto contiene aquellos elementos que están en los \\(A_k\\) para \\(k \\geq n\\). Es fácil probar que la sucesión \\(\\{B_n\\}_{n\\in\\mathbb{N}}\\) es una sucesión creciente de conjuntos, y por tanto se puede obtener el límite \\(\\lim_{n\\to\\infty} B_n\\). Informalmente, ese límite es un conjunto que contiene a todos los elementos de \\(A_n\\) que están en todos los conjuntos \\(A_k\\) a partir de cierto \\(n\\in\\mathbb{N}\\). Definimos el límite inferior de \\(A_n\\) como \\[ \\liminf A_n = \\lim_{n\\to\\infty} B_n = \\bigcup_{n\\in\\mathbb{N}} \\bigcap_{k=n}^{\\infty} A_n \\] Análogamente, podríamos definir la sucesión de conjuntos \\(\\{C_n\\}_{n\\in\\mathbb{N}}\\) como \\[ C_n = \\bigcup_{k=n}^{\\infty} A_n \\] Cada \\(C_n\\) contiene todos los elementos que están presentes en algún \\(A_k\\) para \\(k \\geq n\\). Es fácil también ver que la sucesión \\(\\{C_n\\}_{n\\in\\mathbb{N}}\\) es una sucesión decreciente de conjuntos, y por tanto su límite también está bien definido. Se puede definir entonces el límite superior de \\(\\{A_n\\}_{n\\in\\mathbb{N}}\\) como \\[ \\limsup A_n = \\lim_{n\\to\\infty} C_n = \\bigcap_{n\\in\\mathbb{N}} \\bigcup_{k=n}^{\\infty} A_n \\] Informalmente se puede pensar en este límite superior de \\(A_n\\) como el conjunto de los elementos que están en infinitos conjuntos de la sucesión.\\ A partir de estas definiciones, es fácil comprobar que \\[ \\liminf A_n \\subseteq \\limsup A_n \\] Diremos que una sucesión de conjuntos tiene límite si su límite inferior y superior coinciden, y el límite tendrá como valor efectivamente el de estos límites. Es decir: \\[ \\lim A_n = \\liminf A_n = \\limsup A_n \\] en caso de que límite superior e inferior coincidan. Una vez definidos los conceptos sobre conjuntos con los que vamos a trabajar, pasamos a definir las funciones sobre conjuntos. Vamos entonces a definir lo que es una función de conjunto. Sean los espacios medibles (\\(\\Omega\\), \\(\\mathcal{A}\\)) y(\\(\\Omega&#39;\\), \\(\\mathcal{A}&#39;\\)), definimos la función: \\[ X: \\mathcal{A} \\to \\mathcal{A}&#39;\\] \\[ A \\longrightarrow X(A)\\] A raíz de esta definición podemos definir también lo que se conoce como función inversa. Dada una función \\(X\\), la función inversa de \\(X\\), \\(X^{-1}\\), asigna a cada conjunto \\(A&#39; \\in \\mathcal{A}&#39;\\) el conjunto \\(A \\in \\mathcal{A}\\) tal que \\(X(A) = A&#39;\\). La propiedad básica que cumplen las funciones inversas es que preservan las operaciones e inclusiones de conjuntos. Sea un conjunto \\(\\Omega\\) y una \\(\\sigma\\)-álgebra \\(\\mathcal{A}\\) sobre \\(\\Omega\\). Definimos la función de conjunto: \\[\\varphi : \\mathcal{A} \\rightarrow \\mathbb{R}\\] Se dice que \\(\\varphi\\) es aditiva si \\(\\varphi(\\displaystyle\\sum_{1}^{n}A_k)=\\displaystyle\\sum_1^n\\varphi(A_k)\\)\\ Se dice que \\(\\varphi\\) es \\(\\sigma\\)-aditiva si \\(\\varphi(\\displaystyle\\sum_1^\\infty A_k)=\\displaystyle\\sum_1^\\infty \\varphi(A_k)\\) Sea \\(\\varphi\\) definida como antes. Se dice que \\(\\varphi\\) es si \\(\\varphi(A \\cup B) \\leq \\varphi(A) + \\varphi(B)\\) Una vez introducido el concepto de límite para una sucesión de conjuntos, vamos a tratar de definir la continuidad para funciones de conjunto. Tendremos tres tipos de continuidad, cada uno relacionado con un tipo de sucesiones de conjuntos de las que hemos definido anteriormente. En esta sección trabajaremos con una función \\(\\varphi : \\Omega \\to \\Omega&#39;\\)\\ Diremos que \\(\\varphi\\) es continua por abajo si cumple que, dada una sucesión creciente de elementos \\(A_n \\uparrow A\\), se tiene que \\[ \\lim \\varphi (A_n) = \\varphi (A) \\] Por otra parte, diremos que \\(\\varphi\\) es continua por arriba si cumple que dada una sucesión decreciente de elementos \\(A_n \\downarrow A\\), se tiene que \\[ \\lim \\varphi (A_n) = \\varphi (A)\\] Por último, diremos que una función es continua si lo es por arriba y por abajo. Una vez demostrado este teorema, vamos a ver un teorema que nos relaciona las propiedades del supremo e ínfimo de una función \\(\\sigma\\)-aditiva con los conjuntos sobre los que está dicha función definida: Una función de conjuntos \\(\\mu^\\circ\\) es una medida si verifica: A la tupla (\\(\\Omega\\), \\(\\mathcal{A}\\), \\(\\mu_{\\mathcal{A}}\\)) se le denomina espacio de medida.\\ Una medida exterior es una función de conjuntos positiva y \\(\\sigma\\)-subaditiva, es decir, no se cumple la primera propiedad. La \\(\\sigma\\)-subaditividad implica que \\(\\mu(A \\cup B) \\leq \\mu(A) + \\mu(B)\\).\\ Para que una medida exterior fuera una medida tendría que ser \\(\\sigma\\)-aditiva. Es decir, la falla la primera condición. Sí que es positiva ya que \\(\\mu^\\circ(\\emptyset)=0\\) y es creciente. Esta medida exterior se aplica a cualquier conjunto. Va a haber unos subconjuntos en los que la función se comporte como si fuera aditiva.\\ Una vez definido el concepto de medida, vamos a dar ahora el de probabilidad. Una probabilidad \\(\\mathcal{P}\\) sobre un espacio medible (\\(\\Omega\\), \\(\\mathcal{A}\\)) es una medida que además cumple que \\(\\mathcal{P}(\\Omega)=1\\). Por tanto, tiene las siguientes propiedades:\\ \\(\\mathcal{P}(\\emptyset)=0\\)\\ \\(\\forall A \\in \\mathcal{A}, 0 \\leq \\mathcal{P}(A) \\leq 1\\)\\ Es una función \\(\\sigma\\)-aditiva\\ De forma análoga al concepto de espacio de medida, podemos definir ahora el de espacio probabilístico. Un espacio probabilístico es una tupla formada por un conjunto \\(\\Omega\\), una \\(\\sigma\\)-álgebra sobre ese conjunto, \\(\\mathcal{A}\\), y una función de probabilidad \\(\\mathcal{P}\\). Una vez vistas las definiciones anteriores de medida y medida exterior, vamos a ver un teorema fundamental, que nos servirá para justificar la forma en la que haremos el cálculo de probabilidades a posteriori. Veamos dos lemas antes que nos serán necesarios para la demostración del teorema principal.\\ Un conjunto \\(A\\in S(\\Omega)\\) es \\(\\mu^\\circ\\)-medible si se cumple que \\(\\mu^\\circ(D) \\geq \\mu^\\circ(AD)+\\mu^\\circ(A^cD), \\forall D \\in S(\\Omega)\\)\\ Una extensión exterior \\(\\mu^\\circ\\) de una medida \\(\\mu\\) se define como: \\[\\mu^\\circ (A) = \\inf \\sum_j (A_j), \\quad A \\subset \\cup A_j,\\quad recubrimiento \\] Una vez definidos los conceptos sobre espacios de medida y espacios probabilísticos, vamos a aproximarnos al concepto de función medible. Para ello, daremos dos definiciones de función medible, para demostrar más adelante que estas dos definiciones son equivalentes. Empecemos con la definición constructiva de función medible. Dado que nos interesa que el codominio sea \\(\\mathbb{R}\\), trabajaremos con funciones definidas entre un espacio medible (\\(\\Omega\\), \\(\\mathcal{A}\\)) y (\\(\\mathbb{R}\\), \\(\\mathcal{B}\\)), donde \\(\\mathcal{B}\\) representa la \\(\\sigma\\)-álgebra de Borel.\\ Primero, se define la función puntual \\(X: \\Omega \\to \\mathbb{R}\\), que asigna a cada \\(\\omega \\in \\Omega\\) un número \\(x = X(\\omega)\\) único. Llamaremos a esta función variable aleatoria. Utilizaremos de aquí en adelante la siguiente notación: Sea entonces la función \\(X = \\displaystyle \\sum_j x_jI_{A_j}\\), donde \\(A_j\\) son conjuntos medibles y \\(I_{A_j}\\) denota la función indicadora de dicho conjunto. Estas funciones se llaman funciones elementales, y cuando el número de valores distintos que toma la función \\(X\\) es finito, se conocen como funciones simples. Entonces, la definición constructiva de función medible es la que sigue:\\ Una función es medible si es límite de una sucesión de funciones simples \\(\\{X_n\\}\\) convergentes.\\ Esta definición que hemos dado es constructiva, y por tanto, nos será muy útil para la definición constructiva de las integrales. No obstante, para enunciar y demostrar las propiedades de las funciones medibles, suele ser más útil la definición descriptiva siguiente:\\ Sea una función \\(\\varphi : \\mathcal{A} \\rightarrow \\mathcal{B}\\). Se dice que \\(\\varphi\\) es medible si \\(\\forall B \\in \\mathcal{B} \\Rightarrow \\varphi^{-1}(B) \\in \\mathcal{A}\\). Es decir, una función se dice medible si la imagen inversa de todo conjunto medible es medible.\\ No obstante, se puede dar, a raíz de esta, otra definición más económica:\\ Para la definición anterior, es suficiente con exigir la medibilidad de las imágenes inversas de los elementos de una subclase \\(\\alpha\\) para la cual la \\(\\sigma\\)-álgebra minimal sobre \\(\\alpha\\) sea \\(\\mathcal{B}\\)\\ Veamos ahora que las dos definiciones que hemos dado son equivalentes. Vamos a establecer ahora criterios que nos permitan comparar variables aleatorias. Dado un espacio de probabilidad (\\(\\Omega\\), \\(\\mathcal{A}\\), \\(\\mathcal{P}\\)), se dice que dos variables aleatorias son equivalentes si: \\[ X\\mathcal{R}Y \\Leftrightarrow \\mathcal{P}[X = Y] = 1\\] Análogamente: \\[X(\\omega) = Y(\\omega) \\forall \\omega \\in \\Omega \\setminus \\Lambda, \\mathcal{P}(\\Lambda) = 0\\] Si \\(P[|X_n- X| \\geq \\varepsilon]\\rightarrow 0\\), entonces se dice que \\(X_n\\stackrel{\\mathbb{P}}{\\longrightarrow} X\\), es decir, \\(X_n\\) converge en probabilidad a \\(X\\)\\ Se dice que una variable aleatoria \\(X\\) converge uniformemente en \\(u\\) si se cumple que \\(\\mathcal{P}[\\mid X_{n+u} - X_n \\mid \\geq \\varepsilon] \\to 0\\) Una sucesión de variables aleatorias, \\({ X_n }\\) , converge con probabilidad 1, o de forma casi segura, a una variable aleatoria \\(X\\) ( que puede degenerar en una constante K) cuando se cumple que: \\[P(\\lim_{n\\rightarrow\\infty}X_n=X)=1\\] De esta forma interpretamos que \\(X_n\\stackrel{c.s}{\\longrightarrow}X\\) cuando la probabilidad de que en el límite la sucesión de variables aleatorias y aquella a la que converge sean iguales es uno Para el cálculo de probabilidades, nos será muy útil el concepto de integral sobre funciones de conjunto. Este concepto nos servirá para calcular lo que se conoce como la esperanza matemática de una variable aleatoria \\(X\\). En esta seccion trabajaremos sobre el espacio de probabilidad \\((\\Omega, \\mathcal{A}, \\mathcal{P})\\). Comenzaremos definiendo la integral para las funciones simples, para dar luego una definición de integral para funciones no negativas y por último para funciones cualesquiera.\\ Sea entonces \\(\\{A_k\\} \\in \\mathcal{A}\\), tal que \\(\\displaystyle \\sum_k A_k = \\Omega\\), partición medible del espacio. Sea entonces la función simple \\(X = \\displaystyle \\sum_{j=1}^m x_jI_{A_j}, x_j \\geq 0\\). La integral de la función \\(X\\) se define como: \\[\\int_{\\Omega} X d\\mathcal{P} = \\sum_{j=1}^m x_j\\mathcal{P}_{A_j}\\] Ahora, para cualquier función no negativa \\(X\\), se define la integral de la función como: \\[\\int_{\\Omega} X d\\mathcal{P} = \\lim \\int_{\\Omega}X_n d \\mathcal{P}\\] Donde \\(\\{X_{n}\\} \\to X\\). Finalmente, la integral en \\(\\Omega\\) de una función medible \\(X\\) se define como: \\[\\int_{\\Omega} X d\\mathcal{P} = \\int_{\\Omega}X^{+} d\\mathcal{P} - \\int_{\\Omega} X^{-} d\\mathcal{P}\\] donde \\(X^{+} = XI_{[X\\geq0]}\\) y \\(X^{-} = -XI_{[X&lt;0]}\\). Si \\(\\displaystyle \\int_{\\Omega}Xd\\mathcal{P}\\) es finita, es decir, si los dos términos de la diferencia anterior son finitos, entonces se dice que \\(X\\) es integrable en \\(\\Omega\\). Ahora, una vez definida la integral, vamos a ver algunas de sus propiedades. Tenemos primero una serie de propiedades relacionadas con la aditividad de la integral. Sean \\(X,Y\\) dos funciones medibles, entonces (no se demostrarán las propiedades triviales):\\ \\(\\displaystyle \\int (X+Y)d\\mathcal{P} = \\int Xd\\mathcal{P} + \\int Yd\\mathcal{P}\\)\\ \\(\\displaystyle \\int_{A+B} X d\\mathcal{P} = \\int_A X d\\mathcal{P} + \\int_B X d \\mathcal{P}\\)\\ \\(\\displaystyle \\int cXd\\mathcal{P} = c\\int Xd\\mathcal{P}\\)\\ Veamos ahora algunas propiedades relacionadas con el orden:\\ \\(X \\geq 0 \\rightarrow \\displaystyle \\int X d\\mathcal{P} \\geq \\int 0 = 0\\)\\ \\(X \\geq Y \\rightarrow \\displaystyle \\int X d\\mathcal{P} \\geq \\int Y d\\mathcal{P}\\)\\ \\(\\displaystyle X \\stackrel{c.s}{=} Y \\rightarrow \\int X d\\mathcal{P} = \\int Y d\\mathcal{P}\\)\\ Dado un espacio probabilístico \\((\\Omega, \\mathcal{A}, P)\\) y una v.a. \\(X\\), se define la esperanza matemática de \\(X\\) como: \\[E(X)=\\int_{\\Omega}XdP=\\int_{\\omega \\in \\Omega}{X(\\omega)dP(\\omega)}\\] \\\\ La esperanza matemática se define de forma distinta si estamos trabajando con una v.a. discreta o continua. En el caso de una v.a. discreta, sea \\(p(x_i)\\) su función de probabilidad. Definimos la esperanza matemática como: \\[E(X)=x_1p(X=x_1)+\\ldots + x_n p(X=x_n)=\\sum_{i=1}^{n}{x_i p(x_i)}\\] En el caso de una v.a. continua, sea \\(f(x)\\) la función de densidad. Definimos la esperanza matemática como: \\[E(X)=\\int_{-\\infty}^{+\\infty} {xf(x)dx}\\] Una vez vista la definición de integral y esperanza matemática y algunas de sus propiedades, vamos a enunciar y demostrar un teorema de convergencia que nos será de mucha utilidad para el estudio de variables aleatorias. Veamos su enunciado y demostración: Veamos ahora un resultado que nos da información sobre el límite superior e inferior de una sucesión de variables aleatorias, en caso de que estas estén acotadas por variables aleatorias integrables Vamos a ver algunas desigualdades que nos permitirán simplificar el cálculo de carácterísticas asociadas a variables aleatorias.\\ \\ Dado un espacio probabilístico (\\(\\Omega\\), \\(\\mathcal{A}\\), \\(\\mathcal{P}\\)) y una variable aleatoria \\(X\\), se define el momento r-ésimo o de orden r de la variable \\(X\\) a: \\[ EX^r = \\int X^r \\, d \\mathcal{P} \\] Dicho momento existe si \\(E|X|^r &lt; \\infty\\) \\(E|XY| \\leq \\left(E|X|^r \\right)^{\\frac{1}{r}} \\left(E|Y|^s \\right)^{\\frac{1}{s}}\\) con \\(r&gt;s\\) ; $ +=1$ \\(E|XY|\\leq \\left(E|X|^2 \\right)^{\\frac{1}{2}} \\left(E|Y|^2 \\right)^{\\frac{1}{2}}\\) Si \\(r \\geq 1 \\Rightarrow (E|X+X&#39;|^r)^{\\frac{1}{r}} \\leq \\left(E|X|^r \\right)^{\\frac{1}{r}} + \\left(E|X&#39;|^r \\right)^{\\frac{1}{r}}\\) Sea \\(X\\) una v.a. y sea \\(g\\) una función boreliana. Entonces se tiene: Vamos a demostrar que en efecto se cumple la desigualdad básica: Se dice que una sucesión de variables aleatorias \\(X_n\\) converge en r-medida a \\(X\\) si: \\[ E|X_n - X|^r \\to 0\\] Lo notamos como \\(X_n \\stackrel{r}{\\rightarrow} X\\) Función puntual que se define sobre la recta real, no negativa, no decreciente, continua por la izquierda y que cumple que \\(F(-\\infty) = 0, F(+\\infty) = 1\\)\\ Se conoce como distribución inducida por la variable aleatoria \\(X\\) a (\\(\\mathbb{R}\\), \\(\\mathcal{B}\\), \\(\\mathcal{P}^X\\)), donde la medida de probabilidad \\(\\mathcal{P}^X\\) se define como: \\[ \\mathcal{P}^X(B) = \\mathcal{P}(X^{-1}(B)), \\, \\forall B \\in \\mathcal{B} \\] Tenemos entonces que: Llamamos función generatriz de momentos de la variable aleatoria \\(X\\) a la función: \\[ M_X(t) = E(e^{tX}); \\, t \\in \\mathbb{R} \\] Dada una variable aleatoria \\(\\mathbb{X}\\) continua, definimos su función característica como: \\[\\varphi_X(t)=E(e^{itX})=\\int_{-\\infty}^{\\infty}e^{itx}f_{X}(x)=E(cos(tX))+iE(sen(TX))\\] Se dice que los eventos \\(A_t\\) son independientes si para todo subconjunto finito \\((t_i,\\ldots ,t_n)\\) se tiene que: \\[P\\bigcap_{k=1}^n A_{tk}=\\displaystyle\\prod_{k=1}^{n}P\\, A_{tk}\\] De hecho, el concepto de independencia es relativo a la familia de clases. Una clase de eventos se dice independiente si sus eventos son independientes. Esto tiene tres aplicaciones fundamentales: Una familia de variables aleatorias \\(X_{T_s}=\\{ X_t \\, , \\, t\\in T_s \\}\\) induce \\(\\sigma\\) algebras \\(\\mathcal{B}(X_{T_s})\\) de eventos Las variables aleatorias \\(X_t\\, , \\, t\\in T\\) son independientes si para cada clase finita \\((S_{t_1},\\ldots , S_{t_n})\\) de conjuntos de Borel en R: \\(P\\bigcap_{k=1}^{n}[X_{t_k}\\in S{t_k}]=\\displaystyle\\prod_{k=1}^n P[X_{t_k}\\in S_{t_k}]\\) Se conoce como distribución inducida por la variable aleatoria \\(X\\) a (\\(\\mathbb{R}\\), \\(\\mathcal{B}\\), \\(\\mathcal{P}^X\\)), donde la medida de probabilidad \\(\\mathcal{P}^X\\) se define como: \\[ \\mathcal{P}^X(B) = \\mathcal{P}(X^{-1}(B)), \\, \\forall B \\in \\mathcal{B} \\] Tenemos entonces que: Llamamos función generatriz de momentos de la variable aleatoria \\(X\\) a la función: \\[ M_X(t) = E(e^{tX}); \\, t \\in \\mathbb{R} \\] Dada una variable aleatoria \\(\\mathbb{X}\\) continua, definimos su función característica como: \\[\\varphi_X(t)=E(e^{itX})=\\int_{-\\infty}^{\\infty}e^{itx}f_{X}(x)=E(cos(tX))+iE(sen(TX))\\] Se dice que los eventos \\(A_t\\) son independientes si para todo subconjunto finito \\((t_i,\\ldots ,t_n)\\) se tiene que: \\[P\\bigcap_{k=1}^n A_{tk}=\\displaystyle\\prod_{k=1}^{n}P\\, A_{tk}\\] De hecho, el concepto de independencia es relativo a la familia de clases. Una clase de eventos se dice independiente si sus eventos son independientes. Esto tiene tres aplicaciones fundamentales: Una familia de variables aleatorias \\(X_{T_s}=\\{ X_t \\, , \\, t\\in T_s \\}\\) induce \\(\\sigma\\) campos \\(\\mathcal{B}(X_{T_s})\\) de eventos Las variables aleatorias \\(X_t\\, , \\, t\\in T\\) son independientes si para cada clase finita \\((S_{t_1},\\ldots , S_{t_n})\\) de conjuntos de Borel en R: \\(P\\bigcap_{k=1}^{n}[X_{t_k}\\in S{t_k}]=\\displaystyle\\prod_{k=1}^n P[X_{t_k}\\in S_{t_k}]\\) Sean \\(X_1,\\ldots X_n\\) variables aleatorias independientes e idénticamente distribuidas tales que \\(P[X_n=1]=p\\) y \\(P[X_n=0]=q=1-p\\). Entonces se tiene que: \\[\\dfrac{S_n}{n}-p \\stackrel{\\mathcal{P}}{\\longrightarrow}0\\] El Teorema del Límite Central, que desarrollaremos más adelante, es una consecuencia directa del perfeccionamiento de esta ley por parte de Moivre y Laplace. \\\\ Además enunciamos también otra ley muy parecida a esta y también muy importante que es la , que constituye una versión de esta pero con la convergencia casi segura. Si se tienen las hipótesis anteriores, entonces: \\(\\dfrac{S_n}{n}-p \\stackrel{c.s.}{\\longrightarrow}0\\) Diremos que centramos \\(X\\) a \\(c\\) si reemplazamos \\(X\\) por \\(X-c\\). Si \\(X\\) es integrable, entonces podemos centrarla a su esperanza \\(EX\\) de forma que \\(X\\) es reemplazada por \\(X-EX\\). Es decir, diremos que una variable aleatoria está centrada a su media si y solo si su media existe y es igual a 0. \\\\ Ahora vamos a introducir la varianza matemática. Sea \\(X\\) integrable, llamaremos varianza de \\(X\\) al segundo momento de \\(X-EX\\). Dicho momento existe pero podría ser \\(\\infty\\) y lo denotaremos como \\(\\sigma^2 X\\). Por consiguiente: \\[\\sigma^2 X=E(X-EX)^2=EX^2-(EX)^2\\] Se tiene que para todo \\(c\\) finito: \\[\\sigma^2(X-c)=E(X-c-E(X-c))^2=E(X-EX)^2\\] lo que quiere decir que cuando centramos una variable su varianza no cambia \\\\ Se dice que truncamos \\(X\\) a \\(c&gt;0\\) (finita) cuando reemplazamos \\(X\\) por \\(X^c=X\\) o \\(0\\) dependiendo de si \\(|X|&lt;c\\) o si \\(|X|\\geq c\\) ; se dice entonces que \\(X^c\\) es \\(X\\) truncada a \\(c\\). Lo que estamos haciendo en realidad es definir una variable truncada, \\(X^c\\) como hemos visto anteriormente. \\\\ Tenemos entonces que si \\(F\\) es el diferencial de \\(X\\), entonces existen todos los momentos de \\(X^c\\) y son finitos; estos son: \\[EX^c=\\int_{|X|&lt;c}x\\, dF\\,\\,\\, , \\,\\,\\, E(X^c)^2=\\int_{|X|&lt;c}x^2\\, dF \\,\\,\\, , \\,\\,\\, \\ldots\\] Siempre podremos seleccionar un \\(c\\) suficientemente grande para hacer que \\(P[X\\neq X^c]=P[|X|\\geq c]\\) sea arbitrariamente pequeño. Además nosotros también podemos seleccionar el \\(c_j\\) suficientemente grande para conseguir \\(P\\bigcup [X_j\\neq X_j^{c_j}]\\) arbitrariamente pequeño, de tal forma que, dado \\(\\epsilon &gt; 0\\), tenemos: \\[P\\bigcup [X_j\\neq X_j^{c_j}]\\leq \\sum P[|X_j|\\geq c_j]&lt;\\frac{\\epsilon}{2^j}\\] Por consiguiente, dada una familia contable de variables aleatorias, podemos hacer corresponder una familia de variables aleatorias que difiere de la primera en un evento de probabilidad pequeña. Además, si estamos interesados principalmente en propiedades en el límite, no necesitamos probabilidades pequeñas arbitrarias. Se dice que \\(X_n\\) y \\(X_n^{\\prime}\\) son equivalentes en la convergencia si el conjunto de convergencia de ambos es el mismo c.s. Si las series \\(\\sum P[X_n\\neq X_n^{\\prime}]\\) convergen, entonces las sucesiones \\(X_n\\) y \\(X_n^{\\prime}\\) son cola-equivalentes, y por tanto, las series \\(\\sum X_n\\) y \\(\\sum X_n^{\\prime}\\) son equivalentes en cuanto a convergencia.\\ Sean \\(X_1\\) , \\(X_2\\) integrables. Entonces, como el centramiento no modifica la varianza, podemos asumir, cuando computamos varianzas, que esas variables aleatorias están centradas a la media. Entonces: \\[\\sigma^2 S_n=ES_n^2=\\sum_{k=1}^n EX_k^2+\\sum_{j,k=1}EX_jX_k=\\sum_{k=1}^n\\sigma^2 X_k\\] como \\(X_j\\) y \\(X_k\\) son independientes, tenemos que: \\[EX_jX_k=EX_j\\cdot EX_k=0\\] Con esto en mente podemos enunciar la: Si las variables aleatorias independientes \\(X_n\\) son independientes e integrabless, entonces: \\[ \\sigma^2 S_n=\\sum_{k=1}^n \\sigma^2 X_k\\] Con la desigualdad anterior obtenemos la : \\[\\frac{\\displaystyle\\sum_{k=1}^n \\sigma^2 X_k-\\epsilon ^2}{sup(S_n-ES_n)^2}\\leq P[|S_n-ES_n|\\geq \\epsilon]\\leq \\frac{\\sigma^2(S_n-ES_n)}{\\epsilon^2} \\leq \\frac{1}{\\epsilon^2}\\sum_{k=1}^n\\sigma^2 X_k\\] Esta desigualdad, aplicada a \\((S_{n+k}-ES_{n+k})-(S_n-ES_n)\\) y remplazando \\(\\epsilon\\) por \\(b_n\\) y pasando al límite se tiene el siguiente corolario: \\\\ Este último corolario es debido a Tchebichev (cuando \\(b_n=n\\)). En el caso de Bernouilli, donde \\(b_n=n\\), \\(EX_n=p\\) , \\(\\sigma^2 X_n=pq\\) se reduce a la ley de Bernouilli de los grandes números. Sea \\({X_n}\\) una sucesión de variables aleatorias independientes e idénticamente distribuidas con una ley común \\(\\mathcal{L}(X)\\). Entonces: \\[E|X|&lt;\\infty \\Longleftrightarrow \\dfrac{S_n}{n}=\\dfrac{\\sum_{k=1}^n X_k}{n}=\\dfrac{X_1 \\ldots X_n}{n}\\stackrel{c.s}{\\longrightarrow}EX\\] El problema del límite central en probabilidad es el problema de convergencia de leyes de secuencias de sumas de variables aleatorias. Consideramos \\(S_n\\) el número de ocurrencias de un determinado evento con probabilidad \\(p\\) en \\(n\\) intentos idénticos e independientes. Para evitar trivialidades asumimos que \\(pq\\neq 0\\), donde \\(q=1-p\\). Si \\(X_k\\) denota el indicador del evento en el \\(k\\)-ésimo intento, entonces \\(S_n=\\displaystyle\\sum_{k=1}^n X_k\\) con \\(n=1,2,\\ldots\\) donde los sumandos son indicadores independientes e idénticamente distribuidos. Este es el caso de Bernouilli; como \\(EX_k=p\\) y \\(EX_k^2=p\\), entonces \\(\\sigma^2 X_k=p-p^2=pq\\) de donde se sigue que: \\[ES_n=\\sum_{k=1}^nEX_k=np \\,\\,\\, , \\,\\,\\, \\sigma^2 S_n=\\sum_{k=1}^n \\sigma^2 X_k=npq\\] EL primer teorema del límite central publicado en 1713 dice que \\(\\dfrac{S_n}{n}\\stackrel{\\mathcal{P}}{\\longrightarrow}p\\) \\\\ Puliendo este resultado, de Moivre obtuvo el segundo teorema de límites, que en la forma integral dada por Laplace, dice que: \\[P\\left[ \\frac{S_n -np}{\\sqrt{npq}}&lt; x \\right]\\rightarrow \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{x}e^{-\\frac{1}{2}y^2}dy\\, ,\\,\\,\\, -\\infty\\leq x\\leq \\infty\\] \\\\ El tercer teorema del límite central fue obtenido por Poisson, quien modificó el caso de Bernouilli asumiendo que la probabilidad \\(p=p_n\\) depende del número de intentos de tal manera que \\(np_n\\rightarrow\\lambda&gt;0\\). Por consiguiente, escribiendo ahora \\(X_{nk}\\) y \\(S_{nn}\\) en lugar de \\(X_k\\) y \\(S_n\\), el caso de Poisson corresponde a una serie de sumas \\(S_{nn}=\\displaystyle\\sum_{k=1}^n X_{nk}\\) donde, para cada n, los sumandos \\(X_{nk}\\) son indicadores independientes e idénticamente distribuidos con \\(P[X_{nk}=1]=\\dfrac{\\lambda}{n}+o \\left( \\dfrac{1}{n}\\right)\\) 41.2 Variables aleatorias 41.3 Independencia 41.4 Esperanza condicional 41.5 Sucesiones de variables aleatorias independientes 41.6 Martingalas a tiempo discreto 41.7 Procesos estocásticos 41.8 Movimiento browniano 41.9 Integración estocástica 41.10 Aplicación a las finanzas "],
["apendice.html", "Capítulo 42 Apéndice", " Capítulo 42 Apéndice "],
["software-tools.html", "A Software Tools A.1 R and R packages A.2 Pandoc A.3 LaTeX", " A Software Tools For those who are not familiar with software packages required for using R Markdown, we give a brief introduction to the installation and maintenance of these packages. A.1 R and R packages R can be downloaded and installed from any CRAN (the Comprehensive R Archive Network) mirrors, e.g., https://cran.rstudio.com. Please note that there will be a few new releases of R every year, and you may want to upgrade R occasionally. To install the bookdown package, you can type this in R: install.packages(&quot;bookdown&quot;) This installs all required R packages. You can also choose to install all optional packages as well, if you do not care too much about whether these packages will actually be used to compile your book (such as htmlwidgets): install.packages(&quot;bookdown&quot;, dependencies = TRUE) If you want to test the development version of bookdown on GitHub, you need to install devtools first: if (!requireNamespace(&#39;devtools&#39;)) install.packages(&#39;devtools&#39;) devtools::install_github(&#39;rstudio/bookdown&#39;) R packages are also often constantly updated on CRAN or GitHub, so you may want to update them once in a while: update.packages(ask = FALSE) Although it is not required, the RStudio IDE can make a lot of things much easier when you work on R-related projects. The RStudio IDE can be downloaded from https://www.rstudio.com. A.2 Pandoc An R Markdown document (*.Rmd) is first compiled to Markdown (*.md) through the knitr package, and then Markdown is compiled to other output formats (such as LaTeX or HTML) through Pandoc. This process is automated by the rmarkdown package. You do not need to install knitr or rmarkdown separately, because they are the required packages of bookdown and will be automatically installed when you install bookdown. However, Pandoc is not an R package, so it will not be automatically installed when you install bookdown. You can follow the installation instructions on the Pandoc homepage (http://pandoc.org) to install Pandoc, but if you use the RStudio IDE, you do not really need to install Pandoc separately, because RStudio includes a copy of Pandoc. The Pandoc version number can be obtained via: rmarkdown::pandoc_version() ## [1] &#39;1.19.2.4&#39; If you find this version too low and there are Pandoc features only in a later version, you can install the later version of Pandoc, and rmarkdown will call the newer version instead of its built-in version. A.3 LaTeX LaTeX is required only if you want to convert your book to PDF. The typical choice of the LaTeX distribution depends on your operating system. Windows users may consider MiKTeX (http://miktex.org), Mac OS X users can install MacTeX (http://www.tug.org/mactex/), and Linux users can install TeXLive (http://www.tug.org/texlive). See https://www.latex-project.org/get/ for more information about LaTeX and its installation. Most LaTeX distributions provide a minimal/basic package and a full package. You can install the basic package if you have limited disk space and know how to install LaTeX packages later. The full package is often significantly larger in size, since it contains all LaTeX packages, and you are unlikely to run into the problem of missing packages in LaTeX. LaTeX error messages may be obscure to beginners, but you may find solutions by searching for the error message online (you have good chances of ending up on StackExchange). In fact, the LaTeX code converted from R Markdown should be safe enough and you should not frequently run into LaTeX problems unless you introduced raw LaTeX content in your Rmd documents. The most common LaTeX problem should be missing LaTeX packages, and the error may look like this: ! LaTeX Error: File `titling.sty&#39; not found. Type X to quit or &lt;RETURN&gt; to proceed, or enter new name. (Default extension: sty) Enter file name: ! Emergency stop. &lt;read *&gt; l.107 ^^M pandoc: Error producing PDF Error: pandoc document conversion failed with error 43 Execution halted This means you used a package that contains titling.sty, but it was not installed. LaTeX package names are often the same as the *.sty filenames, so in this case, you can try to install the titling package. Both MiKTeX and MacTeX provide a graphical user interface to manage packages. You can find the MiKTeX package manager from the start menu, and MacTeX’s package manager from the application “TeX Live Utility”. Type the name of the package, or the filename to search for the package and install it. TeXLive may be a little trickier: if you use the pre-built TeXLive packages of your Linux distribution, you need to search in the package repository and your keywords may match other non-LaTeX packages. Personally, I find it frustrating to use the pre-built collections of packages on Linux, and much easier to install TeXLive from source, in which case you can manage packages using the tlmgr command. For example, you can search for titling.sty from the TeXLive package repository: tlmgr search --global --file titling.sty # titling: # texmf-dist/tex/latex/titling/titling.sty Once you have figured out the package name, you can install it by: tlmgr install titling # may require sudo LaTeX distributions and packages are also updated from time to time, and you may consider updating them especially when you run into LaTeX problems. You can find out the version of your LaTeX distribution by: system(&#39;pdflatex --version&#39;) ## pdfTeX 3.14159265-2.6-1.40.18 (TeX Live 2017) ## kpathsea version 6.2.3 ## Copyright 2017 Han The Thanh (pdfTeX) et al. ## There is NO warranty. Redistribution of this software is ## covered by the terms of both the pdfTeX copyright and ## the Lesser GNU General Public License. ## For more information about these matters, see the file ## named COPYING and the pdfTeX source. ## Primary author of pdfTeX: Han The Thanh (pdfTeX) et al. ## Compiled with libpng 1.6.29; using libpng 1.6.29 ## Compiled with zlib 1.2.11; using zlib 1.2.11 ## Compiled with xpdf version 3.04 "],
["referencias.html", "Referencias", " Referencias "]
]
